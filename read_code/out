list print :      (nil) ( 0x232e040  DKDK_HEAD)  0x232e060 
list print :  0x232e040 ( 0x232e060         if)  0x232e080 
list print :  0x232e060 ( 0x232e080        for)  0x232e0a0 
list print :  0x232e080 ( 0x232e0a0      while)      (nil) 
[ OK ] open : 3 ok... 
file : ./test/kernel/fs/ntfs/file.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for now without overflowing the page 
if (sizeof(unsigned long) < 8) { 
ifdef NTFS_RW 
if relevant complete pages are already uptodate in the page cache then 
if the attribute is resident, we do not need to touch the page 
if the page is uptodate, the 
ifies that the behaviour of resizing a file whilst it is mmap()ped 
if 
for i_ino 0x%lx, attribute type 0x%x, " 
if (!NInoAttr(ni)) 
if (NInoNonResident(ni)) 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (new_init_size > old_i_size) { 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
for us. 
if (IS_ERR(page)) { 
if (unlikely(PageError(page))) { 
if (ni->initialized_size > new_init_size) 
ifying the 
for sparse pages and here we would 
for us to only 
while (++index < end_index); 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (ctx) 
if (m) 
if (ctx) 
if (m) 
ifficult to understand, then think of the while loop being 
while (!ret && uaddr < end); 
while (!__get_user(c, uaddr) && (uaddr += PAGE_SIZE, uaddr < end)) 
if (len > bytes) 
while (bytes); 
if (!pages[nr]) { 
if (unlikely(!*cached_page)) { 
if (unlikely(err)) { 
while (nr < nr_pages); 
while (nr > 0) { 
for_read(struct buffer_head *bh) 
for_non_resident_write - prepare pages for receiving data 
for non-resident attributes from ntfs_file_buffered_write() 
if necessary, 
ified yet. 
for_non_resident_write(struct page **pages, 
for inode 0x%lx, attribute type 0x%x, start page " 
if 
if (!page_has_buffers(page)) { 
if (unlikely(!page_has_buffers(page))) 
while (++u < nr_pages); 
for each page over each buffer.  Use goto to 
if (buffer_new(bh)) 
if (buffer_mapped(bh)) { 
if (buffer_uptodate(bh)) 
if (PageUptodate(page)) { 
fore the write, i.e. now. 
if ((bh_pos < pos && bh_end > pos) || 
if (bh_pos < initialized_size) { 
for_read(bh); 
if (likely(!cdelta || (cdelta > 0 && cdelta < vcn_len))) { 
if 
if we allocated it.  On the other 
if (PageUptodate(page)) { 
if (unlikely(was_hole)) { 
if (bh_end <= pos || bh_pos >= end) 
if (likely(!was_hole)) { 
fore the write, 
if (!buffer_uptodate(bh) && bh_pos < end && 
if (bh_pos < initialized_size) { 
for_read(bh); 
if the 
if (bh_end <= pos || bh_pos >= end) { 
if (!buffer_uptodate(bh) && 
if (bh_pos < pos) { 
if (bh_end > end) { 
if (bh_pos > initialized_size) { 
if (!buffer_uptodate(bh)) 
if (!buffer_uptodate(bh)) { 
if (!rl) { 
if (likely(rl != NULL)) { 
while (rl->length && rl[1].vcn <= bh_cpos) 
if (likely(lcn >= 0)) { 
if (likely(vcn + vcn_len >= cend)) { 
if (unlikely(lcn != LCN_HOLE && lcn != LCN_ENOENT)) { 
if (!rl_write_locked) { 
for 
if (likely(!err)) { 
if (err == -ENOENT) { 
if (unlikely(vol->cluster_size < PAGE_CACHE_SIZE)) { 
if ((bh_cend <= cpos || bh_cpos >= cend)) { 
while is 
if (PageUptodate(page)) { 
if (!buffer_uptodate(bh)) { 
if it was not really out of 
if it is locked 
for reading relock it now and retry in case it changed 
if (!rl_write_locked) { 
while (--rl2 >= ni->runlist.rl) { 
if (IS_ERR(rl2)) { 
if (IS_ERR(rl)) { 
if (err != -ENOMEM) 
if (ntfs_cluster_free_from_rl(vol, rl2)) { 
if (!NInoAttr(ni)) 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (!highest_vcn) 
for the new 
for_mapping_pairs(vol, rl2, vcn, 
if (unlikely(mp_size <= 0)) { 
for mapping pairs " 
if (unlikely(err)) { 
if when we reach the end we have not 
if even that fails, add a new attribute 
for the extended attribute " 
if (unlikely(err)) { 
if it was not set. */ 
if (likely(NInoSparse(ni) || NInoCompressed(ni))) { 
if (a->data.non_resident.lowest_vcn) { 
if (unlikely(err)) { 
if (likely(vcn + vcn_len >= cend)) { 
while (bh_pos += blocksize, (bh = bh->b_this_page) != head); 
if (likely(!err && ++u < nr_pages)) 
if we took it. */ 
if (unlikely(rl_write_locked)) { 
if (unlikely(rl)) 
while (wait_bh > wait) { 
if (likely(buffer_uptodate(bh))) { 
if (unlikely(bh_pos + blocksize > initialized_size)) { 
if (likely(bh_pos < initialized_size)) 
if (unlikely(!buffer_uptodate(bh))) */ 
if (likely(!err)) { 
if (buffer_new(bh)) 
while ((bh = bh->b_this_page) != head); 
if (status.attr_switched) { 
if (ntfs_attr_lookup(ni->type, ni->name, ni->name_len, 
ified, need to restore it by punching a 
ify the runlist if we are able to generate a 
if (status.runlist_merged && !status.attr_switched) { 
if (ntfs_rl_punch_nolock(vol, &ni->runlist, bh_cpos, 1)) { 
if (success) */ { 
if we succeeded in punching its vcn out of the 
if (ntfs_bitmap_clear_bit(vol->lcnbmp_ino, lcn)) { 
if the runlist has been 
if (status.mp_rebuilt && !status.runlist_merged) { 
if (success) */ { 
if (status.mft_attr_mapped) { 
if (rl_write_locked) 
if (rl) 
if (u == nr_pages && 
if (!buffer_new(bh)) 
if (!buffer_uptodate(bh)) { 
while ((bh = bh->b_this_page) != head); 
if (len > bytes) 
if (unlikely(left)) { 
if (unlikely(left)) 
if (!bytes) 
while (++pages < last_page); 
while (++pages < last_page) { 
if (!bytes) 
if (len > bytes) 
while (1) { 
if (len > bytes) 
if (unlikely(left)) { 
if (!bytes) 
while (bytes) { 
if (len > bytes) 
if (iov->iov_len == iov_ofs) { 
ifference is that on a fault we need to memset the remainder of the 
ifference between __copy_from_user_inatomic() and 
former 
ifference at all on those architectures. 
if (len > bytes) 
if (unlikely(copied != len)) { 
if (unlikely(copied != len)) 
if (!bytes) 
while (++pages < last_page); 
while (++pages < last_page) { 
if (!bytes) 
if (len > bytes) 
while (nr_pages > 0); 
if (bh_end <= pos || bh_pos >= end) { 
while (bh_pos += blocksize, (bh = bh->b_this_page) != head); 
if (!partial && !PageUptodate(page)) 
while (++u < nr_pages); 
if we do not need to update initialized_size or i_size we 
if (end <= initialized_size) { 
if (!NInoAttr(ni)) 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (end > i_size_read(vi)) { 
if (ctx) 
if (m) 
if (err != -ENOMEM) 
for_non_resident_write() has been called before 
if all buffers in the 
for_non_resident_write(), we do not need to do any page 
for inode 0x%lx, attribute type 0x%x, start page " 
if (NInoNonResident(ni)) 
if (!NInoAttr(ni)) 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if necessary. */ 
if (!PageUptodate(page)) { 
if (end < attr_len) 
if necessary. */ 
if (end > initialized_size) { 
if (err == -ENOMEM) { 
if (PageUptodate(page)) { 
if (ctx) 
if (m) 
if (to > inode->i_size) { 
for i_ino 0x%lx, attribute type 0x%x, " 
if (unlikely(!count)) 
for 
if (ni->type != AT_INDEX_ALLOCATION) { 
if (NInoEncrypted(ni)) { 
for later: Encrypted files are _always_ 
if (NInoCompressed(ni)) { 
for later: If resident, the data is not 
if it 
if (unlikely(NInoTruncateFailed(ni))) { 
if (err || NInoTruncateFailed(ni)) { 
form write to inode " 
if (end > ll) { 
if (likely(ll >= 0)) { 
if (end > ll) { 
if possible or fail. */ 
form write to " 
if (pos > ll) { 
if (err < 0) { 
form write to inode " 
for non-resident 
if (vol->cluster_size > PAGE_CACHE_SIZE && NInoNonResident(ni)) 
form the actual write. */ 
if (likely(nr_segs == 1)) 
if (nr_pages > 1) { 
if (vcn != last_vcn) { 
if (unlikely(lcn < LCN_HOLE)) { 
if (lcn == LCN_ENOMEM) 
form write to " 
if (lcn == LCN_HOLE) { 
if (bytes > count) 
if (likely(nr_segs == 1)) 
if (unlikely(status)) 
if (NInoNonResident(ni)) { 
for_non_resident_write( 
if (unlikely(status)) { 
while (do_pages); 
if (pos + bytes > i_size) { 
if (likely(nr_segs == 1)) { 
if (likely(!status)) { 
if (unlikely(copied != bytes)) 
while (do_pages); 
while (count); 
if (cached_page) 
if (err) 
if (!count) 
if (err) 
if (err) 
if (ret > 0) { 
if (err < 0) 
if non-zero only flush user data and not metadata 
for fsync, fdatasync, and msync 
if @datasync is true, we do not wait on the inode to be written out 
for now. 
for inode 0x%lx.", vi->i_ino); 
if (err) 
if (!datasync || !NInoNonResident(NTFS_I(vi))) 
for dirty blocks then we could optimize the below to be 
if (unlikely(err && !ret)) 
if (likely(!ret)) 
if /* NTFS_RW */ 
ifdef NTFS_RW 
for 
for 
if /* NTFS_RW */ 
form function on the 
ifdef NTFS_RW 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/index.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
if allocation failed. 
if (ictx) 
if (ictx->entry) { 
if (ictx->actx) 
if (ictx->base_ni) 
if (page) { 
for which to search in the index 
fore calling ntfs_index_lookup(), @ictx must have been obtained from a 
ified by the index lookup context @ictx. 
for the @key. 
ified, call flush_dcache_index_entry_page() 
fore the call to ntfs_index_ctx_put() to 
fore writing out and then 
if (!ntfs_is_collation_rule_supported( 
for the index inode. */ 
if (IS_ERR(m)) { 
if (unlikely(!actx)) { 
if (unlikely(err)) { 
ified in read_inode). */ 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if ((u8*)ie < (u8*)actx->mrec || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
if ((u32)sizeof(INDEX_ENTRY_HEADER) + 
if ((key_len == le16_to_cpu(ie->key_length)) && !memcmp(key, 
fore the key of the current entry, there 
if (rc == -1) 
if (!rc) 
for the 
if (!(ie->flags & INDEX_ENTRY_NODE)) { 
ify that an index allocation exists. */ 
if necessary. 
if (IS_ERR(page)) { 
if ((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_CACHE_SIZE) { 
if (unlikely(!ntfs_is_indx_record(ia->magic))) { 
if (sle64_to_cpu(ia->index_block_vcn) != vcn) { 
ifferent from expected VCN (0x%llx).  Inode " 
if (le32_to_cpu(ia->index.allocated_size) + 0x18 != 
iffering from the index " 
if (index_end > kaddr + PAGE_CACHE_SIZE) { 
if (index_end > (u8*)ia + idx_ni->itype.index.block_size) { 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if ((u8*)ie < (u8*)ia || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
if ((u32)sizeof(INDEX_ENTRY_HEADER) + 
if ((key_len == le16_to_cpu(ie->key_length)) && !memcmp(key, 
fore the key of the current entry, there 
if (rc == -1) 
if (!rc) 
for 
if (!(ie->flags & INDEX_ENTRY_NODE)) { 
if ((ia->index.flags & NODE_MASK) == LEAF_NODE) { 
if (vcn >= 0) { 
if (old_vcn << vol->cluster_size_bits >> 
if (!err) 
if (actx) 
if (m) 
file : ./test/kernel/fs/ntfs/compress.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for the decompression engine 
if the allocations failed. 
if (!ntfs_compression_buffer) 
if (((s64)page->index << PAGE_CACHE_SHIFT) >= initialized_size) { 
for now there is no problem. 
for&handle out of bounds compressed page 
if ((page->index >= (initialized_size >> PAGE_CACHE_SHIFT)) && 
if none) (IN) 
ified. 
if success or -EOVERFLOW on error in the compressed stream. 
for uncompressed data / destination. */ 
for tag and token parsing. */ 
for the eight tokens in tag. */ 
if the current 
fore its end so the 
if (cb == cb_end || !le16_to_cpup((le16*)cb) || 
if (nr_completed_pages > 0) { 
for (i = 0; i < nr_completed_pages; i++) { 
if (di == xpage) 
for the current sub-block destination. */ 
if (*dest_index == dest_max_index && do_sb_end > dest_max_ofs) 
if (cb + 6 > cb_end) 
if (cb_sb_end > cb_end) 
if (!dp) { 
if (!*dest_ofs && (++*dest_index > dest_max_index)) 
if (!(le16_to_cpup((le16*)cb) & NTFS_SB_IS_COMPRESSED)) { 
if (cb_sb_end - cb != NTFS_SB_SIZE) 
if (!(*dest_ofs &= ~PAGE_CACHE_MASK)) { 
if (++*dest_index > dest_max_index) 
if (cb == cb_sb_end) { 
if (dp_addr < dp_sb_end) { 
if (!(*dest_ofs &= ~PAGE_CACHE_MASK)) 
if (cb > cb_sb_end || dp_addr > dp_sb_end) 
for (token = 0; token < 8; token++, tag >>= 1) { 
if we are done / still in range. */ 
if ((tag & NTFS_TOKEN_MASK) == NTFS_SYMBOL_TOKEN) { 
if (dp_addr == dp_sb_start) 
for (i = *dest_ofs - do_sb_start - 1; i >= 0x10; i >>= 1) 
if (dp_back_addr < dp_sb_start) 
ify it is in range. */ 
if (*dest_ofs > do_sb_end) 
if (length <= max_non_overlap) { 
for the 
while (length--) 
ified to be locked and the 
for the next compression 
if we were to just overwrite 
if pages are above 8kiB and the NTFS volume only uses 512 byte 
for PAGE_CACHE_SIZE > cb_size we are screwing up both in 
if the two sizes are not equal for a 
if we get here for anything that is not an 
if (unlikely(!pages || !bhs)) { 
if (xpage >= max_page) { 
if (nr_pages < max_page) 
for (i = 0; i < max_page; i++, offset++) { 
if (page) { 
if it isn't already read 
if (!PageDirty(page) && (!PageUptodate(page) || 
for (vcn = start_vcn, start_vcn += cb_clusters; vcn < start_vcn; 
if (!rl) { 
if (likely(rl != NULL)) { 
while (rl->length && rl[1].vcn <= vcn) 
if (lcn < 0) { 
if (lcn == LCN_HOLE) 
if (is_retry || lcn != LCN_RL_NOT_MAPPED) 
for the 
if (!ntfs_map_runlist(ni, vcn)) 
if (unlikely(!(bhs[nr_bhs] = sb_getblk(sb, block)))) 
while (++block < max_block); 
if (rl) 
for (i = 0; i < nr_bhs; i++) { 
if (!trylock_buffer(tbh)) 
if (unlikely(buffer_uptodate(tbh))) { 
for io completion on all buffer heads. */ 
if (buffer_uptodate(tbh)) 
if (unlikely(!buffer_uptodate(tbh))) { 
if (unlikely(!buffer_uptodate(tbh))) 
for (i = 0; i < nr_bhs; i++) { 
if (cb_pos + 2 <= cb + cb_size) 
if present) and destination. */ 
for the current cb. */ 
if (cb_max_page > max_page) 
if (vcn == start_vcn - cb_clusters) { 
if (cb_max_ofs) 
for (; cur_page < cb_max_page; cur_page++) { 
if (page) { 
for now there is no problem. 
if (likely(!cur_ofs)) 
if (cur_page == xpage) 
if (cb_pos >= cb_end) 
if (cb_max_ofs && cb_pos < cb_end) { 
if (page) 
if (vcn == start_vcn) { 
fore we read all the pages and use block_read_full_page() 
for the majority of pages. 
if (cb_max_ofs) 
for (; cur_page < cb_max_page; cur_page++) { 
if (page) 
if (cb_pos >= cb_end) 
if (cb_max_ofs && cb_pos < cb_end) { 
if (page) 
for (; cur2_page < cb_max_page; cur2_page++) { 
if (page) { 
if (cur2_page == xpage) 
if (cb_pos2 >= cb_end) 
if (err) { 
for (; prev_cur_page < cur_page; prev_cur_page++) { 
if (page) { 
if (prev_cur_page != xpage) 
for (i = 0; i < nr_bhs; i++) 
if (nr_cbs) 
if we have any pages left. Should never happen. */ 
for (cur_page = 0; cur_page < max_page; cur_page++) { 
if (page) { 
if (cur_page != xpage) 
if (likely(xpage_done)) 
while reading compressed data."); 
for (i = 0; i < nr_bhs; i++) 
for (i = cur_page; i < max_page; i++) { 
if (page) { 
if (i != xpage) 
file : ./test/kernel/fs/ntfs/dir.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for the name 
if necessary (see below) 
for an inode with name @uname in the directory with inode @dir_ni. 
format, i.e. it 
if the inode is not found -ENOENT is returned. Note that you 
for being negative, you have to check the 
for a case sensitive match first but we also look for a case 
for the case that we don't find an exact match, where we return 
for how quickly one can access them. This also fixes 
fore writing out and then 
for the directory. */ 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
ified in read_inode). */ 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if ((u8*)ie < (u8*)ctx->mrec || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
if that matches 
for 
if (ntfs_are_names_equal(uname, uname_len, 
fore, so we can 
if the perfect match is a short file name, 
if (ie->key.file_name.file_name_type == FILE_NAME_DOS) { 
if (!name) { 
form a case 
for simplicity). 
if (!NVolCaseSensitive(vol) && 
if (name) { 
if that doesn't find any " 
forge.net."); 
if (type != FILE_NAME_DOS) 
if (!name) { 
if (type != FILE_NAME_DOS) { 
fore the name of the current entry, there 
if (rc == -1) 
if (rc) 
for proper 
if (rc == -1) 
if (rc) 
for the 
if (!(ie->flags & INDEX_ENTRY_NODE)) { 
ify that an index allocation exists. */ 
if necessary. 
if (IS_ERR(page)) { 
if ((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_CACHE_SIZE) { 
if (unlikely(!ntfs_is_indx_record(ia->magic))) { 
if (sle64_to_cpu(ia->index_block_vcn) != vcn) { 
ifferent from expected VCN (0x%llx). " 
if (le32_to_cpu(ia->index.allocated_size) + 0x18 != 
iffering from the " 
if (index_end > kaddr + PAGE_CACHE_SIZE) { 
if (index_end > (u8*)ia + dir_ni->itype.index.block_size) { 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if ((u8*)ie < (u8*)ia || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
if that matches 
for 
if (ntfs_are_names_equal(uname, uname_len, 
fore, so we can 
if the perfect match is a short file name, 
if (ie->key.file_name.file_name_type == FILE_NAME_DOS) { 
if (!name) { 
form a case 
for simplicity). 
if (!NVolCaseSensitive(vol) && 
if (name) { 
if that doesn't find any " 
forge.net."); 
if (type != FILE_NAME_DOS) 
if (!name) { 
if (type != FILE_NAME_DOS) { 
fore the name of the current entry, there 
if (rc == -1) 
if (rc) 
for proper 
if (rc == -1) 
if (rc) 
for 
if (ie->flags & INDEX_ENTRY_NODE) { 
if (vcn >= 0) { 
if (old_vcn << vol->cluster_size_bits >> 
if (name) { 
if (!err) 
if (ctx) 
if (m) 
if (name) { 
if 0 
for the time when we 
for the name 
for an inode with name @uname in the directory with inode @dir_ni. 
format, i.e. it 
if the inode is not found -ENOENT is returned. Note that you 
for being negative, you have to check the 
for the directory. */ 
if (IS_ERR(m)) { 
if (!ctx) { 
if (unlikely(err)) { 
ified in read_inode). */ 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if ((u8*)ie < (u8*)ctx->mrec || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
iffer in case, but 
for consistency checking. We 
if (ntfs_are_names_equal(uname, uname_len, 
fore the name of the current entry, there 
if (rc == -1) 
if (rc) 
for proper 
if (rc == -1) 
if (rc) 
for the 
if (!(ie->flags & INDEX_ENTRY_NODE)) { 
ify that an index allocation exists. */ 
if necessary. 
if (IS_ERR(page)) { 
if ((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_CACHE_SIZE) { 
if (unlikely(!ntfs_is_indx_record(ia->magic))) { 
if (sle64_to_cpu(ia->index_block_vcn) != vcn) { 
ifferent from expected VCN (0x%llx). " 
if (le32_to_cpu(ia->index.allocated_size) + 0x18 != 
iffering from the " 
if (index_end > kaddr + PAGE_CACHE_SIZE) { 
if (index_end > (u8*)ia + dir_ni->itype.index.block_size) { 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if ((u8*)ie < (u8*)ia || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
iffer in case, but 
for consistency checking. We 
if (ntfs_are_names_equal(uname, uname_len, 
fore the name of the current entry, there 
if (rc == -1) 
if (rc) 
for proper 
if (rc == -1) 
if (rc) 
for 
if (ie->flags & INDEX_ENTRY_NODE) { 
if (vcn >= 0) { 
if (old_vcn << vol->cluster_size_bits >> 
if (!err) 
if (ctx) 
if (m) 
if 
ific filldir method 
for the converted name 
if we are returning a non-zero value as ntfs_readdir() 
if (name_type == FILE_NAME_DOS) { 
if (MREF_LE(ie->data.dir.indexed_file) == FILE_root) { 
if (MREF_LE(ie->data.dir.indexed_file) < FILE_first_user && 
if (name_len <= 0) { 
if (ie->key.file_name.file_attributes & 
if (ia_page) 
for %s with len %i, fpos 0x%llx, inode " 
if (!dir_emit(actor, name, name_len, mref, dt_type)) 
if we are aborting ->readdir. */ 
for 
ifications). 
fore writing out and then 
for inode 0x%lx, fpos 0x%llx.", 
if (actor->pos >= i_size + vol->mft_record_size) 
for all directories. */ 
format determined by current NLS. 
if (unlikely(!name)) { 
if (actor->pos >= vol->mft_record_size) 
for the directory. */ 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
fore calling the 
for us to 
if (unlikely(!ir)) { 
ified in read_inode). */ 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if (unlikely((u8*)ie < (u8*)ir || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
if continuing previous readdir. */ 
if going to skip the entry. */ 
if (rc) { 
if (!NInoIndexAllocPresent(ndir)) 
if (IS_ERR(bmp_vi)) { 
if (unlikely(bmp_pos >> 3 >= i_size_read(bmp_vi))) { 
if (IS_ERR(bmp_page)) { 
while (!(bmp[cur_bmp_pos >> 3] & (1 << (cur_bmp_pos & 7)))) { 
if (unlikely((cur_bmp_pos >> 3) >= PAGE_CACHE_SIZE)) { 
if (unlikely(((bmp_pos + cur_bmp_pos) >> 3) >= i_size)) 
if ((prev_ia_pos & (s64)PAGE_CACHE_MASK) != 
if (likely(ia_page != NULL)) { 
if necessary. 
if (IS_ERR(ia_page)) { 
if (unlikely((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_CACHE_SIZE)) { 
if (unlikely(!ntfs_is_indx_record(ia->magic))) { 
if (unlikely(sle64_to_cpu(ia->index_block_vcn) != (ia_pos & 
ifferent from expected VCN (0x%llx). " 
if (unlikely(le32_to_cpu(ia->index.allocated_size) + 0x18 != 
iffering from the " 
if (unlikely(index_end > kaddr + PAGE_CACHE_SIZE)) { 
if (unlikely(index_end > (u8*)ia + ndir->itype.index.block_size)) { 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if (unlikely((u8*)ie < (u8*)ia || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
if continuing previous readdir. */ 
if going to skip the entry. */ 
fore returning, unless a non-zero value is returned in 
if (rc) { 
if (ia_page) { 
if (bmp_page) { 
if (ia_page) { 
if (ctx) 
if (m) 
if (!err) 
for now without overflowing the 
if (sizeof(unsigned long) < 8) { 
ifdef NTFS_RW 
if non-zero only flush user data and not metadata 
for fsync, fdatasync, and 
if it is present 
for a directory so things are not too bad. 
for inode 0x%lx.", vi->i_ino); 
if (err) 
if (bmp_vi) { 
if (unlikely(err && !ret)) 
if (likely(!ret)) 
if /* NTFS_RW */ 
ifdef NTFS_RW 
if /* NTFS_RW */ 
form function on the 
file : ./test/kernel/fs/ntfs/inode.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for equality 
ific part of the vfs 
for equality with the ntfs attribute @na. 
if the attributes match and 0 if not. 
if (vi->i_ino != na->mft_no) 
if (likely(!NInoAttr(ni))) { 
for a normal inode this is a mismatch. */ 
if (ni->type != na->type) 
if (ni->name_len != na->name_len) 
if (na->name_len && memcmp(ni->name, na->name, 
if (na->type == AT_INDEX_ALLOCATION) 
if (likely(na->type == AT_UNUSED)) { 
if (na->name_len && na->name != I30) { 
if (!ni->name) 
ific normal inode 
ific normal inode (i.e. a 
if true, the function failed and the error code is obtained from PTR_ERR(). 
if (unlikely(!vi)) 
if (vi->i_state & I_NEW) { 
if the failure was 
if (unlikely(err == -ENOMEM)) { 
if unnamed) 
ified by 
ified by the vfs inode @base_vi. 
for index allocation attributes, you need to use ntfs_index_iget() 
if true, the function failed and the error code is 
for indices. */ 
if (unlikely(!vi)) 
if (vi->i_state & I_NEW) { 
ifies things in that we never need to check for bad attribute 
if (unlikely(err)) { 
ified by @name 
if true, the function failed and the error code is 
if (unlikely(!vi)) 
if (vi->i_state & I_NEW) { 
ifies things in that we never need to check for bad index 
if (unlikely(err)) { 
if (likely(ni != NULL)) { 
if (!atomic_dec_and_test(&ni->count)) 
if (likely(ni != NULL)) { 
if (!atomic_dec_and_test(&ni->count)) 
ific part of an inode 
while the base inode 
for nested inode's mrec_lock's: 
if (likely(ni != NULL)) { 
if a file is in the $Extend directory 
if any of the names are in the $Extend system 
if the file is in the $Extend directory 
while (!(err = ntfs_attr_lookup(AT_FILE_NAME, NULL, 0, 0, 0, NULL, 0, 
if (p < (u8*)ctx->mrec || (u8*)p > (u8*)ctx->mrec + 
if (attr->non_resident) { 
if (attr->flags) { 
if (!(attr->data.resident.flags & RESIDENT_ATTR_IS_INDEXED)) { 
if (p2 < (u8*)attr || p2 > p) 
if (MREF_LE(file_name_attr->parent_directory) == FILE_Extend) 
if (unlikely(err != -ENOENT)) 
if (unlikely(nr_links)) { 
for reading and sets up the necessary @vi fields as well as initializing 
for i_ino 0x%lx.", vi->i_ino); 
for checking whether an inode has changed w.r.t. a file so 
ific part of @vi special casing 
if (vi->i_ino != FILE_MFT) 
if (IS_ERR(m)) { 
if (!ctx) { 
if (!(m->flags & MFT_RECORD_IN_USE)) { 
if (m->base_mft_record) { 
formation from mft record into vfs and ntfs inodes. */ 
for files which have both 
for the short file names by subtracting them or we need 
for now. 
for now. 
if (IS_RDONLY(vi)) 
if (m->flags & MFT_RECORD_IS_DIRECTORY) { 
if (vi->i_nlink > 1) 
formation attribute in the mft record. At this 
if the standard information is in an extent record, but 
if (unlikely(err)) { 
if the 
formation attribute value. */ 
formation from the standard information into vi. */ 
for example but changed whenever the file is written to. 
if present. */ 
if (err) { 
if (!err) */ { 
if (a->flags & ATTR_COMPRESSION_MASK) { 
if (a->flags & ATTR_IS_ENCRYPTED || 
if (a->non_resident) { 
for the attribute list. */ 
if (!ni->attr_list) { 
for attribute list."); 
if (a->non_resident) { 
if (a->data.non_resident.lowest_vcn) { 
for locking as we have 
if (IS_ERR(ni->attr_list_rl.rl)) { 
if ((err = load_attribute_list(vol, &ni->attr_list_rl, 
if (!a->non_resident) */ { 
if (S_ISDIR(vi->i_mode)) { 
if (unlikely(err)) { 
if recovery option is 
if (unlikely(a->non_resident)) { 
fore the value. */ 
if (a->flags & ATTR_COMPRESSION_MASK) 
if (a->flags & ATTR_IS_ENCRYPTED) { 
if (a->flags & ATTR_IS_SPARSE) 
if (ir_end > (u8*)ctx->mrec + vol->mft_record_size) { 
if (index_end > ir_end) { 
if (ir->type != AT_FILE_NAME) { 
if (ir->collation_rule != COLLATION_FILE_NAME) { 
if (ni->itype.index.block_size & 
if (ni->itype.index.block_size > PAGE_CACHE_SIZE) { 
if (ni->itype.index.block_size < NTFS_BLOCK_SIZE) { 
if (vol->cluster_size <= ni->itype.index.block_size) { 
if not present. */ 
if (!(ir->index.flags & LARGE_INDEX)) { 
if (unlikely(err)) { 
if (!a->non_resident) { 
fore the mapping pairs 
if (unlikely(a->name_length && (le16_to_cpu(a->name_offset) >= 
if (a->flags & ATTR_IS_ENCRYPTED) { 
if (a->flags & ATTR_IS_SPARSE) { 
if (a->flags & ATTR_COMPRESSION_MASK) { 
if (a->data.non_resident.lowest_vcn) { 
if (IS_ERR(bvi)) { 
if (NInoCompressed(bni) || NInoEncrypted(bni) || 
if ((bvi_size << 3) < (vi->i_size >> 
for index allocation (0x%llx).", 
for this inode. */ 
if not present. */ 
if (unlikely(err)) { 
if (err != -ENOENT) { 
if (vi->i_ino == FILE_Secure) 
if not all the system files in the $Extend 
if the parent 
if (ntfs_is_extended_system_file(ctx) > 0) 
if recovery option is set. 
if (a->flags & (ATTR_COMPRESSION_MASK | ATTR_IS_SPARSE)) { 
if (vol->cluster_size > 4096) { 
if ((a->flags & ATTR_COMPRESSION_MASK) 
if (a->flags & ATTR_IS_SPARSE) 
if (a->flags & ATTR_IS_ENCRYPTED) { 
if (a->non_resident) { 
if (NInoCompressed(ni) || NInoSparse(ni)) { 
if (a->data.non_resident.compression_unit) { 
if (a->data.non_resident.lowest_vcn) { 
if (vi->i_size > ni->allocated_size) { 
for this inode. */ 
if (NInoMstProtected(ni)) 
for stat). This is in so 
if not entirely 
ificant slowdown as it would involve iterating over all 
if (S_ISREG(vi->i_mode) && (NInoCompressed(ni) || NInoSparse(ni))) 
if (!err) 
if (ctx) 
if (m) 
if (err != -EOPNOTSUPP && err != -ENOMEM) 
for 
for AT_INDEX_ALLOCATION. 
for i_ino 0x%lx.", vi->i_ino); 
if (IS_ERR(m)) { 
if (!ctx) { 
if (unlikely(err)) 
if (a->flags & (ATTR_COMPRESSION_MASK | ATTR_IS_SPARSE)) { 
if ((ni->type != AT_DATA) || (ni->type == AT_DATA && 
forge.net"); 
if (vol->cluster_size > 4096) { 
if ((a->flags & ATTR_COMPRESSION_MASK) != 
if (NInoMstProtected(ni) && ni->type != AT_INDEX_ROOT) { 
forge.net", 
if (a->flags & ATTR_IS_SPARSE) 
if (a->flags & ATTR_IS_ENCRYPTED) { 
if (NInoMstProtected(ni) && ni->type != AT_INDEX_ROOT) { 
forge." 
if (ni->type != AT_DATA) { 
if (!a->non_resident) { 
fore the value. */ 
if (NInoMstProtected(ni)) { 
forge.net"); 
if (vi->i_size > ni->allocated_size) { 
fore the mapping pairs 
if (unlikely(a->name_length && (le16_to_cpu(a->name_offset) >= 
if (NInoCompressed(ni) || NInoSparse(ni)) { 
if (a->data.non_resident.compression_unit) { 
if (a->data.non_resident.lowest_vcn) { 
if (NInoMstProtected(ni)) 
if ((NInoCompressed(ni) || NInoSparse(ni)) && ni->type != AT_INDEX_ROOT) 
if (!err) 
if (ctx) 
while reading attribute " 
if (err != -ENOMEM) 
for 
fore setting up the necessary fields in @vi as well as initializing the 
for small indices the index allocation attribute might not actually exist. 
for directories, we need to have an attribute inode for 
for 
for i_ino 0x%lx.", vi->i_ino); 
for the base inode. */ 
if (IS_ERR(m)) { 
if (!ctx) { 
if (unlikely(err)) { 
if (unlikely(a->non_resident)) { 
fore the value. */ 
for 
if (a->flags & (ATTR_COMPRESSION_MASK | ATTR_IS_ENCRYPTED | 
if (ir_end > (u8*)ctx->mrec + vol->mft_record_size) { 
if (index_end > ir_end) { 
if (ir->type) { 
if (!is_power_of_2(ni->itype.index.block_size)) { 
if (ni->itype.index.block_size > PAGE_CACHE_SIZE) { 
if (ni->itype.index.block_size < NTFS_BLOCK_SIZE) { 
if (vol->cluster_size <= ni->itype.index.block_size) { 
for presence of index allocation attribute. */ 
if (unlikely(err)) { 
if (!a->non_resident) { 
fore the mapping pairs array. 
if (unlikely(a->name_length && (le16_to_cpu(a->name_offset) >= 
if (a->flags & ATTR_IS_ENCRYPTED) { 
if (a->flags & ATTR_IS_SPARSE) { 
if (a->flags & ATTR_COMPRESSION_MASK) { 
if (a->data.non_resident.lowest_vcn) { 
if (IS_ERR(bvi)) { 
if (NInoCompressed(bni) || NInoEncrypted(bni) || 
if ((bvi_size << 3) < (vi->i_size >> ni->itype.index.block_size_bits)) { 
for " 
for this index inode. */ 
if (!err) 
if (ctx) 
if (m) 
while reading index " 
if (err != -EOPNOTSUPP && err != -ENOMEM) 
for mount time use only 
for $MFT/$DATA 
fore anything 
formation for the next step to complete. 
if they are real pits or just smoke... 
ific part of @vi. */ 
for directories. 
if (vol->mft_record_size > 64 * 1024) { 
if (i < sb->s_blocksize) 
if (!m) { 
for $MFT record 0."); 
if (!nr_blocks) 
for (i = 0; i < nr_blocks; i++) { 
if (!bh) { 
if (post_read_mst_fixup((NTFS_RECORD*)m, vol->mft_record_size)) { 
for map_mft_record(). */ 
if (!ctx) { 
if present. */ 
if (err) { 
if (!err) */ { 
if (a->flags & ATTR_COMPRESSION_MASK) { 
if (a->flags & ATTR_IS_ENCRYPTED || 
if (a->non_resident) { 
for the attribute list. */ 
if (!ni->attr_list) { 
for attribute list."); 
if (a->non_resident) { 
if (a->data.non_resident.lowest_vcn) { 
if (IS_ERR(ni->attr_list_rl.rl)) { 
if ((err = load_attribute_list(vol, &ni->attr_list_rl, 
if (!ctx.attr->non_resident) */ { 
if this case is actually possible. 
form a manual search and make sure the first $MFT/$DATA 
if we ever see a report of this error we will need 
for (;; al_entry = next_al_entry) { 
if ((u8*)al_entry < ni->attr_list || 
if ((u8*)al_entry == al_end) 
if (!al_entry->length) 
if ((u8*)al_entry + 6 > al_end || (u8*)al_entry + 
if (le32_to_cpu(al_entry->type) > le32_to_cpu(AT_DATA)) 
if (AT_DATA != al_entry->type) 
if (al_entry->name_length) 
if (al_entry->lowest_vcn) 
if (MREF_LE(al_entry->mft_reference) != vi->i_ino) { 
forge.net"); 
if (MSEQNO_LE(al_entry->mft_reference) != 
while (!(err = ntfs_attr_lookup(AT_DATA, NULL, 0, 0, next_vcn, NULL, 0, 
if (!a->non_resident) { 
if (a->flags & ATTR_COMPRESSION_MASK || 
for locking 
if (IS_ERR(nrl)) { 
if (!next_vcn) { 
ify the number of mft records does not exceed 
if ((vi->i_size >> vol->mft_record_size_bits) >= 
for 
for $MFT, this time entering 
if 
if (is_bad_inode(vi)) { 
if no errors " 
forge.net"); 
ifics about $MFT's inode as 
for anyone. */ 
for $MFT. */ 
for the next extent. */ 
if (next_vcn <= 0) 
if (next_vcn < sle64_to_cpu( 
if (err != -ENOENT) { 
if (!a) { 
if (highest_vcn && highest_vcn != last_vcn - 1) { 
for " 
if (ni->runlist.rl) { 
if (ni->attr_list) { 
if (ni->attr_list_rl.rl) { 
if (ni->name_len && ni->name != I30) { 
for inode 0x%lx.", ni->mft_no); 
ifdef NTFS_RW 
if (!is_bad_inode(VFS_I(ni->ext.base_ntfs_ino))) 
if /* NTFS_RW */ 
ific part of an inode 
ific part 
fore doing anything else. 
ifdef NTFS_RW 
if (!was_bad && (is_bad_inode(vi) || NInoDirty(ni))) { 
if /* NTFS_RW */ 
if (ni->nr_extents > 0) { 
for (i = 0; i < ni->nr_extents; i++) 
if (NInoAttr(ni)) { 
if (ni->nr_extents == -1) { 
for each mounted ntfs volume when someone reads 
ified by @root are written to the seq file 
if (vol->fmask == vol->dmask) 
if (NVolCaseSensitive(vol)) 
if (NVolShowSystemFiles(vol)) 
if (!NVolSparseEnabled(vol)) 
for (i = 0; on_errors_arr[i].val; i++) { 
ifdef NTFS_RW 
for which the i_size was changed 
forced in ntfs_setattr(), see 
for us that @vi is a file inode rather than a directory, index, 
for inode 0x%lx.", vi->i_ino); 
for writing and map the mft record to ensure it is 
if (!NInoAttr(ni)) 
if (IS_ERR(m)) { 
for inode 0x%lx " 
if (unlikely(!ctx)) { 
for " 
if (unlikely(err)) { 
for the attribute value. 
if (NInoNonResident(ni)) 
if no change, >0 if the 
if (new_size - old_size >= 0) { 
if (new_size == old_size) 
for the allocated size. */ 
if (new_alloc_size - old_alloc_size >= 0) { 
if (new_alloc_size == old_alloc_size) 
if (!size_change && !alloc_change) 
if new size is allowed in $AttrDef. */ 
if (unlikely(err)) { 
for its attribute type " 
if (NInoCompressed(ni) || NInoEncrypted(ni)) { 
for %s files, ignoring.", 
if (a->non_resident) 
if (new_size < vol->mft_record_size && 
ifies that the behaviour is unspecified thus we do not 
ify that 
for us as we cannot lock the 
if that fails dropping 
for any given file. 
if successful restart the truncation process. 
if (likely(!err)) 
for this attribute type or there not being enough space, 
if (unlikely(err != -EPERM && err != -ENOSPC)) { 
if (err != -ENOMEM) 
if (err == -ENOSPC) 
for the non-resident attribute value.  " 
if (err == -EPERM) */ 
if 0 
if (!err) 
formation 
if (ni->type == AT_ATTRIBUTE_LIST || 
if (!err) 
if it is not already the only attribute in an mft record in 
if (!err) 
if 
if (alloc_change < 0) { 
if (highest_vcn > 0 && 
fore reducing the allocation. 
if (size_change < 0) { 
if (new_size < ni->initialized_size) { 
if (!alloc_change) 
for the 
if (size_change >= 0) */ { 
if (alloc_change > 0) { 
if the new 
ified as explained above for the resident 
if (!alloc_change) 
if (unlikely(nr_freed < 0)) { 
if (unlikely(err || IS_ERR(m))) { 
for the shrunk mapping pairs array for the runlist. */ 
if (unlikely(mp_size <= 0)) { 
for the mapping pairs failed with error " 
for the new mapping pairs array.  Note, 
if (unlikely(err)) { 
if (NInoSparse(ni) || NInoCompressed(ni)) { 
fore the allocation change). 
if (size_change > 0) 
ified mft record is written out. */ 
fore so it got a copy & paste version 
for real. 
if (!IS_NOCMTIME(VFS_I(base_ni)) && !IS_RDONLY(VFS_I(base_ni))) { 
if (!timespec_equal(&VFS_I(base_ni)->i_mtime, &now) || 
if (sync_it) 
if (likely(!err)) { 
if (err != -ENOMEM && err != -EOPNOTSUPP) 
if (err != -EOPNOTSUPP) 
if (old_size >= 0) 
if (ctx) 
if (m) 
if (err != -ENOMEM && err != -EOPNOTSUPP) 
if (err != -EOPNOTSUPP) 
for ntfs_truncate() that has no return value 
for ntfs_truncate() that has no return value. 
ifdef NTFS_RW 
if 
ify_change() when an attribute is being changed 
if (err) 
if (ia_valid & (ATTR_UID | ATTR_GID | ATTR_MODE)) { 
if (ia_valid & ATTR_SIZE) { 
if (NInoCompressed(ni) || NInoEncrypted(ni)) { 
for " 
if (err || ia_valid == ATTR_SIZE) 
if (ia_valid & ATTR_ATIME) 
if (ia_valid & ATTR_MTIME) 
if (ia_valid & ATTR_CTIME) 
if true, write out synchronously 
for io completion.  This 
for i/o 
forms synchronous writes. 
ified = false; 
for %sinode 0x%lx.", NInoAttr(ni) ? "attr " : "", 
if (NInoAttr(ni)) { 
if (IS_ERR(m)) { 
formation attribute. */ 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if they have changed. */ 
if (si->last_data_change_time != nt) { 
for inode 0x%lx: old = 0x%llx, " 
ified = true; 
if (si->last_mft_change_time != nt) { 
for inode 0x%lx: old = 0x%llx, " 
ified = true; 
if (si->last_access_time != nt) { 
for inode 0x%lx: old = 0x%llx, " 
ified = true; 
ified the standard information attribute we need to 
ified so it 
for $MFT itself is being 
fore 
if (modified) { 
if (!NInoTestSetDirty(ctx->ntfs_ino)) 
if (NInoDirty(ni)) 
if (ni->nr_extents > 0) { 
for (i = 0; i < ni->nr_extents; i++) { 
if (NInoDirty(tni)) { 
if (IS_ERR(tm)) { 
if (unlikely(ret)) { 
if (unlikely(err)) 
if (err == -ENOMEM) { 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/namei.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for the inode 
for the inode represented by the dentry @dent 
ifies which inode to look for by 
for the converted Unicode name. If the name is found in the 
if an actual error occurs, do we return an error via ERR_PTR(). 
iffer in case in ->ntfs_lookup() while maintaining 
for any other case (or for the short file 
for a fully matching file name 
ifferent case and if that has non-POSIX semantics we return 
ify matters for us, we do not treat the short vs long filenames as 
for the corresponding long filename instead. 
if a dentry with this name already exists 
for a dentry with this name, etc, as in case 2), above. 
if (uname_len < 0) { 
if (!IS_ERR_MREF(mref)) { 
if (likely(!IS_ERR(dent_inode))) { 
if (is_bad_inode(dent_inode) || MSEQNO(mref) == 
if (!name) { 
if (MREF_ERR(mref) == -ENOENT) { 
if (name->type != FILE_NAME_DOS) {			/* Case 2. */ 
if (name->type == FILE_NAME_DOS) */ {		/* Case 3. */ 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (err == -ENOENT) 
if (a->non_resident || a->flags) 
if (le16_to_cpu(a->data.resident.value_offset) + 
if ((u32)(fn->file_name_length * sizeof(ntfschar) + 
while (fn->file_name_type != FILE_NAME_WIN32); 
if a conversion error occurred. */ 
if (ctx) 
if (m) 
for directories. 
ified by the 
for inode 0x%lx.", vi->i_ino); 
if (IS_ERR(mrec)) 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (err == -ENOENT) 
if (unlikely(attr->non_resident)) 
if (unlikely((u8 *)fn + le32_to_cpu(attr->data.resident.value_length) > 
if (!IS_ERR(inode)) { 
for now.  Note that they 
for now we will ignore the 
for the system file $MFT 
file : ./test/kernel/fs/ntfs/logfile.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifdef NTFS_RW 
for consistency 
if it is 
if (logfile_system_page_size < NTFS_BLOCK_SIZE || 
if (pos && pos != logfile_system_page_size) { 
if (sle16_to_cpu(rp->major_ver) != 1 || 
if (ntfs_is_chkd_record(rp->magic) && !le16_to_cpu(rp->usa_count)) { 
ify the size of the update sequence array. */ 
if (usa_count != le16_to_cpu(rp->usa_count)) { 
ify the position of the update sequence array. */ 
if (usa_ofs < sizeof(RESTART_PAGE_HEADER) || 
ifies " 
ify the position of the restart area.  It must be: 
if (ra_ofs & 7 || (have_usa ? ra_ofs < usa_end : 
ifies " 
ified by chkdsk are allowed to have chkdsk_lsn 
if (!ntfs_is_chkd_record(rp->magic) && sle64_to_cpu(rp->chkdsk_lsn)) { 
ified."); 
for consistency 
for consistency and return 
fore ra->file_size must be before the first word 
if (ra_ofs + offsetof(RESTART_AREA, file_size) > 
ifies " 
fore the first word protected by an 
if (((ca_ofs + 7) & ~7) != ca_ofs || 
ifies " 
ified by ra->restart_area_length. 
if (ra_ofs + ra_len > le32_to_cpu(rp->system_page_size) || 
ified by the " 
if ((ra->client_free_list != LOGFILE_NO_CLIENT && 
ifies " 
for consistency. 
while (file_size) { 
if (le32_to_cpu(ra->seq_number_bits) != 67 - fs_bits) { 
if (((le16_to_cpu(ra->log_record_header_length) + 7) & ~7) != 
ifies " 
for the log page data offset. */ 
ifies " 
for consistency 
for consistency and 
for (idx_is_first = true; idx != LOGFILE_NO_CLIENT_CPU; nr_clients--, 
if (!nr_clients || idx >= le16_to_cpu(ra->log_clients)) 
if (idx_is_first) { 
if we just did the free list. */ 
for consistency 
if it is consistent 
if @lsn is not NULL, on success *@lsn will be set to the current 
for consistency. */ 
for consistency. */ 
if (!trp) { 
for $LogFile " 
if (size >= le32_to_cpu(rp->system_page_size)) { 
if (IS_ERR(page)) { 
if (err != -EIO && err != -ENOMEM) 
while (to_read > 0); 
if the 
if ((!ntfs_is_chkd_record(trp->magic) || le16_to_cpu(trp->usa_count)) 
if the restart page contents exceed the multi sector 
if (le16_to_cpu(rp->restart_area_offset) + 
ified by chkdsk or there are no active 
for consistency, too. 
if (ntfs_is_rstr_record(rp->magic) && 
if (!ntfs_check_log_client_array(vi, trp)) { 
if (lsn) { 
if (ntfs_is_chkd_record(rp->magic)) */ 
if (wrp) 
for consistency 
if it is 
if the $LogFile was created on a system with a different page size to ours 
fore it got emptied. */ 
if (size > MaxLogFileSize) 
if the page cache size is between the default log page 
if (PAGE_CACHE_SIZE >= DefaultLogPageSize && PAGE_CACHE_SIZE <= 
if (size < log_page_size * 2 || (size - log_page_size * 2) >> 
for a restart page.  Since the restart 
for each page size) rather 
for (pos = 0; pos < size; pos <<= 1) { 
if (!page || page->index != idx) { 
if (IS_ERR(page)) { 
while an 
if (!ntfs_is_empty_recordp((le32*)kaddr)) 
if (!logfile_is_empty) 
if (ntfs_is_rcrd_recordp((le32*)kaddr)) 
ified by chkdsk) restart page, continue. */ 
if (!pos) 
ified by chkdsk) restart page for consistency 
if (!err) { 
ified by chkdsk) 
for the second one. 
if (!pos) { 
ified by chkdsk) 
if the restart page was invalid as we might still 
if (err != -EINVAL) { 
if (!pos) 
if (page) 
if (logfile_is_empty) { 
if (!rstr1_ph) { 
if (rstr2_ph) { 
if (rstr2_lsn > rstr1_lsn) { 
if (rp) 
if (rstr1_ph) 
if the volume is clean 
if it indicates the volume was 
for the five seconds preceding the unclean shutdown. 
if the $LogFile 
fore it got emptied. */ 
if (!ntfs_is_rstr_record(rp->magic) && 
fore calling " 
if (ra->client_in_use_list != LOGFILE_NO_CLIENT && 
if (NVolLogFileEmpty(vol)) { 
for the $LogFile/$DATA attribute and 
if (unlikely(!rl || vcn < rl->vcn || !rl->length)) { 
if (err) { 
while (rl->length && vcn >= rl[1].vcn) 
if (unlikely(lcn == LCN_RL_NOT_MAPPED)) { 
if (unlikely(!rl->length || lcn < LCN_HOLE)) 
if (lcn == LCN_HOLE) 
if (rl[1].vcn > end_vcn) 
if (!buffer_uptodate(bh)) 
if (buffer_dirty(bh)) 
for i/o to complete but 
if one buffer worked all of them will work. 
if (should_wait) { 
if (unlikely(!buffer_uptodate(bh))) 
while (++block < end_block); 
if emptying should fail. 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/debug.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifications 
for the mounted ntfs filesystem described 
format string containing 
format vaf; 
ifndef DEBUG 
if 
if (sb) 
ifications 
for the mounted ntfs filesystem described 
format string containing 
format vaf; 
ifndef DEBUG 
if 
if (sb) 
ifdef DEBUG 
format vaf; 
if (!debug_msgs) 
if (function) 
for @rl. */ 
if (!debug_msgs) 
if (!rl) { 
for (i = 0; ; i++) { 
if (lcn < (LCN)0) { 
if (index > -LCN_ENOENT - 1) 
if (!(rl + i)->length) 
if 
file : ./test/kernel/fs/ntfs/usnjrnl.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifdef NTFS_RW 
if (likely(!NVolUsnJrnlStamped(vol))) { 
if (IS_ERR(page)) { 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/lcnalloc.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifdef NTFS_RW 
for writing on entry and is 
if (!rl) 
for (; rl->length; rl++) { 
if (rl->lcn < 0) 
if (unlikely(err && (!ret || ret == -ENOMEM) && ret != err)) 
for the first allocated cluster 
if none) 
if 'true', this is an attribute extension 
if @start_lcn is -1, on the mounted ntfs volume 
for allocation of normal clusters or 
ifies the vcn of the first allocated cluster.  This makes 
if it is 'false', the caller is allocating clusters to fill a 
if 
ified/standard NTFS 1.x 
for the 
for caller supplied hints as to the location 
while, because this allocator should: 1) be a full implementation of 
for speed, but the algorithm is, so further speed improvements are probably 
for the future.  We will just cause 
fort. (AIA) 
if we have only one of 
for now, I am leaving the double logic - 
for writing and 
for start_vcn 0x%llx, count 0x%llx, start_lcn " 
if @count is zero. */ 
for writing. */ 
ific @start_lcn was requested, use the current data zone 
ified position.  If the latter is out of bounds then we start 
for mft zone, 2 for data zone 1 (end of mft zone till end of 
if (zone_start < 0) { 
if (!zone_start) { 
if (zone == DATA_ZONE && zone_start >= vol->mft_zone_start && 
if (zone == MFT_ZONE && (zone_start < vol->mft_zone_start || 
if (!vol->mft_zone_end) 
if (zone == MFT_ZONE) { 
if (zone == DATA_ZONE) */ { 
if (zone_start >= vol->mft_zone_end) { 
while (1) { 
if (last_read_pos > i_size) { 
if (likely(page)) { 
if (IS_ERR(page)) { 
if (unlikely(last_read_pos + buf_size > i_size)) 
fore inner while loop: buf_size %i, lcn 0x%llx, " 
while (lcn < buf_size && lcn + bmp_pos < zone_end) { 
while loop: buf_size %i, " 
if (*byte == 0xff) { 
while loop 1."); 
if (*byte & bit) { 
while loop 2."); 
if needed, including space for 
if ((rlpos + 2) * sizeof(*rl) > rlsize) { 
if (!rl) 
if (unlikely(!rl2)) { 
if adjacent LCNs. 
if (prev_lcn == lcn + bmp_pos - prev_run_len && rlpos) { 
if (likely(rlpos)) { 
if (!--clusters) { 
fore checks, " 
if (tc >= vol->mft_zone_end) { 
if (!vol->mft_zone_end) 
if ((bmp_initial_pos >= 
fore checks, " 
if (tc >= vol->nr_clusters) 
if ((bmp_initial_pos >= 
fore checks, " 
if (tc >= vol->mft_zone_start) 
if (bmp_initial_pos >= 
while loop: buf_size 0x%x, lcn " 
if (bmp_pos < zone_end) { 
while loop, " 
if (pass == 1) { 
if (zone_end < zone_start) 
while loop, pass 2, " 
fore 0x%x, done_zones after 0x%x.", 
if (done_zones < 7) { 
if (rlpos) { 
fore checks, " 
if (tc >= vol->mft_zone_end) { 
if (!vol->mft_zone_end) 
if ((bmp_initial_pos >= 
if (zone_start == vol->mft_zone_end) 
if (zone_start >= zone_end) { 
if (rlpos) { 
fore checks, " 
if (tc >= vol->nr_clusters) 
if ((bmp_initial_pos >= 
if (!zone_start) 
if (zone_start >= zone_end) { 
if (rlpos) { 
fore checks, " 
if (tc >= vol->mft_zone_start) 
if (bmp_initial_pos >= 
if (zone_start == zone_end) { 
while loop."); 
if (zone == MFT_ZONE || mft_zone_size <= 0) { 
if (mft_zone_size > 0) 
if (vol->mft_zone_pos >= vol->mft_zone_end) { 
if (!vol->mft_zone_end) 
while loop.", 
while loop."); 
if (likely(rl)) { 
if (likely(page && !IS_ERR(page))) { 
if (likely(!err)) { 
if (rl) { 
if (err == -ENOSPC) 
if (err2) { 
if (err == -ENOSPC) 
for all clusters 
if this is a rollback operation 
ified, it is an active search context of @ni and its base mft 
ify @ctx as NULL and __ntfs_cluster_free() will 
form the necessary mapping and unmapping. 
fore returning.  Thus, @ctx will be left pointing to the same attribute on 
ifferent memory locations on return, so you must remember to reset any 
for internal use to rollback 
ify the runlist, so you have to 
if 'true' the @ctx 
for 
for writing on entry 
for writing and 
for i_ino 0x%lx, start_vcn 0x%llx, count " 
if not rolling back.  We 
for another use. 
if (likely(!is_rollback)) 
if (IS_ERR(rl)) { 
if (unlikely(rl->lcn < LCN_HOLE)) { 
if (count >= 0 && to_free > count) 
if (likely(rl->lcn >= 0)) { 
if (unlikely(err)) { 
if (count >= 0) 
for (; rl->length && count != 0; ++rl) { 
if (IS_ERR(rl)) { 
if (!is_rollback) 
if (unlikely(rl->lcn < LCN_HOLE)) { 
if (count >= 0 && to_free > count) 
if (likely(rl->lcn >= 0)) { 
if (unlikely(err)) { 
if (count >= 0) 
if (likely(!is_rollback)) 
if (is_rollback) 
if (!real_freed) { 
if that succeeds just return the error code. 
if (delta < 0) { 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/bitmap.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifdef NTFS_RW 
if 'true' this is a rollback operation 
for internal use to rollback 
for i_ino 0x%lx, start_bit 0x%llx, count 0x%llx, " 
for the pages containing the first and last 
if (IS_ERR(page)) { 
ify the appropriate bits in it. */ 
while ((bit & 7) && cnt) { 
if (value) 
if (!cnt) 
ify all remaining whole bytes in the page up 
if (cnt < 8) 
while (index < end_index) { 
if (IS_ERR(page)) 
ify all remaining whole bytes in the 
ify the appropriate bits in it.  Note, @len is the 
if (cnt) { 
while (bit--) { 
ified 
if (is_rollback) 
if (count != cnt) 
if (!pos) { 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/mft.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ific mft record resides 
if that is true PTR_ERR() 
for 
if the volume was that big... 
for $MFT's data. */ 
if (unlikely(index >= end_index)) { 
if (likely(!IS_ERR(page))) { 
if (likely(ntfs_is_mft_recordp((le32*)(page_address(page) + 
while waiting 
fore being 
if 
formed, the page gets PG_uptodate set and PG_locked cleared (this is done 
if 
while we are accessing it. 
fore I/O can proceed. In that case we 
if that is true, PTR_ERR() will return 
for setting the mft record dirty before calling 
ified the mft record... 
for mft_no 0x%lx.", ni->mft_no); 
if (likely(!IS_ERR(m))) 
ific mft record resides 
if highmem is not configured. 
for others to get hold of. We also release the ntfs 
ified the mft record, it is imperative to set the mft 
for mft_no 0x%lx.", ni->mft_no); 
if IS_ERR(result) is false.  Otherwise 
if this extent inode has already been added to the base inode, 
fore returning it. 
if (base_ni->nr_extents > 0) { 
for (i = 0; i < base_ni->nr_extents; i++) { 
if (likely(ni != NULL)) { 
if (likely(!IS_ERR(m))) { 
if (likely(le16_to_cpu(m->sequence_number) == seq_no)) { 
if (unlikely(!ni)) { 
if (IS_ERR(m)) { 
ify the sequence number if it is present. */ 
if needed. */ 
if (unlikely(!tmp)) { 
if (base_ni->nr_extents) { 
if (destroy_ni) 
ifdef NTFS_RW 
ified, 
fore, a 
for inode 0x%lx.", ni->mft_no); 
if (likely(ni->nr_extents >= 0)) 
forge.net and say that you saw " 
for use at umount time when the mft mirror inode has 
while the mft mirror 
if true, wait for i/o completion 
form synchronous i/o and ignore the @sync parameter. 
for inode 0x%lx.", mft_no); 
if (unlikely(!vol->mftmirr_ino)) { 
if (likely(!err)) 
if (IS_ERR(page)) { 
if not present. */ 
while (bh); 
if (block_end <= m_start) 
if (unlikely(block_start >= m_end)) 
if it is not mapped already. */ 
if (!rl) { 
while (rl->length && rl[1].vcn <= vcn) 
if (likely(lcn >= 0)) { 
while (block_start = block_end, (bh = bh->b_this_page) != head); 
if (likely(!err)) { 
for (i_bhs = 0; i_bhs < nr_bhs; i_bhs++) { 
if (!trylock_buffer(tbh)) 
for (i_bhs = 0; i_bhs < nr_bhs; i_bhs++) { 
if (unlikely(!buffer_uptodate(tbh))) { 
if (unlikely(err)) */ { 
for (i_bhs = 0; i_bhs < nr_bhs; i_bhs++) 
if (likely(!err)) { 
while writing mft mirror " 
if true, wait for i/o completion 
if the ntfs inode @ni is dirty and the first 
form synchronous i/o and ignore the @sync parameter. 
for i/o completion, and only then write 
for completion 
if you asked 
for asynchronous writing you probably do not care about that anyway. 
for inode 0x%lx.", ni->mft_no); 
if needed. 
for the mft record @m and the page it is in. 
if (!NInoTestClearDirty(ni)) 
if (block_end <= m_start) 
if (unlikely(block_start >= m_end)) 
if (block_start == m_start) { 
if (!buffer_dirty(bh)) { 
if it is not mapped already. */ 
if (!rl) { 
while (rl->length && rl[1].vcn <= vcn) 
if (likely(lcn >= 0)) { 
while (block_start = block_end, (bh = bh->b_this_page) != head); 
if (!nr_bhs) 
if (unlikely(err)) 
if (err) { 
for (i_bhs = 0; i_bhs < nr_bhs; i_bhs++) { 
if (!trylock_buffer(tbh)) 
if not @sync. */ 
for (i_bhs = 0; i_bhs < nr_bhs; i_bhs++) { 
if (unlikely(!buffer_uptodate(tbh))) { 
if (PageUptodate(page)) 
if (sync && ni->mft_no < vol->mftmirr_size) 
if (unlikely(err)) { 
while writing mft record " 
for (i_bhs = 0; i_bhs < nr_bhs; i_bhs++) 
if (err == -ENOMEM) { 
if an mft record may be written out 
if one is returned 
for unlocking the ntfs inode and unpinning the base 
if the mft record may be written out and 'false' if not. 
form: 
ify the base mft record because Windows 
formation attribute is not in the base 
if we find the lock was already taken, it is not safe to write the mft 
for the lock 
fore the page is locked but we already have the page locked here 
form further checks. 
if the inode 
if it is.  If it is not, we can safely write it and return 'true'. 
for the extent mft record.  We check if it has an 
for the extent mft record is attached to the base inode so we 
for actually writing dirty mft records here and not just 
ified without them ever having actual inodes in memory.  Also we can have 
if we only 
format().  The clean inode can then 
for inode 0x%lx.", mft_no); 
if the inode corresponding to this mft record is in the VFS 
for inode 0x%lx in icache.", mft_no); 
for it rather often. 
if (!mft_no) { 
for the 
if (vi) { 
if (NInoDirty(ni)) { 
if (unlikely(!mutex_trylock(&ni->mrec_lock))) { 
while we hold the mft record lock so 
if it is not a mft record (type "FILE"). */ 
if it is a base inode. */ 
if the inode corresponding to 
for base " 
if (!na.mft_no) { 
if (!vi) { 
if it has the extent inode 
if (ni->nr_extents <= 0) { 
for (eni = NULL, i = 0; i < ni->nr_extents; ++i) { 
if (!eni) { 
if (unlikely(!mutex_trylock(&eni->mrec_lock))) { 
if (NInoTestClearDirty(eni)) 
while we hold the mft record lock so return 
for a free mft record 
for a free mft record in the mft bitmap attribute on the ntfs volume 
for writing. 
for free mft record in the currently " 
if (pass_end > ll) 
if (!base_ni) 
if (data_pos < 24) 
if (data_pos >= pass_end) { 
formatted volume. */ 
for (; pass <= 2;) { 
if (size > ll) 
for a zero bit. 
if (size) { 
if (IS_ERR(page)) { 
fore inner for loop: size 0x%x, " 
for (; bit < size && data_pos + bit < pass_end; 
if (*byte == 0xff) 
if (b < 8 && b >= (bit & 7)) { 
if (unlikely(ll > (1ll << 32))) { 
for loop: size 0x%x, " 
for a zero bit. 
if (data_pos < pass_end) 
if (++pass == 2) { 
if (data_pos >= pass_end) 
for writing. 
fore returning. 
fore returning. 
if (unlikely(IS_ERR(rl) || !rl->length || rl->lcn < 0)) { 
if (!IS_ERR(rl)) 
if (IS_ERR(page)) { 
if (*b != 0xff && !(*b & tb)) { 
if (IS_ERR(rl2)) { 
for " 
if (IS_ERR(rl)) { 
for mft " 
if (ntfs_cluster_free_from_rl(vol, rl2)) { 
for (; rl[1].length; rl++) 
if (IS_ERR(mrec)) { 
if (unlikely(!ctx)) { 
if (unlikely(ret)) { 
if (ret == -ENOENT) 
for the previous last allocated cluster of mft bitmap. */ 
if (ll >= rl2->vcn) 
for the new mapping pairs array for this extent. */ 
if (unlikely(mp_size <= 0)) { 
for mapping pairs failed for " 
if (!ret) 
if necessary. */ 
if (unlikely(ret)) { 
for mft bitmap attribute."); 
if none of 
if (unlikely(ret)) { 
for " 
if (a->data.non_resident.lowest_vcn) { 
if (unlikely(ret)) { 
if (ntfs_attr_lookup(mftbmp_ni->type, mftbmp_ni->name, 
if (status.added_cluster) { 
if (status.added_run) { 
if (ntfs_bitmap_clear_bit(vol->lcnbmp_ino, lcn)) { 
if (status.mp_rebuilt) { 
if (ntfs_attr_record_resize(ctx->mrec, a, old_alen)) { 
if (ctx) 
if (!IS_ERR(mrec)) 
for writing. 
if (IS_ERR(mrec)) { 
if (unlikely(!ctx)) { 
if (unlikely(ret)) { 
if (ret == -ENOENT) 
fore filling the space 
if (mftbmp_ni->initialized_size > old_data_size) { 
if (likely(!ret)) { 
if (IS_ERR(mrec)) { 
if (unlikely(!ctx)) { 
if (ntfs_attr_lookup(mftbmp_ni->type, mftbmp_ni->name, 
if (i_size_read(mftbmp_vi) != old_data_size) { 
ifdef DEBUG 
if /* DEBUG */ 
if not enough space for this by one mft record worth 
for writing. 
fore returning. 
fore returning. 
if (unlikely(IS_ERR(rl) || !rl->length || rl->lcn < 0)) { 
if (!IS_ERR(rl)) 
if (!min_nr) 
if (!nr) 
if (unlikely((ll + (nr << vol->cluster_size_bits)) >> 
if (unlikely((ll + (nr << vol->cluster_size_bits)) >> 
if (likely(!IS_ERR(rl2))) 
if (PTR_ERR(rl2) != -ENOSPC || nr == min_nr) { 
for the " 
fore failing. 
while (1); 
if (IS_ERR(rl)) { 
for mft data " 
if (ntfs_cluster_free_from_rl(vol, rl2)) { 
for (; rl[1].length; rl++) 
if (IS_ERR(mrec)) { 
if (unlikely(!ctx)) { 
if (unlikely(ret)) { 
if (ret == -ENOENT) 
for the previous last allocated cluster of mft bitmap. */ 
if (ll >= rl2->vcn) 
for the new mapping pairs array for this extent. */ 
if (unlikely(mp_size <= 0)) { 
for mapping pairs failed for " 
if (!ret) 
if necessary. */ 
if (unlikely(ret)) { 
for mft data attribute."); 
if (unlikely(ret)) { 
if (a->data.non_resident.lowest_vcn) { 
if (unlikely(ret)) { 
if (ntfs_attr_lookup(mft_ni->type, mft_ni->name, mft_ni->name_len, 
if (ntfs_cluster_free(mft_ni, old_last_vcn, -1, ctx) < 0) { 
if (ntfs_rl_truncate_nolock(vol, &mft_ni->runlist, old_last_vcn)) { 
if (mp_rebuilt && !IS_ERR(ctx->mrec)) { 
if (ntfs_attr_record_resize(ctx->mrec, a, old_alen)) { 
if (IS_ERR(ctx->mrec)) { 
if (ctx) 
if (!IS_ERR(mrec)) 
ifying the mft record number 
ified in NTFS 3.1 so we need to know which volume version this mft 
for mft record 0x%llx.", (long long)mft_no); 
if (vol->major_ver < 3 || (vol->major_ver == 3 && !vol->minor_ver)) 
ific fields while we know that the 
if (vol->mft_record_size >= NTFS_BLOCK_SIZE) 
forge.net stating " 
ified filesystem created was corrupt.  " 
for the termination attribute). 
format - format an mft record on an ntfs volume 
format 
format(const ntfs_volume *vol, const s64 mft_no) 
for mft record 0x%llx.", (long long)mft_no); 
for $MFT's data. */ 
if (unlikely(index >= end_index)) { 
format non-existing mft " 
if (IS_ERR(page)) { 
format 0x%llx.", (long long)mft_no); 
if (unlikely(err)) { 
if an inode is in icache and so on but this is 
if want a file or directory, i.e. base inode or 0 
for extent inodes. 
for a zero bit.  To 
form wrap around 
for storing extension mft records 
for 
while Windows can still create up to 8 small files.  We can start 
if cluster size is above 16kiB.  If there 
if cluster size is above the mft record size. 
for it and return it to the caller, unless 
for use by normal files. 
if required.  The bitmap data size has to be at least equal to the 
if necessary, 
for the allocated mft record, and we will have 
for example for attribute resizing, etc, because when the run list overflows 
formation contained inside them, as 
for finding the mft records, but on 
formatted = false; 
for " 
if (mode) { 
if (!S_ISREG(mode) && !S_ISDIR(mode)) 
if (bit >= 0) { 
if (bit != -ENOSPC) { 
if (old_data_initialized << 3 > ll && old_data_initialized > 3) { 
if (bit < 24) 
if (unlikely(bit >= (1ll << 32))) 
if (unlikely(bit >= (1ll << 32))) 
fore extension: allocated_size 0x%llx, " 
if (old_data_initialized + 8 > old_data_size) { 
if (unlikely(err)) { 
ifdef DEBUG 
if /* DEBUG */ 
if necessary and fill the new space with 
if (unlikely(err)) { 
ifdef DEBUG 
if /* DEBUG */ 
if (unlikely(err)) { 
for writing until all 
for mft bitmap and mft record allocation done 
if (ll <= old_data_initialized) { 
formatted volume is 
fore extension: " 
while (ll > mft_ni->allocated_size) { 
if (unlikely(err)) { 
formatting the mft records allong the way. 
format().  We will update the attribute 
while (ll > mft_ni->initialized_size) { 
if (new_initialized_size > i_size_read(vol->mft_ino)) 
format(vol, mft_no); 
format mft record."); 
formatted = true; 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (IS_ERR(page)) { 
formatted the mft record no need to do it again. */ 
if (ntfs_is_file_record(m->magic) && 
format the mft record, preserving the 
if it is not zero or -1 (0xffff).  This 
if (unlikely(err)) { 
if (seq_no) 
if (usn && le16_to_cpu(usn) != 0xffff) 
if (S_ISDIR(mode)) 
if (base_ni) { 
for the new mft record, 
if (IS_ERR(m_tmp)) { 
if (unlikely(!vi)) { 
for checking whether an inode has changed w.r.t. a 
ific part of @vi. */ 
if (S_ISDIR(mode)) { 
if (vol->cluster_size <= ni->itype.index.block_size) { 
if (IS_RDONLY(vi)) 
formation attribute yet.  Also, there is no need 
for the superblock. */ 
if (ntfs_bitmap_clear_bit(vol->mftbmp_ino, bit)) { 
if something is wrong with it as long as it is properly detached 
for extent inode 0x%lx, base inode 0x%lx.\n", 
if (atomic_read(&ni->count) > 2) { 
for (i = 0; i < base_ni->nr_extents; i++) { 
if (unlikely(err)) { 
if it is not zero. */ 
if (seq_no == 0xffff) 
if (seq_no) 
if (unlikely(err)) { 
if (unlikely(err)) { 
if (!(base_ni->nr_extents & 3)) { 
if (unlikely(!extent_nis)) { 
if (base_ni->nr_extents) { 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/runlist.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
if (likely((dst != src) && (size > 0))) 
if (likely(size > 0)) 
for runlists 
for 
ifferent number of pages in 
if (old_size == new_size) 
if (unlikely(!new_rl)) 
if (likely(rl != NULL)) { 
for runlists 
for 
for as long as it takes to complete the allocation. 
ifferent number of pages in 
if (old_size == new_size) 
if (likely(rl != NULL)) { 
if two runlists can be joined together 
for mergeability with @dst 
if they are misaligned. */ 
if ((dst->vcn + dst->length) != src->vcn) 
if ((dst->lcn >= 0) && (src->lcn >= 0) && 
if ((dst->lcn == LCN_HOLE) && (src->lcn == LCN_HOLE)) 
if they can be merged 
if necessary. Adjust the size of the hole before the 
fore returning so you cannot use 
ified. The following 
if the right hand end needs merging. */ 
if we merged. */ 
if (IS_ERR(dst)) 
ifying the 
if necessary. */ 
if (dst[marker].lcn == LCN_ENOENT) 
fore this element in @dst 
if necessary. Adjust the size of the hole 
fore returning so you cannot use 
ified. The following 
if (loc == 0) 
if (left) 
if we merged, plus 
if (IS_ERR(dst)) 
ifying the 
if (left) 
if @left, then the first run in @src has 
if (dst[marker].lcn == LCN_HOLE || dst[marker].lcn == LCN_RL_NOT_MAPPED) 
if (disc) { 
if necessary. 
fore returning so you cannot use 
ified. The following 
if the left and right ends need merging. */ 
if (loc > 0) 
if the left, right, or both 
for the run being replaced. 
if (delta > 0) { 
if (IS_ERR(dst)) 
ifying the 
if necessary. */ 
if (left) 
for the runs to be copied from @src, i.e. the first 
if @right, then one of @dst's runs is 
if @left, then the first run in @src has 
if (dsize - tail > 0 && dst[marker].lcn == LCN_ENOENT) 
fore returning so you cannot use 
ified. The following 
if (IS_ERR(dst)) 
ifying the 
fore returning so you cannot use 
ified. The following 
ifdef DEBUG 
if 
for silly calling... */ 
if (IS_ERR(srl) || IS_ERR(drl)) 
for the case where the first mapping is being done now. */ 
if necessary. */ 
for (dend = 0; likely(drl[dend].length); dend++) 
if (IS_ERR(drl)) 
while (srl[si].length && srl[si].lcn < LCN_HOLE) 
forward in @drl until we reach the position where @srl needs to 
for (; drl[di].length; di++) { 
for illegal overlaps. */ 
for (send = si; srl[send].length; send++) 
for (dend = di; drl[dend].length; dend++) 
if (srl[send].lcn == LCN_ENOENT) 
for (sfinal = send; sfinal >= 0 && srl[sfinal].lcn < LCN_HOLE; sfinal--) 
for (dfinal = dend; dfinal >= 0 && drl[dfinal].lcn < LCN_HOLE; dfinal--) 
if (finish && !drl[dins].length) 
if (marker && (drl[dins].vcn + drl[dins].length > srl[send - 1].vcn)) 
if 0 
if 
if (finish) 
if (finish) 
if (IS_ERR(drl)) { 
if (marker) { 
for (ds = dend; drl[ds].length; ds++) 
if @srl ended after @drl. */ 
if (drl[ds].vcn == marker_vcn) { 
fore adding the 
if (drl[ds].lcn == LCN_ENOENT) { 
if (drl[ds].lcn != LCN_RL_NOT_MAPPED) { 
if (!slots) { 
if it isn't set already. */ 
if (!slots) 
ified in that case. 
if that is possible (we check for overlap and discard the new 
fore returning ERR_PTR(-ERANGE)). 
ifdef DEBUG 
if (!attr || !attr->non_resident || sle64_to_cpu( 
if 
if (unlikely(buf < (u8*)attr || buf > attr_end)) { 
if (!vcn && !*buf) 
if (unlikely(!rl)) 
if necessary. */ 
while (buf < attr_end && *buf) { 
if needed, including space for the 
if (((rlpos + 3) * sizeof(*old_rl)) > rlsize) { 
if (unlikely(!rl2)) { 
if (b) { 
for (deltaxcn = (s8)buf[b--]; b; b--) 
if (unlikely(deltaxcn < 0)) { 
for 
if (!(*buf & 0xf0)) 
if (buf + b > attr_end) 
for (deltaxcn = (s8)buf[b--]; b > b2; b--) 
ifdef DEBUG 
ified ourselves 
if either is found give us a message so we 
if (vol->major_ver < 3) { 
if (unlikely(lcn == (LCN)-1)) 
if 
if (unlikely(lcn < (LCN)-1)) { 
if (unlikely(buf >= attr_end)) 
ified, it must be equal to the final 
if (unlikely(deltaxcn && vcn - 1 != deltaxcn)) { 
if this is the base extent. */ 
if (deltaxcn) { 
ifference between the highest_vcn and 
if (deltaxcn < max_cluster) { 
if (unlikely(deltaxcn > max_cluster)) { 
ified, we are done. */ 
for overlaps. */ 
if (likely(!IS_ERR(old_rl))) 
for conversion 
for reading or writing). 
if 
if (unlikely(!rl)) 
if (unlikely(vcn < rl[0].vcn)) 
for (i = 0; likely(rl[i].length); i++) { 
if (likely(rl[i].lcn >= (LCN)0)) 
if (likely(rl[i].lcn < (LCN)0)) 
ifdef NTFS_RW 
if @rl is NULL or @vcn is in an unmapped part/out of bounds of 
if (unlikely(!rl || vcn < rl[0].vcn)) 
while (likely(rl->length)) { 
if (likely(rl->lcn >= LCN_HOLE)) 
if (likely(rl->lcn == LCN_ENOENT)) 
ificant_bytes - get number of bytes needed to store a number 
for which to get the number of bytes for 
ific run length. 
ificant_bytes(const s64 n) 
while (l != 0 && l != -1); 
if ((n < 0 && j >= 0) || (n > 0 && j < 0)) 
for_mapping_pairs - get bytes needed for mapping pairs array 
for example allows us to allocate a buffer of the right size when 
for the single terminator byte). 
for reading or writing), it 
for_mapping_pairs(const ntfs_volume *vol, 
if (!rl) { 
while (rl->length && first_vcn >= rl[1].vcn) 
if (unlikely((!rl->length && first_vcn > rl->vcn) || 
if present. */ 
if (unlikely(length < 0 || rl->lcn < LCN_HOLE)) 
if (unlikely(last_vcn >= 0 && rl[1].vcn > last_vcn)) { 
if (unlikely(rl[1].vcn > s1)) 
ificant_bytes(length - delta); 
if (likely(rl->lcn >= 0 || vol->major_ver < 3)) { 
if (likely(rl->lcn >= 0)) 
ificant_bytes(prev_lcn); 
for (; rl->length && !the_end; rl++) { 
if (unlikely(length < 0 || rl->lcn < LCN_HOLE)) 
if (unlikely(last_vcn >= 0 && rl[1].vcn > last_vcn)) { 
if (unlikely(rl[1].vcn > s1)) 
ificant_bytes(length); 
if (likely(rl->lcn >= 0 || vol->major_ver < 3)) { 
ificant_bytes(rl->lcn - 
if (rl->lcn == LCN_RL_NOT_MAPPED) 
ificant_bytes - write the significant bytes of a number 
for bounds checking 
ify @n unambiguously as a signed number, taking care not to exceed 
ific run length to the minimum 
ificant_bytes(s8 *dst, const s8 *dst_max, 
if (unlikely(dst > dst_max)) 
while (l != 0 && l != -1); 
if (n < 0 && j >= 0) { 
if (n > 0 && j < 0) { 
for the ntfs version) 
for which to build the mapping pairs array 
for_mapping_pairs(). 
if @stop_vcn is not NULL, *@stop_vcn is set to 
for reading or writing), it 
if (!rl) { 
if (stop_vcn) 
while (rl->length && first_vcn >= rl[1].vcn) 
if (unlikely((!rl->length && first_vcn > rl->vcn) || 
for bounds checking in 
if present. */ 
if (unlikely(length < 0 || rl->lcn < LCN_HOLE)) 
if (unlikely(last_vcn >= 0 && rl[1].vcn > last_vcn)) { 
if (unlikely(rl[1].vcn > s1)) 
ificant_bytes(dst + 1, dst_max, 
if (unlikely(len_len < 0)) 
if (likely(rl->lcn >= 0 || vol->major_ver < 3)) { 
if (likely(rl->lcn >= 0)) 
ificant_bytes(dst + 1 + 
if (unlikely(lcn_len < 0)) 
if (unlikely(dst_next > dst_max)) 
for (; rl->length && !the_end; rl++) { 
if (unlikely(length < 0 || rl->lcn < LCN_HOLE)) 
if (unlikely(last_vcn >= 0 && rl[1].vcn > last_vcn)) { 
if (unlikely(rl[1].vcn > s1)) 
ificant_bytes(dst + 1, dst_max, 
if (unlikely(len_len < 0)) 
if (likely(rl->lcn >= 0 || vol->major_ver < 3)) { 
ificant_bytes(dst + 1 + 
if (unlikely(lcn_len < 0)) 
if (unlikely(dst_next > dst_max)) 
if (stop_vcn) 
if (rl->lcn == LCN_RL_NOT_MAPPED) 
ified vcn 
for error output) 
if @new_length is 
if the last runlist element is a sparse 
for unmapped runlist elements.  It is assumed that 
for writing. 
for new_length 0x%llx.", (long long)new_length); 
if (!new_length) { 
if (rl) 
if (unlikely(!rl)) { 
if (unlikely(!rl)) { 
while (likely(rl->length && new_length >= rl[1].vcn)) 
if (rl->length) { 
while (likely(trl->length)) 
if (rl->length) { 
if (!rl->length) 
if necessary. */ 
if (IS_ERR(rl)) 
if (likely(/* !rl->length && */ new_length > rl->vcn)) { 
if ((rl > runlist->rl) && ((rl - 1)->lcn == LCN_HOLE)) 
if necessary. */ 
if (IS_ERR(rl)) { 
fore in the old runlist. 
if (unlikely(!rl->length && new_length == rl->vcn)) */ { 
for error output) 
ified. 
for writing. 
for start 0x%llx, length 0x%llx.", 
if (unlikely(!rl)) { 
while (likely(rl->length && start >= rl[1].vcn)) 
while (likely(rl_end->length && end >= rl_end[1].vcn)) { 
if (unlikely(rl_end->lcn < LCN_HOLE)) 
if (unlikely(rl_end->length && rl_end->lcn < LCN_HOLE)) 
if (!rl_end->length && end > rl_end->vcn) 
if (!length) 
if (!rl->length) 
while (likely(rl_real_end->length)) 
if (rl->lcn == LCN_HOLE) { 
if (end <= rl[1].vcn) { 
if (rl_end->lcn == LCN_HOLE) { 
if (rl < rl_end) 
if necessary. */ 
if it is real. */ 
if the allocation changed. */ 
if (IS_ERR(rl)) 
if (start == rl->vcn) { 
if (rl > runlist->rl && (rl - 1)->lcn == LCN_HOLE) { 
if (end >= rl[1].vcn) { 
for the 
for the remaining non-sparse 
if (IS_ERR(trl)) 
if (runlist->rl != trl) { 
ift all the runs up by one. */ 
if it is real. */ 
if (rl_end->lcn == LCN_HOLE) { 
if (rl < rl_end) 
if @end is in the next run need to split the run into a sparse 
if (end >= rl[1].vcn) { 
if (rl[1].length && end >= rl[2].vcn) { 
if (IS_ERR(trl)) 
if (runlist->rl != trl) { 
if (rl->lcn >= 0) { 
for the non-sparse 
for the remaining 
if (IS_ERR(trl)) 
if (runlist->rl != trl) { 
ift all the runs up by two. */ 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/aops.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for reading attributes 
for reading pages belonging to the 
form the post read mst fixups when all IO on the 
if (likely(uptodate)) { 
if (unlikely(init_size > i_size)) { 
for the current buffer head overflowing. */ 
if (file_ofs < init_size) 
if (!buffer_uptodate(tmp)) 
if (buffer_async_read(tmp)) { 
while (tmp != bh); 
if the 
if (!NInoMstProtected(ni)) { 
ified before we got here... */ 
for (i = 0; i < recs; i++) 
if (likely(page_uptodate && !PageError(page))) 
if required, automatically 
fore finally marking it uptodate and 
force allocated_size limit because i_size is checked for in 
if (!page_has_buffers(page)) { 
if (unlikely(!page_has_buffers(page))) { 
for the whole 
if the page is being 
if (unlikely(init_size > i_size)) { 
if (unlikely(buffer_uptodate(bh))) 
if (unlikely(buffer_mapped(bh))) { 
if (iblock < lblock) { 
if (!rl) { 
if (likely(rl != NULL)) { 
while (rl->length && rl[1].vcn <= vcn) 
if (lcn >= 0) { 
if (iblock < zblock) { 
if (lcn == LCN_HOLE) 
if (!is_retry && lcn == LCN_RL_NOT_MAPPED) { 
for 
if (likely(!err)) 
if (!rl) 
for example. 
if (err == -ENOENT || lcn == LCN_ENOENT) { 
if (!err) 
if (likely(!err)) 
while (i++, iblock++, (bh = bh->b_this_page) != head); 
if (rl) 
for i/o. */ 
for (i = 0; i < nr; i++) { 
for (i = 0; i < nr; i++) { 
if (likely(!buffer_uptodate(tbh))) 
if (likely(!PageError(page))) 
if the mft record is not cached at this point in time, we need to wait 
for it to be read in before we can do the copy. 
if (unlikely(page->index >= (i_size + PAGE_CACHE_SIZE - 1) >> 
if (PageUptodate(page)) { 
for 
if (ni->type != AT_INDEX_ALLOCATION) { 
if (NInoEncrypted(ni)) { 
if (NInoNonResident(ni) && NInoCompressed(ni)) { 
if (NInoNonResident(ni)) { 
if it is resident the actual data is not compressed so we are 
if (unlikely(page->index > 0)) { 
if (!NInoAttr(ni)) 
if (IS_ERR(mrec)) { 
if (unlikely(NInoNonResident(ni))) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) 
if (unlikely(attr_len > ni->initialized_size)) 
if (unlikely(attr_len > i_size)) { 
ifdef NTFS_RW 
for writing pages belonging to non-resident, non-mst 
for the 
for example.) 
for inode 0x%lx, attribute type 0x%x, page index " 
if (!page_has_buffers(page)) { 
if (unlikely(!page_has_buffers(page))) { 
for_writepage(wbc, page); 
ifferent naming scheme to ntfs_read_block()! */ 
for the data size. */ 
if (unlikely(block >= dblock)) { 
fore we get here, 
if (!buffer_dirty(bh)) 
if (unlikely((block >= iblock) && 
if (block > iblock) { 
for each page do: 
for each page do: 
if the page is uptodate. 
fore. 
if (!PageUptodate(page)) { 
if (buffer_mapped(bh)) 
if (!rl) { 
if (likely(rl != NULL)) { 
while (rl->length && rl[1].vcn <= vcn) 
if (lcn >= 0) { 
if (lcn == LCN_HOLE) { 
if the buffer is zero. */ 
if (unlikely(*bpos)) 
while (likely(++bpos < bend)); 
if (bpos == bend) { 
if (!is_retry && lcn == LCN_RL_NOT_MAPPED) { 
for 
if (likely(!err)) 
if (!rl) 
if (err == -ENOENT || lcn == LCN_ENOENT) { 
if (!err) 
while (block++, (bh = bh->b_this_page) != head); 
if (rl) 
if (unlikely(!PageUptodate(page))) { 
if (!buffer_uptodate(bh)) { 
while ((bh = bh->b_this_page) != head); 
for async write i/o. */ 
if (buffer_mapped(bh) && buffer_dirty(bh)) { 
if (test_clear_buffer_dirty(bh)) { 
if (unlikely(err)) { 
if (err != -ENOMEM) 
while ((bh = bh->b_this_page) != head); 
if (unlikely(err == -EOPNOTSUPP)) 
if (err == -ENOMEM) { 
for_writepage(wbc, page); 
for i/o. */ 
if (buffer_async_write(bh)) { 
while (bh != head); 
if (unlikely(need_end_writeback)) 
for writing pages belonging to non-resident, mst protected 
for the index allocation case. 
if we were to unlock the 
fore undoing the fixups, any other user of the page will see the 
for the duration of the function to ensure 
for inode 0x%lx, attribute type 0x%x, page index " 
if a page 
for now. 
for sync purposes? */ 
for the data size. */ 
if (likely(block < rec_block)) { 
if (!rec_is_dirty) 
if (unlikely(err2)) { 
if (block == rec_block) */ { 
if (unlikely(block >= dblock)) { 
if (!buffer_dirty(bh)) { 
if it is not mapped already. */ 
if (!rl) { 
if (likely(rl != NULL)) { 
while (rl->length && rl[1].vcn <= vcn) 
if (likely(lcn >= 0)) { 
if (!is_mft && !is_retry && 
for the duration. 
if (likely(!err2)) 
if (err2 == -ENOMEM) 
if (!rl) 
if (!err || err == -ENOMEM) 
if not error -ENOMEM. 
if (rec_start_bh != bh) { 
while (bhs[--nr_bhs] != rec_start_bh) 
if (err2 != -ENOMEM) { 
while ((rec_start_bh = 
while (block++, (bh = bh->b_this_page) != head); 
if (!nr_bhs) 
for (i = 0; i < nr_bhs; i++) { 
if (i % bhs_per_rec) 
if (is_mft) { 
if (!ntfs_may_write_mft_record(vol, mft_no, 
fore 
while (++i % bhs_per_rec); 
if (tni) 
if (unlikely(err2)) { 
while (++i % bhs_per_rec); 
if (!nr_recs) 
for (i = 0; i < nr_bhs; i++) { 
if (!tbh) 
if (!trylock_buffer(tbh)) 
if not @sync. */ 
for (i = 0; i < nr_bhs; i++) { 
if (!tbh) 
if (unlikely(!buffer_uptodate(tbh))) { 
while writing ntfs " 
if (!err || err == -ENOMEM) 
if (is_mft && sync) { 
for (i = 0; i < nr_bhs; i++) { 
if (i % bhs_per_rec) 
if (!tbh) 
if (mft_no < vol->mftmirr_size) 
if (!sync) 
for (i = 0; i < nr_bhs; i++) { 
if (!tbh) 
while (nr_locked_nis-- > 0) { 
if (tni->nr_extents >= 0) 
if (unlikely(err && err != -ENOMEM)) { 
if there is only one ntfs record in the page. 
if (ni->itype.index.block_size == PAGE_CACHE_SIZE) 
if (page_is_dirty) { 
for_writepage(wbc, page); 
if (likely(!err)) 
if necessary creates and writes the 
for the inode the mft record belongs to or via the 
if (unlikely(page->index >= (i_size + PAGE_CACHE_SIZE - 1) >> 
for 
if (ni->type != AT_INDEX_ALLOCATION) { 
if (NInoEncrypted(ni)) { 
if (NInoNonResident(ni) && NInoCompressed(ni)) { 
if (NInoNonResident(ni) && NInoSparse(ni)) { 
if (NInoNonResident(ni)) { 
if (page->index >= (i_size >> PAGE_CACHE_SHIFT)) { 
if (NInoMstProtected(ni)) 
if it is resident the actual data is not 
if (unlikely(page->index > 0)) { 
if (!NInoAttr(ni)) 
if (IS_ERR(m)) { 
if (unlikely(NInoNonResident(ni))) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) 
if (unlikely(attr_len > i_size)) { 
if (err == -ENOMEM) { 
for_writepage(wbc, page); 
if (ctx) 
if (m) 
if	/* NTFS_RW */ 
for inodes and attributes 
ifdef NTFS_RW 
if /* NTFS_RW */ 
for mst protecteed inodes 
ifdef NTFS_RW 
if /* NTFS_RW */ 
ifdef NTFS_RW 
if (unlikely(!page_has_buffers(page))) { 
if (likely(!page_has_buffers(page))) { 
while (bh); 
if (bh_ofs + bh_size <= ofs) 
if (unlikely(bh_ofs >= end)) 
while ((bh = bh->b_this_page) != head); 
if (unlikely(buffers_to_free)) { 
while (buffers_to_free); 
file : ./test/kernel/fs/ntfs/super.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
if (s) { 
if (!strcmp(s, "0") || !strcmp(s, "no") || 
for the ntfs volume described by @vol. 
if (!strcmp(p, option)) {					\ 
if (*v)						\ 
if (!strcmp(p, option)) {					\ 
if (*v)							\ 
if (!strcmp(p, option)) {					\ 
if (!v || !*v)						\ 
if (*v)							\ 
if (!uid_valid(variable))				\ 
if (!strcmp(p, option)) {					\ 
if (!v || !*v)						\ 
if (*v)							\ 
if (!gid_valid(variable))				\ 
if (!strcmp(p, option)) {					\ 
if (*v)							\ 
if (!strcmp(p, option)) {					\ 
if (!simple_getbool(v, &val))				\ 
if (!strcmp(p, option)) {					\ 
if (!v || !*v)						\ 
if (variable == -1)					\ 
for (_i = 0; opt_array[_i].str && *opt_array[_i].str; _i++) \ 
if (!opt_array[_i].str || !*opt_array[_i].str)		\ 
if (!opt || !*opt) 
while ((p = strsep(&opt, ","))) { 
if (!strcmp(p, "posix") || !strcmp(p, "show_inodes")) 
if (!strcmp(p, "nls") || !strcmp(p, "iocharset")) { 
if (!v || !*v) 
if (!nls_map) { 
if (!strcmp(p, "utf8")) { 
if (!v || !*v) 
if (!simple_getbool(v, &val)) 
if (val) { 
if (errors < INT_MAX) 
if (errors && !sloppy) 
if (sloppy) 
if (on_errors != -1) { 
if (nls_map) { 
if (!vol->nls_map) { 
if (!vol->nls_map) { 
if (mft_zone_multiplier != -1) { 
if (mft_zone_multiplier < 1 || mft_zone_multiplier > 4) { 
if (!vol->mft_zone_multiplier) 
if (on_errors != -1) 
if (!vol->on_errors || vol->on_errors == ON_ERRORS_RECOVER) 
if (uid_valid(uid)) 
if (gid_valid(gid)) 
if (fmask != (umode_t)-1) 
if (dmask != (umode_t)-1) 
if (show_sys_files != -1) { 
if (case_sensitive != -1) { 
if (disable_sparse != -1) { 
if (!NVolSparseEnabled(vol) && 
ifdef NTFS_RW 
formation flags 
for the volume information flags 
formation flags on the volume @vol with the value 
ify with the old flags and use 
if (vol->vol_flags == flags) 
if (IS_ERR(m)) { 
if (!ctx) { 
if (err) 
if (ctx) 
formation flags 
formation flags on the volume @vol. 
formation flags 
formation flags on the volume @vol. 
if /* NTFS_RW */ 
ifndef NTFS_RW 
force read-only flag. */ 
if we are remounting read-write, 
if the volume is not umounted 
if no volume errors 
if ((sb->s_flags & MS_RDONLY) && !(*flags & MS_RDONLY)) { 
if (NVolErrors(vol)) { 
if (vol->vol_flags & VOLUME_IS_DIRTY) { 
if (vol->vol_flags & VOLUME_MODIFIED_BY_CHKDSK) { 
if (vol->vol_flags & VOLUME_MUST_MOUNT_RO_MASK) { 
if (ntfs_set_volume_flags(vol, VOLUME_IS_DIRTY)) { 
formation flags%s", es); 
if 0 
ifferent between NTFS 1.2 and 3.x... 
if ((vol->major_ver > 1)) { 
if 
if (!ntfs_mark_quotas_out_of_date(vol)) { 
if (!ntfs_stamp_usnjrnl(vol)) { 
if (!(sb->s_flags & MS_RDONLY) && (*flags & MS_RDONLY)) { 
if (!NVolErrors(vol)) { 
formation " 
if /* NTFS_RW */ 
if (!parse_options(vol, opt)) 
if it is valid and 'false' if not. 
for warning/error output, i.e. it can be NULL when silent 
if this is the case. 
if ((void*)b < (void*)&b->checksum && b->checksum && !silent) { 
for (i = 0, u = (le32*)b; u < (le32*)(&b->checksum); ++u) 
if (le32_to_cpu(b->checksum) != i) 
ifier is "NTFS    " */ 
if (le16_to_cpu(b->bpb.bytes_per_sector) < 0x100 || 
if ((u32)le16_to_cpu(b->bpb.bytes_per_sector) * 
if (le16_to_cpu(b->bpb.reserved_sectors) || 
if ((u8)b->clusters_per_mft_record < 0xe1 || 
if ((u8)b->clusters_per_index_record < 0xe1 || 
for valid end of sector marker. We will work without it, but 
if (!silent && b->end_of_sector_marker != cpu_to_le16(0xaa55)) 
if true, suppress all output 
fore. 
if ((bh_primary = sb_bread(sb, 0))) { 
if (!silent) 
if (!silent) 
if (!(NTFS_SB(sb)->on_errors & ON_ERRORS_RECOVER)) { 
if (!silent) 
if ((bh_backup = sb_bread(sb, nr_blocks - 1))) { 
if (!silent) 
if ((bh_backup = sb_bread(sb, nr_blocks >> 1))) { 
if (!silent) 
if (!silent) 
if (bh_primary) 
if (bh_primary) { 
if the backup boot sector 
if (!(sb->s_flags & MS_RDONLY)) { 
if (buffer_uptodate(bh_primary)) { 
while " 
formation therein in 
if (vol->sector_size < vol->sb->s_blocksize) { 
if (vol->cluster_size < vol->sector_size) { 
if (clusters_per_mft_record > 0) 
if (vol->mft_record_size > PAGE_CACHE_SIZE) { 
if (vol->mft_record_size < vol->sector_size) { 
if (clusters_per_index_record > 0) 
if (vol->index_record_size < vol->sector_size) { 
for 64-bit-ness. 
if ((u64)ll >= 1ULL << 32) { 
if (sizeof(unsigned long) < 8) { 
for this architecture.  " 
if (ll >= vol->nr_clusters) { 
if (ll >= vol->nr_clusters) { 
ifdef NTFS_RW 
if (vol->cluster_size <= (4 << vol->mft_record_size_bits)) 
if /* NTFS_RW */ 
for which to setup the allocators 
ifdef NTFS_RW 
if /* NTFS_RW */ 
ifdef NTFS_RW 
ified NTFS volume (see mkntfs 
for the mft bitmap as well within the mft_zone itself. 
if (mft_lcn * vol->cluster_size < 16 * 1024) 
if (vol->mft_zone_start <= mft_lcn) 
while (vol->mft_zone_end >= vol->nr_clusters) { 
if /* NTFS_RW */ 
for a volume 
if (IS_ERR(tmp_ino) || is_bad_inode(tmp_ino)) { 
ifics about $MFTMirr's inode as 
for anyone. */ 
for $MFTMirr. */ 
for directories. 
if necessary. */ 
if (index) { 
if (IS_ERR(mft_page)) { 
if (IS_ERR(mirr_page)) { 
if it is not in use. */ 
if (ntfs_is_baad_recordp((le32*)kmft)) { 
if it is not in use. */ 
if (ntfs_is_baad_recordp((le32*)kmirr)) { 
if (bytes < sizeof(MFT_RECORD_OLD) || 
if (bytes < sizeof(MFT_RECORD_OLD) || 
if (memcmp(kmft, kmirr, bytes)) { 
while (++i < vol->mftmirr_size); 
for it. 
if (rl2[i].vcn != rl[i].vcn || rl2[i].lcn != rl[i].lcn || 
while (rl2[i++].length); 
for a volume 
if (IS_ERR(tmp_ino) || is_bad_inode(tmp_ino)) { 
if (!ntfs_check_logfile(tmp_ino, rp)) { 
if Windows is suspended on a volume 
if Windows is hibernated on the ntfs volume @vol.  This is done by 
for the file hiberfil.sys in the root directory of the volume.  If 
for now this should do fine. 
if 
for the above mentioned caveat of a 
if Windows is not hibernated on the volume, >0 if Windows is 
for the hibernation file by looking up the 
if (IS_ERR_MREF(mref)) { 
if (ret == -ENOENT) { 
for " 
for the type of match that was found. */ 
if (IS_ERR(vi) || is_bad_inode(vi)) { 
if (unlikely(i_size_read(vi) < NTFS_HIBERFIL_HEADER_SIZE)) { 
if (IS_ERR(page)) { 
if (*(le32*)kaddr == cpu_to_le32(0x72626968)/*'hibr'*/) { 
if (unlikely(*kaddr)) { 
while (++kaddr < kend); 
if present 
for the quota file by looking up the filename 
if (IS_ERR_MREF(mref)) { 
if (MREF_ERR(mref) == -ENOENT) { 
if they are 
for $Quota."); 
for the type of match that was found. */ 
if (IS_ERR(tmp_ino) || is_bad_inode(tmp_ino)) { 
if (IS_ERR(tmp_ino)) { 
if present 
for the transaction log file by looking up the 
if (IS_ERR_MREF(mref)) { 
if (MREF_ERR(mref) == -ENOENT) { 
if 
for " 
for the type of match that was found. */ 
if (unlikely(IS_ERR(tmp_ino) || is_bad_inode(tmp_ino))) { 
if (unlikely(vol->vol_flags & VOLUME_DELETE_USN_UNDERWAY)) { 
if (IS_ERR(tmp_ino)) { 
if (unlikely(i_size_read(tmp_ino) < sizeof(USN_HEADER))) { 
if (IS_ERR(tmp_ino)) { 
ify $J is non-resident and sparse. */ 
if (unlikely(!NInoNonResident(tmp_ni) || !NInoSparse(tmp_ni))) { 
if (IS_ERR(page)) { 
if (unlikely(sle64_to_cpu(uh->allocation_delta) > 
if (unlikely(sle64_to_cpu(uh->lowest_valid_usn) >= 
if (likely(sle64_to_cpu(uh->lowest_valid_usn) == 
if the volume does " 
for a volume 
if (IS_ERR(ino) || is_bad_inode(ino)) { 
if (i_size <= 0 || i_size > 0x7fffffff) 
if (!vol->attrdef) 
while (index < max_index) { 
if (IS_ERR(page)) 
if (size == PAGE_CACHE_SIZE) { 
if (size) 
if /* NTFS_RW */ 
for an ntfs volume 
if (IS_ERR(ino) || is_bad_inode(ino)) { 
if (!i_size || i_size & (sizeof(ntfschar) - 1) || 
if (!vol->upcase) 
while (index < max_index) { 
if (IS_ERR(page)) 
if (size == PAGE_CACHE_SIZE) { 
if (size) 
if (!default_upcase) { 
if (max > vol->upcase_len) 
for (i = 0; i < max; i++) 
if (i == max) { 
ified $UpCase matches default. Using " 
ified $UpCase since it does not match " 
if (default_upcase) { 
ifdef NTFS_RW 
if /* NTFS_RW */ 
ifdef NTFS_RW 
if (!load_and_init_mft_mirror(vol) || !check_mft_mirror(vol)) { 
if (!(sb->s_flags & MS_RDONLY)) { 
ified%s", 
if /* NTFS_RW */ 
if (IS_ERR(vol->mftbmp_ino)) { 
if (!load_and_init_upcase(vol)) 
ifdef NTFS_RW 
if (!load_and_init_attrdef(vol)) 
if /* NTFS_RW */ 
ify the size, no 
for any locking at this stage as we are already running 
if (IS_ERR(vol->lcnbmp_ino) || is_bad_inode(vol->lcnbmp_ino)) { 
if ((vol->nr_clusters + 7) >> 3 > i_size_read(vol->lcnbmp_ino)) { 
if (IS_ERR(vol->vol_ino) || is_bad_inode(vol->vol_ino)) { 
if (IS_ERR(m)) { 
if (!(ctx = ntfs_attr_get_search_ctx(NTFS_I(vol->vol_ino), m))) { 
if (ntfs_attr_lookup(AT_VOLUME_INFORMATION, NULL, 0, 0, 0, NULL, 0, 
if ((u8*)vi < (u8*)ctx->attr || (u8*)vi + 
if (vol->major_ver < 3 && NVolSparseEnabled(vol)) { 
ifdef NTFS_RW 
if (vol->vol_flags & VOLUME_MUST_MOUNT_RO_MASK) { 
ified by chkdsk"; 
if (vol->vol_flags & VOLUME_IS_DIRTY) 
if (vol->vol_flags & VOLUME_MODIFIED_BY_CHKDSK) { 
if (!(sb->s_flags & MS_RDONLY)) { 
ified%s", 
if the volume 
if (!load_and_check_logfile(vol, &rp) || 
if (!(sb->s_flags & MS_RDONLY)) { 
ified%s", 
if (vol->logfile_ino) { 
if /* NTFS_RW */ 
if (IS_ERR(vol->root_ino) || is_bad_inode(vol->root_ino)) { 
ifdef NTFS_RW 
if Windows is suspended to disk on the target volume.  If it 
if (unlikely(err)) { 
if (!(sb->s_flags & MS_RDONLY)) { 
ified%s", 
if (!(sb->s_flags & MS_RDONLY) && 
formation flags"; 
if (!(vol->on_errors & (ON_ERRORS_REMOUNT_RO | 
ified%s", 
if 0 
ifferent between NTFS 1.2 and 3.x... 
if (!(sb->s_flags & MS_RDONLY) && (vol->major_ver > 1) && 
if (!(vol->on_errors & (ON_ERRORS_REMOUNT_RO | 
ified%s", 
if 
if (!(sb->s_flags & MS_RDONLY) && 
if (!(vol->on_errors & (ON_ERRORS_REMOUNT_RO | 
ified%s", 
if /* NTFS_RW */ 
fore 3.0, we are done. */ 
ific initialization. */ 
if (IS_ERR(vol->secure_ino) || is_bad_inode(vol->secure_ino)) { 
if (IS_ERR(vol->extend_ino) || is_bad_inode(vol->extend_ino)) { 
ifdef NTFS_RW 
if (!load_and_init_quota(vol)) { 
if (!(sb->s_flags & MS_RDONLY)) { 
ified%s", 
if (!(sb->s_flags & MS_RDONLY) && 
if (!(vol->on_errors & (ON_ERRORS_REMOUNT_RO | 
ified%s", 
if present, check 
if (!load_and_init_usnjrnl(vol)) { 
if (!(sb->s_flags & MS_RDONLY)) { 
ified%s", 
if (!(sb->s_flags & MS_RDONLY) && !ntfs_stamp_usnjrnl(vol)) { 
if (!(vol->on_errors & (ON_ERRORS_REMOUNT_RO | 
ified%s", 
if /* NTFS_RW */ 
ifdef NTFS_RW 
if (vol->usnjrnl_j_ino) 
if (vol->usnjrnl_max_ino) 
if (vol->usnjrnl_ino) 
if (vol->quota_q_ino) 
if (vol->quota_ino) 
if /* NTFS_RW */ 
ifdef NTFS_RW 
if /* NTFS_RW */ 
if (vol->attrdef) { 
ifdef NTFS_RW 
if /* NTFS_RW */ 
if (vol->upcase == default_upcase) { 
if (vol->upcase) { 
ifdef NTFS_RW 
if /* NTFS_RW */ 
ific part of the 
ifdef NTFS_RW 
while they are still open in case some of them 
ific. */ 
if (vol->usnjrnl_j_ino) 
if (vol->usnjrnl_max_ino) 
if (vol->usnjrnl_ino) 
if (vol->quota_q_ino) 
if (vol->quota_ino) 
if (vol->extend_ino) 
if (vol->secure_ino) 
if (vol->logfile_ino) 
if (vol->mftmirr_ino) 
if (!(sb->s_flags & MS_RDONLY)) { 
if (ntfs_clear_volume_flags(vol, VOLUME_IS_DIRTY)) 
formation " 
if (vol->mftmirr_ino) 
if /* NTFS_RW */ 
ific clean up. */ 
ifdef NTFS_RW 
if (vol->usnjrnl_max_ino) { 
if (vol->usnjrnl_ino) { 
if (vol->quota_q_ino) { 
if (vol->quota_ino) { 
if /* NTFS_RW */ 
if (vol->secure_ino) { 
ifdef NTFS_RW 
if (vol->mftmirr_ino) { 
if /* NTFS_RW */ 
if (vol->attrdef) { 
if necessary.  Also decrease 
if (vol->upcase == default_upcase) { 
if (!ntfs_nr_upcase_users && default_upcase) { 
if (vol->cluster_size <= 4096 && !--ntfs_nr_compression_users) 
if (vol->upcase) { 
for which to obtain free cluster count 
if we have one 
for (index = 0; index < max_index; index++) { 
if necessary, and increment the use count. 
if (IS_ERR(page)) { 
for eventual bits outside logical ntfs volume (see function 
if (vol->nr_clusters & 63) 
if (nr_free < 0) 
for which to obtain free inode count 
for reading or writing. 
for (index = 0; index < max_index; index++) { 
if necessary, and increment the use count. 
if (IS_ERR(page)) { 
if (nr_free < 0) 
formation about mounted NTFS volume 
formation 
fore ntfs_statfs is 
if we run out and we can keep doing this until 
if (size < 0LL) 
if we 
ificant 32-bits in f_fsid[0] and the most significant 
ifdef NTFS_RW 
if 
ifdef NTFS_RW 
if /* NTFS_RW */ 
if errors are detected. This is used 
ifficult piece of bootstrap by reading the 
for this context 
while mounting NTFS. [The validator is still active 
ifndef NTFS_RW 
if /* ! NTFS_RW */ 
if (!vol) { 
while owner has full access. Further, files by 
if (!parse_options(vol, (char*)opt)) 
if (bdev_logical_block_size(sb->s_bdev) > PAGE_CACHE_SIZE) { 
if (blocksize < NTFS_BLOCK_SIZE) { 
if (!i_size_read(sb->s_bdev->bd_inode)) { 
if (!(bh = read_ntfs_boot_sector(sb, silent))) { 
if (!result) { 
for each sector 
for sure that it works. 
if (vol->sector_size > blocksize) { 
if (blocksize != vol->sector_size) { 
for the file size, i.e. correct would be: 
for the page cache and our address 
for $MFT which is sufficient to allow our normal inode 
if (!tmp_ino) { 
if (ntfs_read_inode_mount(tmp_ino) < 0) { 
if the cluster size is 
if (vol->cluster_size <= 4096 && !ntfs_nr_compression_users++) { 
if (result) { 
for compression engine."); 
if necessary.  Also 
if (!default_upcase) 
if (!load_system_files(vol)) { 
if ((sb->s_root = d_make_root(vol->root_ino))) { 
if it has no users. */ 
if (!--ntfs_nr_upcase_users && default_upcase) { 
ific clean up. */ 
ifdef NTFS_RW 
if (vol->usnjrnl_max_ino) { 
if (vol->usnjrnl_ino) { 
if (vol->quota_q_ino) { 
if (vol->quota_ino) { 
if /* NTFS_RW */ 
if (vol->secure_ino) { 
ifdef NTFS_RW 
if (vol->mftmirr_ino) { 
if /* NTFS_RW */ 
if (vol->attrdef) { 
if (vol->upcase == default_upcase) { 
if (vol->upcase) { 
if (vol->nls_map) { 
if necessary. 
if (!--ntfs_nr_upcase_users && default_upcase) { 
if (vol->cluster_size <= 4096 && !--ntfs_nr_compression_users) 
if (vol->mft_ino && vol->mft_ino != tmp_ino) 
for efficient allocation/deallocation of inodes. */ 
for the inode slab cache. */ 
for the slab caches. */ 
ifdef NTFS_RW 
if 
if 
if 
if (!ntfs_index_ctx_cache) { 
if (!ntfs_attr_ctx_cache) { 
if (!ntfs_name_cache) { 
if (!ntfs_inode_cache) { 
if (!ntfs_big_inode_cache) { 
if (err) { 
if (!err) { 
if (!err) { 
fore we 
ifdef DEBUG 
if 
file : ./test/kernel/fs/ntfs/collate.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
if (!rc && (data1_len != data2_len)) { 
if (d1 < d2) 
if (d1 == d2) 
ified collation rule 
if @data1 is found, respectively, to collate before, 
for everything else for now. 
if (i <= 0x02) 
if (likely(i <= 3)) 
file : ./test/kernel/fs/ntfs/attrib.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for which to map (part of) a runlist 
if present or NULL if not 
ified, it is an active search context of @ni and its base mft 
ify @ctx as NULL and ntfs_map_runlist_nolock() 
form the necessary mapping and unmapping. 
fore returning.  Thus, @ctx will be left pointing to the same 
ifferent memory locations on return, so you must remember to reset 
if @vcn is zero and 
if 'true' the @ctx 
for 
for writing on entry 
if (!NInoAttr(ni)) 
if (!ctx) { 
if (IS_ERR(m)) 
if (unlikely(!ctx)) { 
if (!a->data.non_resident.lowest_vcn && end_vcn <= 0) 
if vcn exceeds the allocated size, we will refuse to 
if (vcn >= allocated_size_vcn || (a->type == ni->type && 
if (old_ctx.base_ntfs_ino && old_ctx.ntfs_ino != 
if (ctx_needs_reset) { 
if (unlikely(err)) { 
if @vcn is inside it.  Otherwise 
if (unlikely(vcn && vcn >= end_vcn)) { 
if (IS_ERR(rl)) 
if (ctx_is_temporary) { 
if (ctx_needs_reset) { 
if (NInoAttrList(base_ni)) { 
fore, we need to unmap it and map the 
if (ctx->ntfs_ino != old_ctx.ntfs_ino) { 
if (ctx->base_ntfs_ino && ctx->ntfs_ino != 
if (old_ctx.base_ntfs_ino && 
if (IS_ERR(ctx->mrec)) { 
if (ctx->mrec != old_ctx.mrec) { 
for chkdsk to pick up 
if (put_this_page) 
for which to map (part of) a runlist 
ify 
while we were sleeping. */ 
if the runlist is locked for writing 
for writing and 
for reading, 
for i_ino 0x%lx, vcn 0x%llx, %s_locked.", 
if (!ni->runlist.rl) { 
if (!ni->allocated_size) { 
if (likely(lcn >= LCN_HOLE)) { 
if (lcn != LCN_RL_NOT_MAPPED) { 
if (!is_retry) { 
if (!write_locked) { 
if (unlikely(ntfs_rl_vcn_to_lcn(ni->runlist.rl, vcn) != 
if (!write_locked) { 
if (likely(!err)) { 
if (err == -ENOENT) 
if (err == -ENOMEM) 
if (lcn != LCN_ENOENT) 
if present or NULL if not 
ified, it is an active search context of @ni and its base mft 
ify @ctx as NULL and ntfs_attr_find_vcn_nolock() 
form the necessary mapping and unmapping. 
fore returning.  Thus, @ctx will be left pointing to the same 
ifferent memory locations on return, so you must remember to reset 
if the return is success or failure and PTR_ERR() to get to the 
if 'true' the @ctx 
for 
for writing on entry 
for i_ino 0x%lx, vcn 0x%llx, with%s ctx.", 
if (!ni->runlist.rl) { 
if (!ni->allocated_size) { 
if (likely(rl && vcn >= rl[0].vcn)) { 
while (likely(rl->length)) { 
if (likely(rl->lcn >= LCN_HOLE)) { 
if (likely(rl->lcn != LCN_RL_NOT_MAPPED)) { 
if (!err && !is_retry) { 
if (IS_ERR(ctx->mrec)) 
if (likely(!err)) { 
if (err == -EINVAL) 
if (!err) 
if (err != -ENOENT) 
if @name present) 
ified by @ctx->mrec, beginning at @ctx->attr, for an 
fore which the attribute being 
for. 
ified mft record and it ignores the 
for, obviously).  If you need to take attribute lists into consideration, 
for extent records of non-resident 
while the last extent is in the base 
for attribute types which can be 
if @ctx->is_first is 'true'. 
if (ctx->is_first) { 
for (;;	a = (ATTR_RECORD*)((u8*)a + le32_to_cpu(a->length))) { 
if (unlikely(le32_to_cpu(a->type) > le32_to_cpu(type) || 
if (unlikely(!a->length)) 
if (a->type != type) 
if (!name) { 
if (a->name_length) 
if (!ntfs_are_names_equal(name, name_len, 
fore a->name, there is no 
if (rc == -1) 
if (rc) 
if (rc == -1) 
if (rc) 
ified, we have found the attribute 
if (!val) 
fore the current attribute's 
if (!rc) { 
if (val_len == avl) 
if (val_len < avl) 
if (rc < 0) 
if (!vol || !runlist || !al || size <= 0 || initialized_size < 0 || 
if (!initialized_size) { 
if (!rl) { 
ified by the runlist one run at a time. */ 
while (rl->length) { 
if (lcn < 0) { 
if (!bh) { 
if (al + block_size >= al_end) 
while (++block < max_block); 
if (initialized_size < size) { 
if (al < al_end) { 
if (initialized_size < size) 
if @name present) 
for the corresponding 
ifferent mft record/inode, ntfs_attr_find() the attribute 
for the attribute. 
fore which 
fore which the attribute being searched for would 
if there is not enough space, the 
for the inserted attribute should be inserted in the 
for inode 0x%lx, type 0x%x.", ni->mft_no, type); 
if (ni == base_ni) 
if (type == AT_END) 
if (!ctx->al_entry) 
if @ctx->is_first is 'true'. 
if (ctx->is_first) { 
for (;; al_entry = next_al_entry) { 
if ((u8*)al_entry < base_ni->attr_list || 
if ((u8*)al_entry == al_end) 
if (!al_entry->length) 
if ((u8*)al_entry + 6 > al_end || (u8*)al_entry + 
if (le32_to_cpu(al_entry->type) > le32_to_cpu(type)) 
if (type != al_entry->type) 
if (!name) { 
if (!ntfs_are_names_equal(al_name, al_name_len, name, 
fore al_name, there is no 
if (rc == -1) 
if (rc) 
ifferent.  Perhaps I 
if (rc == -1) 
if (rc) 
if the 
if (lowest_vcn && (u8*)next_al_entry >= al_start	    && 
if (MREF_LE(al_entry->mft_reference) == ni->mft_no) { 
if (ni != base_ni) 
if (MREF_LE(al_entry->mft_reference) == 
if (IS_ERR(ctx->mrec)) { 
if (err == -ENOENT) 
for example which become 
ified ntfs_attr_find() here. 
if ((u8*)a < (u8*)ctx->mrec || (u8*)a > (u8*)ctx->mrec + 
if (a->type == AT_END) 
if (!a->length) 
if (al_entry->instance != a->instance) 
if (al_entry->type != a->type) 
if (!ntfs_are_names_equal((ntfschar*)((u8*)a + 
ified or @val specified and it matches, we 
if (!val || (!a->non_resident && le32_to_cpu( 
if (!err) { 
if (ni != base_ni) { 
if (err != -ENOMEM) 
for AT_END, we reset the search context @ctx and 
if (type == AT_END) { 
fore we return, we want to ensure 
if (ni != base_ni) 
for enumeration would 
while (!err); 
if @name present) 
if the search was successful and -errno if not. 
if one wants to add the attribute to the 
if one wants to add the attribute to the mft record this is the 
if (ctx->base_ntfs_ino) 
for debugging really. */ 
if (!NInoAttrList(base_ni) || type == AT_ATTRIBUTE_LIST) 
formed elsewhere. */ 
if present, and initialize the search context again. 
for a new attribute is being started to reset 
if (likely(!ctx->base_ntfs_ino)) { 
formed elsewhere. */ 
if (ctx->ntfs_ino != ctx->base_ntfs_ino) 
if allocation failed. 
if (ctx) 
if present. 
if (ctx->base_ntfs_ino && ctx->ntfs_ino != ctx->base_ntfs_ino) 
ifdef NTFS_RW 
for the attribute definition record corresponding to the attribute 
if found and NULL if not found. 
for (ad = vol->attrdef; (u8*)ad - (u8*)vol->attrdef < 
if (likely(le32_to_cpu(ad->type) < le32_to_cpu(type))) 
if (likely(ad->type == type)) 
for validity 
for an attribute of @type on the 
if valid, -ERANGE if not valid, or -ENOENT if the attribute is not 
if (unlikely(type == AT_ATTRIBUTE_LIST && size > 256 * 1024)) 
for the attribute @type. */ 
if (unlikely(!ad)) 
if (((sle64_to_cpu(ad->min_size) > 0) && 
if an attribute can be non-resident 
formation is obtained from $AttrDef system file. 
if the attribute is not listed in $AttrDef. 
if (unlikely(!ad)) 
if (ad->flags & ATTR_DEF_RESIDENT) 
if an attribute can be resident 
formation is derived from our ntfs knowledge and may 
for index 
if the attribute is allowed to be non-resident and -EPERM if not. 
for this here as we do not know which inode's $Bitmap is 
if (type == AT_INDEX_ALLOCATION) 
form the resize. 
for new_size %u.", new_size); 
if (new_size & 7) 
if (new_size != le32_to_cpu(a->length)) { 
if (new_muse > le32_to_cpu(m->bytes_allocated)) 
if (new_size >= offsetof(ATTR_REC, length) + sizeof(a->length)) 
form the resize. 
if (ntfs_attr_record_resize(m, a, 
if (new_size > old_size) 
fore we can map the mft record and our callers 
for trying to make 
for this kind of -ENOSPC or is it always worth trying 
if (unlikely(err)) { 
for them. 
for allocation 
if (new_size > 0) { 
if (unlikely(!page)) 
if (IS_ERR(rl)) { 
for_mapping_pairs(vol, rl, 0, -1); 
for mapping pairs array, error " 
if (!NInoAttr(ni)) 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
for the name and the mapping pairs array. 
if (NInoSparse(ni) || NInoCompressed(ni)) 
if (page && !PageUptodate(page)) { 
if (unlikely(err)) 
if it exists and update the offset. */ 
ific to non-resident attributes. */ 
if (NInoSparse(ni) || NInoCompressed(ni)) { 
if (NInoCompressed(ni) || vol->major_ver < 3) 
if (unlikely(err)) { 
if (NInoSparse(ni) || NInoCompressed(ni)) { 
if (a->data.non_resident.compression_unit) { 
if (page) { 
if it exists and update the offset. */ 
if (a->name_length) 
if (unlikely(err2)) { 
if memory corruption is at work it 
if ((mp_ofs + attr_size) > arec_size) { 
ific to resident attributes. */ 
if (page) { 
if (ctx) 
if (m) 
if (rl) { 
if (err == -EINVAL) 
if this is required. 
for @new_data_size to be smaller than the old data 
if 
for regions that are being made sparse) and 
if necessary moving it and/or other 
if present and in some of the 
if not already present. 
if present) is possible, the allocation is partially extended 
if the extension was partial.  If @data_start is -1 then partial 
formed. 
for writing as well as 
for example be converted 
for anything other 
ifdef DEBUG 
for i_ino 0x%lx, attribute type 0x%x, " 
if 
for allocation purposes. 
if (NInoNonResident(ni)) { 
if new size is allowed in $AttrDef. */ 
if (unlikely(err)) { 
if (start < 0 || start >= allocated_size) { 
for " 
formant for write(2). */ 
if (!NInoAttr(ni)) 
ifying both the runlist (if non-resident) and the mft 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if someone did the work whilst we waited for the locks.  If we 
if we need to update the data size. 
if (unlikely(new_alloc_size <= allocated_size)) { 
if (new_data_size < 0) 
if (unlikely(err)) { 
if (a->non_resident) 
if the size is 
if (new_alloc_size < vol->mft_record_size && 
if (new_data_size >= 0) { 
if that fails dropping 
if successful restart the extension process. 
if (likely(!err)) 
for this attribute type or there not being enough space, 
if (unlikely(err != -EPERM && err != -ENOSPC)) { 
if (start < 0 || start >= allocated_size) 
if (err != -ENOMEM) 
if (start < 0 || start >= allocated_size) { 
for the non-resident " 
if (err == -EPERM) */ 
if 0 
if (!err) 
formation 
if (ni->type == AT_ATTRIBUTE_LIST || 
if (!err) 
if it is not already the only attribute in an mft record in 
if (!err) 
if 
if (new_alloc_size == allocated_size) { 
for this inode, then create a sparse region between the old 
if ((start >= 0 && start <= allocated_size) || ni->type != AT_DATA || 
for now... 
if (likely(rl)) { 
while (rl->length) 
if (unlikely(!rl || rl->lcn == LCN_RL_NOT_MAPPED || 
if (!rl && !allocated_size) 
if (IS_ERR(rl)) { 
if (start < 0 || start >= allocated_size) 
if (err != -ENOMEM) 
while (rl->length) 
while (rl->lcn < 0 && rl > ni->runlist.rl) 
formed when start >= 0.  (Needed for POSIX write(2) 
if (IS_ERR(rl2)) { 
if (start < 0 || start >= allocated_size) 
if (err != -ENOMEM && err != -ENOSPC) 
if (IS_ERR(rl)) { 
if (start < 0 || start >= allocated_size) 
if (err != -ENOMEM) 
if (ntfs_cluster_free_from_rl(vol, rl2)) { 
for the new mapping pairs array for this extent. */ 
if (unlikely(mp_size <= 0)) { 
if (start < 0 || start >= allocated_size) 
for the " 
if (unlikely(err)) { 
for the remainder, or by making 
if (start < 0 || start >= allocated_size) 
for the extended attribute " 
if (unlikely(err)) { 
if (a->data.non_resident.lowest_vcn) { 
if (unlikely(err)) 
if @ni is a directory, $MFT, or an index, 
for 
if we created a hole above.  For now 
if (NInoSparse(ni) || NInoCompressed(ni)) { 
if (new_data_size >= 0) { 
if (start < 0 || start >= allocated_size) 
if (err == -ENOENT) 
if (ntfs_attr_lookup(ni->type, ni->name, ni->name_len, CASE_SENSITIVE, 
if @ni is a directory...  See above. 
if (NInoSparse(ni) || NInoCompressed(ni)) { 
if (ntfs_cluster_free(ni, ll, -1, ctx) < 0) { 
if (ntfs_rl_truncate_nolock(vol, &ni->runlist, ll) || IS_ERR(m)) { 
if (mp_rebuilt) { 
if (success) */ { 
if (ctx) 
if (m) 
ified 
formed. 
for ofs 0x%llx, cnt 0x%llx, val 0x%hx.", 
if (!cnt) 
for them. 
if (unlikely(end > i_size_read(VFS_I(ni)))) { 
if (start_ofs) { 
if (IS_ERR(page)) { 
if (idx == end) 
if (idx == end) 
for (; idx < end; idx++) { 
if (unlikely(!page)) { 
if (page_has_buffers(page)) { 
while ((bh = bh->b_this_page) != head); 
if (end_ofs) { 
if (IS_ERR(page)) { 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/upcase.c 
[ OK ] open : 4 ok... 
ify it 
for more details. 
if not, write to the Free Software Foundation, 
if (!uc) 
for (i = 0; i < default_upcase_len; i++) 
for (r = 0; uc_run_table[r][0]; r++) 
for (r = 0; uc_dup_table[r][0]; r++) 
for (r = 0; uc_word_table[r][0]; r++) 
file : ./test/kernel/fs/ntfs/sysctl.c 
[ OK ] open : 4 ok... 
for sysctl handling in NTFS Linux kernel driver. Part of 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifdef DEBUG 
ifdef CONFIG_SYSCTL 
for the sysctls header. */ 
if (add) { 
if (!sysctls_root_table) 
if /* CONFIG_SYSCTL */ 
file : ./test/kernel/fs/ntfs/unistr.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for equality 
if @ic == IGNORE_CASE) 
if the names are 
forma a case insensitive comparison. 
if (s1_len != s2_len) 
if (ic == CASE_SENSITIVE) 
if @name1 contains an invalid character return this value 
if @ic is CASE_SENSITIVE) 
if the first name collates before the second one, 
if the second name collates before the first one, or 
if (name1_len > name2_len) 
for (cnt = 0; cnt < min_len; ++cnt) { 
if (ic) { 
if (c2 < upcase_len) 
if (c1 < 64 && legal_ansi_char_array[c1] & 8) 
if (c1 < c2) 
if (c1 > c2) 
if (name1_len < name2_len) 
if (name1_len == name2_len) 
if (c1 < 64 && legal_ansi_char_array[c1] & 8) 
format and appropriate le16_to_cpu() 
if @s1 (or the first @n Unicode characters thereof) is found, respectively, 
for (i = 0; i < n; ++i) { 
if (c1 < c2) 
if (c1 > c2) 
if (!c1) 
format and appropriate 
fore the comparison. 
if @s1 (or the first @n Unicode characters thereof) is found, respectively, 
for (i = 0; i < n; ++i) { 
if ((c2 = le16_to_cpu(s2[i])) < upcase_size) 
if (c1 < c2) 
if (c1 > c2) 
if (!c1) 
for (i = 0; i < name_len; i++) 
format the loaded NLS 
for 
if (likely(ins)) { 
if (likely(ucs)) { 
for (i = o = 0; i < ins_len; i += wc_len) { 
if (likely(wc_len >= 0 && 
if (likely(wc)) { 
if (!wc) */ 
if (wc_len < 0 || 
if (!ucs) */ 
for converted " 
if (!ins) */ 
if (wc_len < 0) { 
if (o >= NTFS_MAX_NAME_LEN) */ { 
for a " 
format dictated by the loaded NLS. 
for calling kfree(*@outs); when finished with it. In this case 
if (ins) { 
if (ns && !ns_len) { 
if (!ns) { 
if (!ns) 
for (i = o = 0; i < ins_len; i++) { 
if (wc > 0) { 
if (!wc) 
if (wc == -ENAMETOOLONG && ns != *outs) { 
if (tc) { 
if (ns != *outs) 
if (wc != -ENAMETOOLONG) 
file : ./test/kernel/fs/ntfs/mst.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
form the necessary post read multi sector transfer fixup and detect the 
if ( size & (NTFS_BLOCK_SIZE - 1)	|| 
fore they are fixed up. Note no need to care for 
ifference. 
for incomplete multi sector transfer(s). 
while (usa_count--) { 
while (usa_count--) { 
form the necessary pre write multi sector transfer fixup on the data 
if fixup applied (success) or -EINVAL if no fixup was performed 
fore calling this function, otherwise it 
fore calling this function 
if it makes sense. */ 
if ( size & (NTFS_BLOCK_SIZE - 1)	|| 
if (usn == 0xffff || !usn) 
while (usa_count--) { 
form the necessary post write multi sector transfer fixup, not checking 
while (usa_count--) { 
file : ./test/kernel/fs/ntfs/quota.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifdef NTFS_RW 
if (NVolQuotaOutOfDate(vol)) 
if (!vol->quota_ino || !vol->quota_q_ino) { 
if (!ictx) { 
if (err) { 
if (ictx->data_len < offsetof(QUOTA_CONTROL_ENTRY, sid)) { 
if (le32_to_cpu(qce->version) != QUOTA_VERSION) { 
if (qce->flags & QUOTA_FLAG_OUT_OF_DATE) 
if (!(qce->flags & (QUOTA_FLAG_TRACKING_ENABLED | 
ified on WinXP to be sufficient to cause windows to 
ified flags are written to disk. */ 
if (ictx) 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ext4/xattr_user.c 
[ OK ] open : 4 ok... 
for extended user attributes. 
if (!test_opt(dentry->d_sb, XATTR_USER)) 
if (list && total_len <= list_size) { 
if (strcmp(name, "") == 0) 
if (!test_opt(dentry->d_sb, XATTR_USER)) 
if (strcmp(name, "") == 0) 
if (!test_opt(dentry->d_sb, XATTR_USER)) 
file : ./test/kernel/fs/ext4/extents_status.c 
[ OK ] open : 4 ok... 
ified by 
ify the implementation of fiemap and bigalloc, and introduce 
for better understand 
fore the implementation of fiemap and bigalloc 
fore 
fore, the extent status tree may not 
while we define a shrinker 
for ext4. 
ifies a delayed extent by looking 
if a 
ify delayed allocations from holes. 
if a block is 
for the cluster. 
if a buffer is 
if a block or a range of 
ifferent status.  The extent in the 
formance analysis 
if writes are 
if (ext4_es_cachep == NULL) 
if (ext4_es_cachep) 
ifdef ES_DEBUG__ 
for inode %lu:", inode->i_ino); 
while (node) { 
if 
for an delayed extent with a given offset.  If 
while (node) { 
if (lblk < es->es_lblk) 
if (lblk > ext4_es_end(es)) 
if (es && lblk < es->es_lblk) 
if (es && lblk > ext4_es_end(es)) { 
if it exists, otherwise, the next extent after @es->lblk. 
if (tree->cache_es) { 
if (in_range(lblk, es1->es_lblk, es1->es_len)) { 
if (es1 && !ext4_es_is_delayed(es1)) { 
while ((node = rb_next(&es1->rb_node)) != NULL) { 
if (es1->es_lblk > end) { 
if (ext4_es_is_delayed(es1)) 
if (es1 && ext4_es_is_delayed(es1)) { 
if (es == NULL) 
if (!ext4_es_is_delayed(es)) { 
if (!ext4_es_is_delayed(es)) { 
if (ext4_es_status(es1) != ext4_es_status(es2)) 
if (((__u64) es1->es_len) + es2->es_len > EXT_MAX_BLOCKS) { 
if (((__u64) es1->es_lblk) + es1->es_len != es2->es_lblk) 
if ((ext4_es_is_written(es1) || ext4_es_is_unwritten(es1)) && 
if (ext4_es_is_hole(es1)) 
if (ext4_es_is_delayed(es1) && !ext4_es_is_unwritten(es1)) 
if (!node) 
if (ext4_es_can_be_merged(es1, es)) { 
if (!node) 
if (ext4_es_can_be_merged(es, es1)) { 
ifdef ES_AGGRESSIVE_TEST 
if (IS_ERR(path)) 
if (ex) { 
if (!ext4_es_is_written(es) && !ext4_es_is_unwritten(es)) { 
for " 
if (es->es_lblk < ee_block || 
for inode: %lu " 
if (ee_status ^ es_status) { 
for inode: %lu " 
if (!ext4_es_is_delayed(es) && !ext4_es_is_hole(es)) { 
for inode: %lu " 
if (path) { 
if (retval > 0) { 
for inode: %lu " 
if (ext4_es_is_written(es)) { 
for " 
if (map.m_pblk != ext4_es_pblock(es)) { 
for " 
if (retval == 0) { 
for inode: %lu " 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if 
while (*p) { 
if (newes->es_lblk < es->es_lblk) { 
ify es_lblk directly 
if (ext4_es_is_written(es) || 
if (newes->es_lblk > ext4_es_end(es)) { 
if (!es) 
formation to an inode's extent 
if (!len) 
if (err != 0) 
if (err == -ENOMEM && __ext4_es_shrink(EXT4_SB(inode->i_sb), 1, 
if (err == -ENOMEM && !ext4_es_is_delayed(&newes)) 
formation into the extent status 
if (!len) 
if (!es || es->es_lblk > end) 
if (tree->cache_es) { 
if (in_range(lblk, es1->es_lblk, es1->es_len)) { 
while (node) { 
if (lblk < es1->es_lblk) 
if (lblk > ext4_es_end(es1)) 
if (found) { 
if (!es) 
if (es->es_lblk > end) 
if (len1 > 0) 
if (len2 > 0) { 
if (ext4_es_is_written(&orig_es) || 
if (err) { 
if ((err == -ENOMEM) && 
if (ext4_es_is_written(es) || 
if (len1 > 0) { 
if (node) 
while (es && ext4_es_end(es) <= end) { 
if (!node) { 
if (es && es->es_lblk < end + 1) { 
if (ext4_es_is_written(es) || ext4_es_is_unwritten(es)) { 
if (!len) 
if (ext4_test_inode_state(&eia->vfs_inode, EXT4_STATE_EXT_PRECACHED) && 
if (!ext4_test_inode_state(&eia->vfs_inode, EXT4_STATE_EXT_PRECACHED) && 
if (eia->i_touch_when == eib->i_touch_when) 
if (time_after(eia->i_touch_when, eib->i_touch_when)) 
for_each_safe(cur, tmp, &sbi->s_es_lru) { 
if (percpu_counter_read_positive(&sbi->s_extent_cache_cnt) == 0) 
if ((sbi->s_es_last_sorted < ei->i_touch_when) || 
if (ei->i_es_lru_nr == 0 || ei == locked_ei) 
if (ei->i_es_lru_nr == 0) 
if (nr_to_scan == 0) 
forward progress, sort the list and try again. 
if ((nr_shrunk == 0) && nr_skipped && !retried) { 
iffies; 
if (ext4_test_inode_state(&ei->vfs_inode, 
if (locked_ei && nr_shrunk == 0) 
if (!nr_to_scan) 
iffies; 
if (list_empty(&ei->i_es_lru)) 
if (!list_empty(&ei->i_es_lru)) 
if (ei->i_es_lru_nr == 0) 
if (ext4_test_inode_state(inode, EXT4_STATE_EXT_PRECACHED) && 
forced shrink of precached extents"); 
while (node != NULL) { 
if (!ext4_es_is_delayed(es)) { 
if (--nr_to_scan == 0) 
file : ./test/kernel/fs/ext4/file.c 
[ OK ] open : 4 ok... 
forms by Jakub Jelinek 
ifferent 
if (ext4_test_inode_state(inode, EXT4_STATE_DA_ALLOC_CLOSE)) { 
if we are the last writer on the inode, drop the block reservation */ 
if (is_dx(inode) && filp->private_data) 
if (pos >= i_size_read(inode)) 
if ((pos | iov_iter_alignment(from)) & blockmask) 
if (o_direct && 
if (file->f_flags & O_APPEND) 
format file, the size limit 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) { 
if ((pos > sbi->s_bitmap_maxbytes) || 
if (pos + length > sbi->s_bitmap_maxbytes) 
if (o_direct) { 
if (ext4_should_dioread_nolock(inode) && !aio_mutex && 
for initialized extents.  1) If we 
if (err == len && (map.m_flags & EXT4_MAP_MAPPED)) 
if (ret > 0) { 
if (err < 0) 
if (o_direct) 
if (aio_mutex) 
if (!mapping->a_ops->readpage) 
if (unlikely(!(sbi->s_mount_flags & EXT4_MF_MNTDIR_SAMPLED) && 
for sysadmin convenience 
if (!IS_ERR(cp)) { 
if (IS_ERR(handle)) 
if (err) { 
if we are opening the inode for 
if (filp->f_mode & FMODE_WRITE) { 
if (ret < 0) 
for a extent-based 
for block-mapped and extent-mapped file at the same 
for a file and we can directly use it to 
for SEEK_DATA/SEEK_HOLE, we would need to 
if this range contains an unwritten extent, 
if (nr_pages == 0) { 
if (lastoff == startoff || lastoff < endoff) 
if (lastoff == startoff && whence == SEEK_HOLE && 
for (i = 0; i < nr_pages; i++) { 
if (lastoff < endoff && whence == SEEK_HOLE && 
if (unlikely(page->mapping != inode->i_mapping)) { 
if (!page_has_buffers(page)) { 
if (page_has_buffers(page)) { 
if (buffer_uptodate(bh) || 
if (whence == SEEK_DATA) 
if (whence == SEEK_HOLE) 
if (found) { 
while (bh != head); 
if (nr_pages < num && whence == SEEK_HOLE) { 
while (index <= end); 
for SEEK_DATA. 
if (offset >= isize) { 
if (ret > 0 && !(map.m_flags & EXT4_MAP_UNWRITTEN)) { 
if (es.es_len != 0 && in_range(last, es.es_lblk, es.es_len)) { 
if (map.m_flags & EXT4_MAP_UNWRITTEN) { 
if (unwritten) 
while (last <= end); 
if (dataoff > isize) 
for SEEK_HOLE. 
if (offset >= isize) { 
if (ret > 0 && !(map.m_flags & EXT4_MAP_UNWRITTEN)) { 
if (es.es_len != 0 && in_range(last, es.es_lblk, es.es_len)) { 
if (map.m_flags & EXT4_MAP_UNWRITTEN) { 
if (!unwritten) { 
while (last <= end); 
if (holeoff > isize) 
for each. 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) 
ifdef CONFIG_COMPAT 
if 
file : ./test/kernel/fs/ext4/ext4_jbd2.c 
[ OK ] open : 4 ok... 
for jbd2_journal_start/end. 
if (sb->s_flags & MS_RDONLY) 
if the journal has aborted behind our 
if (journal && is_journal_aborted(journal)) { 
if (err < 0) 
if (!journal) 
if (!ext4_handle_valid(handle)) { 
if (!err) 
if (err) 
if (!ext4_handle_valid(handle)) 
if (err < 0) { 
if (err < 0) 
if (bh) 
if (!handle->h_err) 
if (is_handle_aborted(handle)) 
if (ext4_handle_valid(handle)) { 
if (err) 
if we are freeing data 
forget(const char *where, unsigned int line, handle_t *handle, 
forget(inode, is_metadata, blocknr); 
forgetting bh %p: is_metadata = %d, mode %o, " 
forget and return */ 
forget(bh); 
if we are doing full data 
if (test_opt(inode->i_sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA || 
if (bh) { 
forget"); 
if (err) 
if (err) { 
if (ext4_handle_valid(handle)) { 
if (err) 
if (ext4_handle_valid(handle)) { 
if there is a bug */ 
if (inode == NULL) { 
if (inode) 
if (inode && inode_needs_sync(inode)) { 
if (buffer_req(bh) && !buffer_uptodate(bh)) { 
if (ext4_handle_valid(handle)) { 
if (err) 
file : ./test/kernel/fs/ext4/ioctl.c 
[ OK ] open : 4 ok... 
for @len bytes. 
while (len-- > 0) { 
for the primary swap between inode1 and inode2 
fore you have to make sure, that calling this method twice 
formation from the given @inode and the inode 
if (inode->i_nlink != 1 || !S_ISREG(inode->i_mode)) 
if (!inode_owner_or_capable(inode) || !capable(CAP_SYS_ADMIN)) 
if (IS_ERR(inode_bl)) 
for all existing dio workers */ 
if (IS_ERR(handle)) { 
if (inode_bl->i_nlink == 0) { 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, 
if (err < 0) { 
if (err < 0) { 
if (!inode_owner_or_capable(inode)) 
if (get_user(flags, (int __user *) arg)) 
if (err) 
if (IS_NOQUOTA(inode)) 
ifiable only by root */ 
if ((flags ^ oldflags) & (EXT4_APPEND_FL | EXT4_IMMUTABLE_FL)) { 
if ((jflag ^ oldflags) & (EXT4_JOURNAL_DATA_FL)) { 
if ((flags ^ oldflags) & EXT4_EXTENTS_FL) 
if (flags & EXT4_EOFBLOCKS_FL) { 
if (!(oldflags & EXT4_EOFBLOCKS_FL)) { 
if (oldflags & EXT4_EOFBLOCKS_FL) 
if (IS_ERR(handle)) { 
if (IS_SYNC(inode)) 
if (err) 
for (i = 0, mask = 1; i < 32; i++, mask <<= 1) { 
if (mask & flags) 
if (err) 
if ((jflag ^ oldflags) & (EXT4_JOURNAL_DATA_FL)) 
if (err) 
if (migrate) { 
if (!inode_owner_or_capable(inode)) 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (err) 
if (get_user(generation, (int __user *) arg)) { 
if (IS_ERR(handle)) { 
if (err == 0) { 
if (err) 
if (get_user(n_blocks_count, (__u32 __user *)arg)) { 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (err) 
if (EXT4_SB(sb)->s_journal) { 
if (err == 0) 
if (!(filp->f_mode & FMODE_READ) || 
if (copy_from_user(&me, 
if (!donor.file) 
if (!(donor.file->f_mode & FMODE_WRITE)) { 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (err) 
if (copy_to_user((struct move_extent __user *)arg, 
if (err) 
if (copy_from_user(&input, (struct ext4_new_group_input __user *)arg, 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (err) 
if (EXT4_SB(sb)->s_journal) { 
if (err == 0) 
if (!err && ext4_has_group_desc_csum(sb) && 
if (!inode_owner_or_capable(inode)) 
if (err) 
fore we switch the 
if (!inode_owner_or_capable(inode)) 
if (err) 
if (!(filp->f_mode & FMODE_WRITE)) 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (copy_from_user(&n_blocks_count, (__u64 __user *)arg, 
if (err) 
if (err) 
if (EXT4_SB(sb)->s_journal) { 
if (err == 0) 
if (!err && (o_group > EXT4_SB(sb)->s_groups_count) && 
if (!capable(CAP_SYS_ADMIN)) 
if (!blk_queue_discard(q)) 
if (copy_from_user(&range, (struct fstrim_range __user *)arg, 
if (ret < 0) 
if (copy_to_user((struct fstrim_range __user *)arg, &range, 
ifdef CONFIG_COMPAT 
if (err) 
if 
file : ./test/kernel/fs/ext4/extents.c 
[ OK ] open : 4 ok... 
iffer <pierre.peiffer@bull.net> 
for more details. 
if not, write to the Free Software 
for EXT4 
if split fails \ 
ify(struct inode *inode, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (et->et_checksum != ext4_extent_block_csum(inode, eh)) 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!ext4_handle_valid(handle)) 
if (handle->h_buffer_credits > needed) 
if (err <= 0) 
if (err == 0) 
if (path->p_bh) { 
if (path->p_bh) { 
if (path) { 
if we are writing a sparse file such as 
if the latter case turns out to be 
if (ex) { 
if (block > ext_block) 
if (path[depth].p_bh) 
for a meta data block 
ifdef AGGRESSIVE_TEST 
if 
ifdef AGGRESSIVE_TEST 
if 
ifdef AGGRESSIVE_TEST 
if 
ifdef AGGRESSIVE_TEST 
if 
if (ei->i_da_metadata_calc_len && 
if ((ei->i_da_metadata_calc_len % idxs) == 0) 
if ((ei->i_da_metadata_calc_len % (idxs*idxs)) == 0) 
if ((ei->i_da_metadata_calc_len % (idxs*idxs*idxs)) == 0) { 
if (depth == ext_depth(inode)) { 
if (depth == 0) 
if (lblock > last) 
if (eh->eh_entries == 0) 
if (depth == 0) { 
while (entries) { 
for overlapping extents */ 
if ((lblock <= prev) && prev) { 
while (entries) { 
if (unlikely(eh->eh_magic != EXT4_EXT_MAGIC)) { 
if (unlikely(le16_to_cpu(eh->eh_depth) != depth)) { 
if (unlikely(eh->eh_max == 0)) { 
if (unlikely(le16_to_cpu(eh->eh_max) > max)) { 
if (unlikely(le16_to_cpu(eh->eh_entries) > le16_to_cpu(eh->eh_max))) { 
if (!ext4_valid_extent_entries(inode, eh, depth)) { 
ify checksum on non-root extent tree nodes */ 
ify(inode, eh)) { 
if (unlikely(!bh)) 
if (!bh_uptodate_or_lock(bh)) { 
if (err < 0) 
if (buffer_verified(bh) && !(flags & EXT4_EX_FORCE_CACHE)) 
if (err) 
ified(bh); 
if (!(flags & EXT4_EX_NOCACHE) && depth == 0) { 
for (i = le16_to_cpu(eh->eh_entries); i > 0; i--, ex++) { 
if (prev && (prev != lblk)) 
if (ext4_ext_is_unwritten(ex)) 
formation in the 
if (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if (path == NULL) { 
if there are no external extent blocks */ 
if (ret) 
while (i >= 0) { 
if ((i == depth) || 
if (IS_ERR(bh)) { 
ifdef EXT_DEBUG 
for (k = 0; k <= l; k++, path++) { 
if (path->p_ext) { 
if (!path) 
for inode %lu\n", inode->i_ino); 
if (depth != level) { 
while (idx <= EXT_MAX_INDEX(path[level].p_hdr)) { 
while (ex <= EXT_MAX_EXTENT(path[depth].p_hdr)) { 
if 
for (i = 0; i <= depth; i++, path++) 
for the closest index of the given block 
for %u(idx):  ", block); 
while (l <= r) { 
if (block < le32_to_cpu(m->ei_block)) 
ifdef CHECK_BINSEARCH 
for (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ix++) { 
if (block < le32_to_cpu(ix->ei_block)) 
if 
for closest extent of the given block 
if (eh->eh_entries == 0) { 
for %u:  ", block); 
while (l <= r) { 
if (block < le32_to_cpu(m->ee_block)) 
ifdef CHECK_BINSEARCH 
for (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ex++) { 
if (block < le32_to_cpu(ex->ee_block)) 
if 
if (!path) { 
if (!path) 
while (i) { 
if (IS_ERR(bh)) { 
if (unlikely(ppos > depth)) { 
if not an empty leaf */ 
if (alloc) 
fore @curp or after @curp 
if (err) 
if (unlikely(logical == le32_to_cpu(curp->p_idx->ei_block))) { 
if (unlikely(le16_to_cpu(curp->p_hdr->eh_entries) 
if (logical > le32_to_cpu(curp->p_idx->ei_block)) { 
fore */ 
if (len > 0) { 
if (unlikely(ix > EXT_MAX_INDEX(curp->p_hdr))) { 
if (unlikely(ix > EXT_LAST_INDEX(curp->p_hdr))) { 
if current leaf will be split, then we should use 
if (unlikely(path[depth].p_ext > EXT_MAX_EXTENT(path[depth].p_hdr))) { 
if (path[depth].p_ext != EXT_MAX_EXTENT(path[depth].p_hdr)) { 
if (!ablocks) 
for indexes/leaf\n", depth - at); 
if (newblock == 0) 
if (unlikely(newblock == 0)) { 
if (unlikely(!bh)) { 
if (err) 
if (unlikely(path[depth].p_hdr->eh_entries != 
if (m) { 
if (err) 
if (m) { 
if (err) 
if (err) 
if (unlikely(k < 0)) { 
if (k) 
while (k--) { 
if (unlikely(!bh)) { 
if (err) 
if (unlikely(EXT_MAX_INDEX(path[i].p_hdr) != 
if (m) { 
if (err) 
if (m) { 
if (err) 
if (err) 
if (bh) { 
if (err) { 
for (i = 0; i < depth; i++) { 
if (newblock == 0) 
if (unlikely(!bh)) 
if (err) { 
if (ext_depth(inode)) 
if (err) 
if (neh->eh_depth == 0) { 
if no free index is found, then it requests in-depth growing. 
for free index entry */ 
while (i > 0 && !EXT_HAS_FREE_INDEX(curp)) { 
for index block, 
if (EXT_HAS_FREE_INDEX(curp)) { 
if (err) 
if (IS_ERR(path)) 
if (err) 
if (IS_ERR(path)) { 
if (path[depth].p_hdr->eh_entries == path[depth].p_hdr->eh_max) { 
for *logical 
if *logical is the smallest allocated block, the function 
if (unlikely(path == NULL)) { 
if (depth == 0 && path->p_ext == NULL) 
if (*logical < le32_to_cpu(ex->ee_block)) { 
while (--depth >= 0) { 
if (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) { 
if (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) { 
for *logical 
if *logical is the largest allocated block, the function 
if (unlikely(path == NULL)) { 
if (depth == 0 && path->p_ext == NULL) 
if (*logical < le32_to_cpu(ex->ee_block)) { 
while (--depth >= 0) { 
if (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) { 
if (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) { 
if (ex != EXT_LAST_EXTENT(path[depth].p_hdr)) { 
for index to the right */ 
while (--depth >= 0) { 
if (ix != EXT_LAST_INDEX(path[depth].p_hdr)) 
while (++depth < path->p_depth) { 
if (IS_ERR(bh)) 
if (IS_ERR(bh)) 
if (bh) 
if (depth == 0 && path->p_ext == NULL) 
while (depth >= 0) { 
if (path[depth].p_ext && 
if (path[depth].p_idx != 
if (depth == 0) 
while (depth >= 0) { 
if leaf gets modified and modified extent is first in the leaf, 
if (unlikely(ex == NULL || eh == NULL)) { 
if (depth == 0) { 
if (ex != EXT_FIRST_EXTENT(eh)) { 
if border is smaller than current one 
if (err) 
if (err) 
while (k--) { 
if (path[k+1].p_idx != EXT_FIRST_INDEX(path[k+1].p_hdr)) 
if (err) 
if (err) 
if (ext4_ext_is_unwritten(ex1) != ext4_ext_is_unwritten(ex2)) 
if (le32_to_cpu(ex1->ee_block) + ext1_ee_len != 
for preallocated extents to be added 
if (ext1_ee_len + ext2_ee_len > EXT_INIT_MAX_LEN) 
if (ext4_ext_is_unwritten(ex1) && 
ifdef AGGRESSIVE_TEST 
if 
if the extents (ex and ex+1) were _not_ merged and returns 
while (ex < EXT_LAST_EXTENT(eh)) { 
if (unwritten) 
if (ex + 1 < EXT_LAST_EXTENT(eh)) { 
if (!eh->eh_entries) 
if we can collapse 
if ((path[0].p_depth != 1) || 
ify the block allocation bitmap and the block 
if (ext4_journal_extend(handle, 2)) 
if merge left else 0. 
if (ex > EXT_FIRST_EXTENT(eh)) 
if (!merge_done) 
if a portion of the "newext" extent overlaps with an 
if (!path[depth].p_ext) 
if the extent in the path 
fore the requested block(s) 
if (b2 < b1) { 
if (b2 == EXT_MAX_BLOCKS) 
for wrap through zero on extent logical start block*/ 
for overlap */ 
if (unlikely(ext4_ext_get_actual_len(newext) == 0)) { 
if (unlikely(path[depth].p_hdr == NULL)) { 
if (ex && !(gb_flags & EXT4_GET_BLOCKS_PRE_IO)) { 
if (ex < EXT_LAST_EXTENT(eh) && 
if ((ex > EXT_FIRST_EXTENT(eh)) && 
if (ext4_can_extents_be_merged(inode, ex, newext)) { 
if (err) 
if (unwritten) 
if (ext4_can_extents_be_merged(inode, newext, ex)) { 
if (err) 
if (unwritten) 
if (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) 
for us? */ 
if (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block)) 
if (next != EXT_MAX_BLOCKS) { 
if (IS_ERR(npath)) 
if (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) { 
if (gb_flags & EXT4_GET_BLOCKS_METADATA_NOFAIL) 
if (err) 
if (err) 
if (!nearex) { 
if (le32_to_cpu(newext->ee_block) 
fore: " 
fore */ 
if (len > 0) { 
if (!(gb_flags & EXT4_GET_BLOCKS_PRE_IO)) 
if (err) 
if (npath) { 
while (block < last && block != EXT_MAX_BLOCKS) { 
for this block */ 
if (path && ext_depth(inode) != depth) { 
if (IS_ERR(path)) { 
if (unlikely(path[depth].p_hdr == NULL)) { 
if (!ex) { 
if (le32_to_cpu(ex->ee_block) > block) { 
fore found extent */ 
if (block + num < end) 
if (block >= le32_to_cpu(ex->ee_block) 
if (end >= next) 
if (block >= le32_to_cpu(ex->ee_block)) { 
if (block + num < end) 
if (!exists) { 
if (ext4_ext_is_unwritten(ex)) 
if (!exists && next_del) { 
if (unlikely(es.es_len == 0)) { 
iff next == next_del == EXT_MAX_BLOCKS. 
if (next == next_del && next == EXT_MAX_BLOCKS) { 
if (unlikely(next_del != EXT_MAX_BLOCKS || 
if (exists) { 
if (err < 0) 
if (err == 1) { 
if (path) { 
if (ex == NULL) { 
if (block < le32_to_cpu(ex->ee_block)) { 
fore): %u [%u:%u]", 
if (!ext4_find_delalloc_range(inode, lblock, lblock + len - 1)) 
if (block >= le32_to_cpu(ex->ee_block) 
if (!ext4_find_delalloc_range(inode, lblock, lblock + len - 1)) 
if (unlikely(path->p_hdr->eh_entries == 0)) { 
if (err) 
if (path->p_idx != EXT_LAST_INDEX(path->p_hdr)) { 
if (err) 
while (--depth >= 0) { 
if (err) 
if (err) 
for_single_extent: 
for_single_extent(struct inode *inode, int nrblocks, 
if (path) { 
if (le16_to_cpu(path[depth].p_hdr->eh_entries) 
for leaf block credit 
if (ext4_has_inline_data(inode)) 
if (extents <= 1) 
if (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode)) 
if (ext4_should_journal_data(inode)) 
if we 
ifferent from the 
if ((*partial_cluster > 0) && 
ifdef EXTENTS_STATS 
if (ee_len < sbi->s_ext_min) 
if (ee_len > sbi->s_ext_max) 
if (ext_depth(inode) > sbi->s_depth_max) 
if 
for the situation when the cluster is still 
if (*partial_cluster < 0 && 
if we determine that the truncate operation has 
if we did not manage to free the whole 
if (unaligned && (ee_len == num) && 
if (unaligned) 
if (*partial_cluster > 0) 
if "start" and "end" appear in the same extent 
if all extents 
if (!path[depth].p_hdr) 
if (unlikely(path[depth].p_hdr == NULL)) { 
if (!ex) 
if it shares a cluster with the extent to 
if (ex != EXT_LAST_EXTENT(eh)) { 
if (current_cluster == right_cluster && 
while (ex >= EXT_FIRST_EXTENT(eh) && 
if (ext4_ext_is_unwritten(ex)) 
if (end < ex_ee_block) { 
if this extent is not cluster aligned we have 
if (EXT4_PBLK_COFF(sbi, pblk)) 
if (b != ex_ee_block + ex_ee_len - 1) { 
if (a != ex_ee_block) { 
for leaf, sb, and inode plus 2 (bmap and group 
for 
if (ex == EXT_FIRST_EXTENT(eh)) { 
if (err) 
if (err) 
if (err) 
if (num == 0) 
if all the blocks in the 
if (unwritten && num) 
if (num == 0) { 
if (*partial_cluster > 0) 
if (err) 
if (correct_index && eh->eh_entries) 
if it isn't shared with the 
if it's not shared with 
if (*partial_cluster > 0 && eh->eh_entries && 
if this leaf is free, then we should 
if (err == 0 && eh->eh_entries == 0 && path[depth].p_bh != NULL) 
if current index has to be freed (even partial) 
if (path->p_idx < EXT_FIRST_INDEX(path->p_hdr)) 
if truncate on deeper level happened, it wasn't partial, 
for truncation 
if (le16_to_cpu(path->p_hdr->eh_entries) == path->p_block) 
if (IS_ERR(handle)) 
if we are removing extents inside the extent tree. If that 
if (end < EXT_MAX_BLOCKS - 1) { 
for this block */ 
if (IS_ERR(path)) { 
if inode has no blocks at all */ 
if (!ex) { 
if the last block is inside the extent, if so split 
if (end >= ee_block && 
if (ext4_ext_is_unwritten(ex)) 
if that happens. 
if (err < 0) 
if (path) { 
while (--k > 0) 
if (path == NULL) { 
if (ext4_ext_check(inode, path[0].p_hdr, depth, 0)) { 
while (i >= 0 && err == 0) { 
if (!path[i].p_hdr) { 
if (!path[i].p_idx) { 
if (ext4_ext_more_to_rm(path + i)) { 
if (IS_ERR(bh)) { 
if we did IO above. */ 
if (WARN_ON(i + 1 > depth)) { 
if (path[i].p_hdr->eh_entries == 0 && i > 0) { 
if (partial_cluster > 0 && path->p_hdr->eh_entries == 0) { 
if (path->p_hdr->eh_entries == 0) { 
if (err == 0) { 
if (err == -EAGAIN) { 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) { 
ifdef AGGRESSIVE_TEST 
if 
if 
if 
if 
if 
if (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) 
ifdef EXTENTS_STATS 
if 
if (ee_len == 0) 
if (ret > 0) 
if the extent could be zeroout if split fails, and 
if (err) 
if (split == ee_block) { 
if (split_flag & EXT4_EXT_MARK_UNWRIT2) 
if (!(flags & EXT4_GET_BLOCKS_PRE_IO)) 
if (split_flag & EXT4_EXT_MARK_UNWRIT1) 
if (err) 
if (split_flag & EXT4_EXT_MARK_UNWRIT2) 
if (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) { 
if (split_flag & EXT4_EXT_DATA_VALID1) { 
if (err) 
if (err) 
if (err) 
if (map->m_lblk + map->m_len < ee_block + ee_len) { 
if (unwritten) 
if (split_flag & EXT4_EXT_DATA_VALID2) 
if (err) 
if (IS_ERR(path)) 
if (!ex) { 
if (map->m_lblk >= ee_block) { 
if (unwritten) { 
if (err) 
if someone tries to write 
if (eof_block < map->m_lblk + map_len) 
for workloads doing fallocate(FALLOC_FL_KEEP_SIZE) 
if the transfer 
if ((map->m_lblk == ee_block) && 
if ((!ext4_ext_is_unwritten(abut_ex)) &&		/*C1*/ 
if (err) 
ift the start of ex by 'map_len' blocks */ 
if (((map->m_lblk + map_len) == (ee_block + ee_len)) && 
if we can merge right */ 
if ((!ext4_ext_is_unwritten(abut_ex)) &&		/*C1*/ 
if (err) 
ift the start of abut_ex by 'map_len' blocks */ 
if (allocated) { 
if extent is fully inside i_size or new_size. 
if (EXT4_EXT_MAY_ZEROOUT & split_flag) 
if (max_zeroout && (ee_len <= max_zeroout)) { 
if (err) 
if (err) 
if (max_zeroout && (allocated > map->m_len)) { 
if (err) 
if (map->m_lblk - ee_block + map->m_len < max_zeroout) { 
if (map->m_lblk != ee_block) { 
if (err) 
if (allocated < 0) 
if (!err) 
if the extent tree grow after 
fore DIO submit 
if (eof_block < map->m_lblk + map->m_len) 
if extent is fully insde i_size or new_size. 
if (flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN) { 
if (flags & EXT4_GET_BLOCKS_CONVERT) { 
if (ee_block != map->m_lblk || ee_len > map->m_len) { 
if (err < 0) 
if (IS_ERR(path)) { 
if (!ex) { 
if (err) 
ified extent as dirty */ 
if (ee_block != map->m_lblk || ee_len > map->m_len) { 
if 
if (err < 0) 
if (IS_ERR(path)) { 
if (err) 
ified extent as dirty */ 
for (i = 0; i < count; i++) 
if necessary 
if (!ext4_test_inode_flag(inode, EXT4_INODE_EOFBLOCKS)) 
for this case anymore. Simply remove the flag 
if (unlikely(!eh->eh_entries)) 
if we are writing the 
if the caller to 
if (lblk + len < le32_to_cpu(last_ex->ee_block) + 
if the current extent is the last extent in the file, by 
for (i = depth-1; i >= 0; i--) 
if there is a delalloc block in the range, otherwise 0. 
if (es.es_len == 0) 
if (es.es_lblk <= lblk_start && 
if (lblk_start <= es.es_lblk && es.es_lblk <= lblk_end) 
ified by the 'map') 
for. 
fore the delayed allocation could be resolved. 
for delayed allocation. In this case, we will exclude that 
if we 
for this allocation */ 
if (c_offset) { 
if (ext4_find_delalloc_range(inode, lblk_from, lblk_to)) 
if (allocated_clusters && c_offset) { 
if (ext4_find_delalloc_range(inode, lblk_from, lblk_to)) 
if (map->m_len > EXT_UNWRITTEN_MAX_LEN) 
if (ret >= 0) { 
if (allocated > map->m_len) 
if needed. 
fore submit the IO, split the extent */ 
if (ret <= 0) 
if (io) 
if (flags & EXT4_GET_BLOCKS_CONVERT) { 
if (ret >= 0) { 
if (allocated > map->m_len) 
if (flags & EXT4_GET_BLOCKS_UNWRIT_EXT) { 
if ((flags & EXT4_GET_BLOCKS_CREATE) == 0) { 
for us.  But 
if (ret >= 0) 
if (ret <= 0) { 
if we allocated more blocks than requested 
if (allocated > map->m_len) { 
for this offset. So cancel these reservation 
if (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) { 
if (reserved_clusters) 
if ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) { 
if (err < 0) 
if (allocated > map->m_len) 
if the requested 
for is this one: 
for data in 
if ((rr_cluster_start == ex_cluster_end) || 
if (rr_cluster_start == ex_cluster_end) 
for and handle this case: 
if (map->m_lblk < ee_block) 
for the case where there is already another allocated 
if (map->m_lblk > ee_block) { 
for extents based files 
if not allocating file system block 
if create == 0 and these are pre-allocated blocks 
if plain look up failed (blocks have not been allocated) 
for inode %lu\n", 
for this block */ 
if (IS_ERR(path)) { 
ification; 
if (unlikely(path[depth].p_ext == NULL && depth != 0)) { 
if (ex) { 
if found extent covers block, simply return it */ 
if ((!ext4_ext_is_unwritten(ex)) && 
if (!ext4_ext_is_unwritten(ex)) 
if (ret < 0) 
if ((sbi->s_cluster_ratio > 1) && 
if create flag is zero 
if ((flags & EXT4_GET_BLOCKS_CREATE) == 0) { 
if ((flags & EXT4_GET_BLOCKS_NO_PUT_HOLE) == 0) 
if the extent returned 
if (cluster_offset && ex && 
if (err) 
if (err) 
if the extent after searching to the right implies a 
if ((sbi->s_cluster_ratio > 1) && ex2 && 
if request is beyond maximum number of blocks we can have in 
for an unwritten extent this limit is 
if (map->m_len > EXT_INIT_MAX_LEN && 
if (map->m_len > EXT_UNWRITTEN_MAX_LEN && 
if we can really insert (m_lblk)::(m_lblk + m_len) extent */ 
if (err) 
for the logical block number, since when we allocate a 
if (S_ISREG(inode->i_mode)) 
for non-regular files */ 
if (flags & EXT4_GET_BLOCKS_NO_NORMALIZE) 
if (!newblock) 
if (ar.len > allocated) 
if (flags & EXT4_GET_BLOCKS_UNWRIT_EXT){ 
for every IO write to an 
form conversion when IO is done. 
if (flags & EXT4_GET_BLOCKS_PRE_IO) 
if ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) 
if (!err) 
if (!err && set_unwritten) { 
if (err && free_on_err) { 
if (allocated > map->m_len) 
if (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) { 
if (map->m_flags & EXT4_MAP_FROM_CLUSTER) { 
for this range. 
if (reserved_clusters < allocated_clusters) { 
for 
for this write. 
for [0-3] and [4-7]; and not for [8-11] as 
for writing these 
for all newly allocated blocks. 
if ((flags & EXT4_GET_BLOCKS_UNWRIT_EXT) == 0) 
if (allocated > map->m_len) 
if (path) { 
if (err == -ENOMEM) { 
if (err) { 
if it can fit in one extent so 
if (len <= EXT_UNWRITTEN_MAX_LEN) 
while (ret >= 0 && ret < len) { 
if (IS_ERR(handle)) { 
if (ret <= 0) { 
if (ret2) 
if (ret == -ENOSPC && 
if (!S_ISREG(inode->i_mode)) 
force_commit to flush all data in case of data=journal. */ 
force_commit(inode->i_sb); 
if (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) { 
if (ret) 
if (start < offset || end > offset + len) 
if (max_blocks < lblk) 
if (mode & FALLOC_FL_KEEP_SIZE) 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) { 
if (!(mode & FALLOC_FL_KEEP_SIZE) && 
if (ret) 
if (partial) 
if (max_blocks > 0) { 
if (ret) 
if (ret) 
if (IS_ERR(handle)) { 
if (new_size) { 
if (new_size > EXT4_I(inode)->i_disksize) 
if the new size is the same as i_size. 
if ((offset + len) > i_size_read(inode)) 
if (file->f_flags & O_SYNC) 
for a file. This implements ext4's fallocate file 
for file systems which do not support fallocate() system call). 
if mode is not supported */ 
if (mode & FALLOC_FL_PUNCH_HOLE) 
if (ret) 
for extent-based 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) 
if (mode & FALLOC_FL_COLLAPSE_RANGE) 
if (mode & FALLOC_FL_ZERO_RANGE) 
if (mode & FALLOC_FL_KEEP_SIZE) 
if (!(mode & FALLOC_FL_KEEP_SIZE) && 
if (ret) 
if (ret) 
if (IS_ERR(handle)) 
if (new_size) { 
if (new_size > EXT4_I(inode)->i_disksize) 
if the new size is the same as i_size. 
if ((offset + len) > i_size_read(inode)) 
if (file->f_flags & O_SYNC) 
for conversion of each extent separately. 
if (handle) { 
if (IS_ERR(handle)) 
while (ret >= 0 && ret < max_blocks) { 
if (credits) { 
if (IS_ERR(handle)) { 
if (ret <= 0) 
if (credits) 
if (ret <= 0 || ret2) 
if (!credits) 
if no delayed 
if (newes->es_pblk == 0) { 
if (es.es_len == 0) 
if (es.es_lblk > newes->es_lblk) { 
if (es.es_len == 0) 
ified here */ 
if (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) { 
if (error) 
if (physical) 
if (ext4_has_inline_data(inode)) { 
if (has_inline) 
if (fieinfo->fi_flags & FIEMAP_FLAG_CACHE) { 
if (error) 
if not in extents fmt */ 
if (fiemap_check_flags(fieinfo, EXT4_FIEMAP_FLAGS)) 
if (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) { 
if (last_blk >= EXT_MAX_BLOCKS) 
formation 
for marking it dirty. 
if (!ext4_handle_valid(handle)) 
if need to extend journal credits 
for leaf, sb, and inode plus 2 (bmap and group 
if (handle->h_buffer_credits < 7) { 
if (err && err != -EAGAIN) 
ift_path_extents: 
ift 
for each extent. 
ift_path_extents(struct ext4_ext_path *path, ext4_lblk_t shift, 
while (depth >= 0) { 
if (!ex_start) 
if (!ex_last) 
if (err) 
if (ex_start == EXT_FIRST_EXTENT(path[depth].p_hdr)) 
while (ex_start <= ex_last) { 
if ((ex_start > 
if (err) 
if (--depth < 0 || !update) 
if (err) 
ift); 
if (err) 
if current index is not a starting index */ 
ift_extents: 
ifted downwards by shift blocks. 
ift_extents(struct inode *inode, handle_t *handle, 
if (IS_ERR(path)) 
if (!extent) { 
ift, if hole is at the end of file */ 
ifting extents until we make sure the hole is big 
if (IS_ERR(path)) 
if (extent) { 
if ((start == ex_start && shift > ex_start) || 
while (start < stop_block) { 
if (IS_ERR(path)) 
if (!extent) { 
if (start > current_block) { 
if (ret != 0) { 
if (ret == 1) 
ift_path_extents(path, shift, inode, 
if (ret) 
for ext4 
if (offset & (EXT4_BLOCK_SIZE(sb) - 1) || 
if (!S_ISREG(inode->i_mode)) 
if (EXT4_SB(inode->i_sb)->s_cluster_ratio > 1) 
force_commit to flush all data in case of data=journal. */ 
force_commit(inode->i_sb); 
for page size > block size. 
if (ret) 
if (offset + len >= i_size_read(inode)) { 
for extent based files */ 
for existing dio to complete */ 
if (IS_ERR(handle)) { 
if (ret) { 
if (ret) { 
ift_extents(inode, handle, punch_stop, 
if (ret) { 
if (IS_SYNC(inode)) 
file : ./test/kernel/fs/ext4/dir.c 
[ OK ] open : 4 ok... 
if the given dir-inode refers to an htree-indexed directory 
if it is a dx dir, 0 if not 
if (EXT4_HAS_COMPAT_FEATURE(inode->i_sb, 
if the directory entry is OK, and 1 if there is a problem 
if (unlikely(rlen < EXT4_DIR_REC_LEN(1))) 
if (unlikely(rlen % 4 != 0)) 
if (unlikely(rlen < EXT4_DIR_REC_LEN(de->name_len))) 
for name_len"; 
if (unlikely(le32_to_cpu(de->inode) > 
if (filp) 
if (is_dx_dir(inode)) { 
if (err != ERR_BAD_DX_DIR) { 
if (ext4_has_inline_data(inode)) { 
if (has_inline_data) 
while (ctx->pos < inode->i_size) { 
if (err > 0) { 
if (!ra_has_index(&file->f_ra, index)) 
if (!bh) { 
if (ctx->pos > inode->i_blocks << 9) 
if (!buffer_verified(bh) && 
ified(bh); 
if (file->f_version != inode->i_version) { 
for (i = 0; i < sb->s_blocksize && i < offset; ) { 
if (ext4_rec_len_from_disk(de->rec_len, 
while (ctx->pos < inode->i_size 
if (ext4_check_dir_entry(inode, file, de, bh, 
if (le32_to_cpu(de->inode)) { 
if (ctx->pos < inode->i_size) { 
ifdef CONFIG_COMPAT 
if 
for dx directories 
ified. 
if ((filp->f_mode & FMODE_32BITHASH) || 
if ((filp->f_mode & FMODE_32BITHASH) || 
if ((filp->f_mode & FMODE_32BITHASH) || 
for dx directories 
if ((filp->f_mode & FMODE_32BITHASH) || 
if (likely(dx_dir)) 
for_each_entry_safe(fname, next, root, rb_hash) 
while (fname) { 
if (!p) 
if (!new_fn) 
while (*p) { 
if ((new_fn->hash == fname->hash) && 
if (new_fn->hash < fname->hash) 
if (new_fn->hash > fname->hash) 
if (new_fn->minor_hash < fname->minor_hash) 
if (new_fn->minor_hash > fname->minor_hash) */ 
for ext4_dx_readdir.  It calls filldir 
if (!fname) { 
while (fname) { 
if (!info) { 
if (!info) 
if (ctx->pos == ext4_get_htree_eof(file)) 
if (info->last_pos != ctx->pos) { 
if (info->extra_fname) { 
if (!info->curr_node) 
while (1) { 
if we have no more entries, 
if ((!info->curr_node) || 
if (ret < 0) 
if (ret == 0) { 
if (call_filldir(file, ctx, fname)) 
if (info->curr_node) { 
if (info->next_hash == ~0) { 
if (filp->private_data) 
ifdef CONFIG_COMPAT 
if 
file : ./test/kernel/fs/ext4/indirect.c 
[ OK ] open : 4 ok... 
if the referred-to block is likely to be 
for UNIX filesystems - tree of pointers anchored in the inode, with 
ifferently, because otherwise on an 
if our filesystem had 8Kb blocks. We might use long long, but that would 
if (i_block < direct_blocks) { 
if ((i_block -= direct_blocks) < indirect_blocks) { 
if ((i_block -= indirect_blocks) < double_blocks) { 
if (((i_block -= double_blocks) >> (ptrs_bits * 2)) < ptrs) { 
if (boundary) 
if everything went OK or the pointer to the last filled triple 
for i==0 and into the bh->b_data 
for i>0 and NULL for i==0. In other words, it holds the block 
ify that chain did not change) and buffer_heads hosting these 
if (!p->key) 
while (--depth) { 
if (unlikely(!bh)) { 
if (!bh_uptodate_or_lock(bh)) { 
if (ext4_check_indirect_blockref(inode, bh)) { 
if (!p->key) 
for allocation with sufficient locality 
for block allocation. 
if there is a block to the left of our position - allocate near it. 
if pointer will live in inode - allocate in the same 
ifferent inode 
for (p = ind->p - 1; p >= start; p--) { 
if (ind->bh) 
for allocation. 
for block allocation, 
for non-extent files, we limit the block nr 
for the given branch. 
for indirect blocks 
if (k > 0) { 
if (blks < blocks_to_boundary + 1) 
while (count < blks && count <= blocks_to_boundary && 
for this transaction 
for allocation 
if we are synchronous) writes them to disk. 
formation about that chain in the branch[], in 
forget 
for the direct block allocation 
if (S_ISREG(inode->i_mode)) 
for (i = 0; i <= indirect_blks; i++) { 
if (err) { 
if (i == 0) 
if (unlikely(!bh)) { 
if (err) { 
if (i == indirect_blks) 
for (j = 0; j < len; j++) 
if (err) 
for (; i >= 0; i--) { 
forget() only freshly allocated indirect 
fore ext4_alloc_branch() was called. 
if (i > 0 && i != indirect_blks && branch[i].bh) 
forget(handle, 1, inode, branch[i].bh, 
for this transaction 
fore the splice. 
if (where->bh) { 
if (err) 
if (num == 0 && blks > 1) { 
for (i = 1; i < blks; i++) 
if (where->bh) { 
if it is being spliced 
if (err) 
for (i = 1; i <= num; i++) { 
for ext4_map_blocks(). 
fore attaching anything 
if check fails, otherwise 
force the 
if create == 0. 
if plain lookup failed. 
if allocating filesystem 
if not allocating file system 
if (depth == 0) 
if (!partial) { 
while (count < map->m_len && count <= blocks_to_boundary) { 
if (blk == first_block + count) 
if ((flags & EXT4_GET_BLOCKS_CREATE) == 0 || err == -EIO) 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
for " 
for [d,t]indirect blocks */ 
for this branch. 
while we alter the tree 
forget any buffers 
for bitmaps where the 
if (!err) 
if (err) 
if (count > blocks_to_boundary) 
while (partial > chain) { 
for ext3 (or indirect map) based files 
if the machine crashes during the write. 
if (rw == WRITE) { 
if (final_size > inode->i_size) { 
for sb + inode write */ 
if (IS_ERR(handle)) { 
if (ret) { 
if (rw == READ && ext4_should_dioread_nolock(inode)) { 
while holding extra i_dio_count ref. 
if (unlikely(ext4_test_inode_state(inode, 
if (unlikely((rw & WRITE) && ret < 0)) { 
if (end > isize) 
if (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries)) 
if (orphan) { 
for sb + inode write */ 
if (IS_ERR(handle)) { 
if (inode->i_nlink) 
if (inode->i_nlink) 
if (ret > 0) { 
if (end > inode->i_size) { 
if (ret == 0) 
for non extent file based file 
if (lblock < EXT4_NDIR_BLOCKS) 
if (ei->i_da_metadata_calc_len && 
for the purposes of truncation.  If 
if we managed to create more room.  If we can't create more 
if (!ext4_handle_valid(handle)) 
if (ext4_handle_has_enough_credits(handle, EXT4_RESERVE_TRANS_BLOCKS+1)) 
if (!ext4_journal_extend(handle, ext4_blocks_for_truncate(inode))) 
for first non-zero word 
while (p < q) 
for partial truncation. 
if some data below the new i_size is referred 
for (k = depth; k > 1 && !offsets[k-1]; k--) 
if (!partial) 
if (!partial->key && *partial->p) 
for (p = partial; (p > chain) && all_zeroes((__le32 *) p->bh->b_data, p->p); p--) 
if that rest 
if (p == chain + k - 1 && p > chain) { 
if 0 
if 
while (partial > p) { 
ification. 
if (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode)) 
if (ext4_should_journal_data(inode)) 
if (!ext4_data_block_valid(EXT4_SB(inode->i_sb), block_to_free, 
if (try_to_extend_transaction(handle, inode)) { 
if (unlikely(err)) 
if (unlikely(err)) 
for_truncate(inode)); 
if (bh) { 
if (unlikely(err)) 
for (p = first; p < last; p++) 
for this transaction 
if these 
if @first and @last point into the inode's direct 
for current block */ 
if (this_bh) {				/* For indirect block */ 
if we can't update the indirect pointers 
if (err) 
for (p = first; p < last; p++) { 
if (nr) { 
if (count == 0) { 
if (nr == block_to_free + count) { 
if (err) 
if (!err && count > 0) 
if (err < 0) 
if (this_bh) { 
if the data is corrupted and an indirect 
for this instead of OOPSing. 
if ((EXT4_JOURNAL(inode) == NULL) || bh2jh(this_bh)) 
for this transaction 
if (ext4_handle_is_aborted(handle)) 
if (depth--) { 
while (--p >= first) { 
if (!nr) 
if (!ext4_data_block_valid(EXT4_SB(inode->i_sb), 
for the next level down */ 
if (!bh) { 
if extend_transaction() 
for some reason fails to put the bitmap changes and 
if (ext4_handle_is_aborted(handle)) 
if (try_to_extend_transaction(handle, inode)) { 
for_truncate(inode)); 
if 
former) 
if (parent_bh) { 
if (!ext4_journal_get_write_access(handle, 
if (last_block != max_block) { 
if (n == 0) 
fore the truncate completes, so it is now safe to propagate 
if (last_block == max_block) { 
if last_block is 
if (n == 1) {		/* direct blocks */ 
if (nr) { 
for it here. 
while (partial > chain) { 
if (nr) { 
if (nr) { 
if (nr) { 
for (i = 0, offset = 0; i < max; i++, i_data++, offset += inc) { 
if (*i_data == 0 || (offset + inc) <= first) 
if (level > 0) { 
if (!bh) { 
if (first > offset) { 
if (ret) { 
if (level == 0 || 
for (level = 0; level < 4; level++, max *= addr_per_block) { 
if (ret) 
if (count > max - first) 
if (level == 0) { 
file : ./test/kernel/fs/ext4/inode.c 
[ OK ] open : 4 ok... 
forms by Jakub Jelinek 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE && 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE && 
ify(struct inode *inode, struct ext4_inode *raw, 
if (EXT4_SB(inode->i_sb)->s_es->s_creator_os != 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE && 
if (EXT4_SB(inode->i_sb)->s_es->s_creator_os != 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE && 
for 
if (!EXT4_I(inode)->jinode) 
if (ext4_has_inline_data(inode)) 
fore we call here everything must be consistently dirtied against 
for blocks inside i_size since 
if i_nlink is zero. 
if (inode->i_nlink) { 
for reaping the inode might still have some pages to 
if we did not discard these 
if he tries to 
fore the transaction is checkpointed. So be 
if (ext4_should_journal_data(inode) && 
if (!is_bad_inode(inode)) 
if (ext4_should_order_data(inode)) 
if (is_bad_inode(inode)) 
for_truncate(inode)+3); 
if (IS_SYNC(inode)) 
if (err) { 
if (inode->i_blocks) 
fore there may not be 
if (!ext4_handle_has_enough_credits(handle, 3)) { 
if (err > 0) 
if (err != 0) { 
if'. 
if ext4_truncate() actually created an orphan record. 
if anything has gone wrong 
if the mark_dirty 
if (ext4_mark_inode_dirty(handle, inode)) 
ifdef CONFIG_QUOTA 
if 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if (unlikely(used > ei->i_reserved_data_blocks)) { 
if (unlikely(ei->i_allocated_meta_blocks > ei->i_reserved_meta_blocks)) { 
if (ei->i_reserved_data_blocks == 0) { 
for data blocks */ 
for fallocated blocks. 
if 
if ((ei->i_reserved_data_blocks == 0) && 
if (!ext4_data_block_valid(EXT4_SB(inode->i_sb), map->m_pblk, 
ifdef ES_AGGRESSIVE_TEST 
if (!(flags & EXT4_GET_BLOCKS_NO_LOCK)) 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) { 
if (!(flags & EXT4_GET_BLOCKS_NO_LOCK)) 
if (es_map->m_lblk != map->m_lblk || 
for inode: %lu " 
if /* ES_AGGRESSIVE_TEST */ 
if the blocks are already mapped. 
if create==0 and the blocks are pre-allocated and unwritten block, 
if plain look up failed (blocks have not been allocated), in 
ifdef ES_AGGRESSIVE_TEST 
if 
if (unlikely(map->m_len > INT_MAX)) 
if (unlikely(map->m_lblk >= EXT_MAX_BLOCKS)) 
if (ext4_es_lookup_extent(inode, map->m_lblk, &es)) { 
if (ext4_es_is_written(&es) || ext4_es_is_unwritten(&es)) { 
if (retval > map->m_len) 
if (ext4_es_is_delayed(&es) || ext4_es_is_hole(&es)) { 
ifdef ES_AGGRESSIVE_TEST 
if 
if we can get the block without requesting a new 
if (!(flags & EXT4_GET_BLOCKS_NO_LOCK)) 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) { 
if (retval > 0) { 
if (unlikely(retval != map->m_len)) { 
for inode " 
if (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) && 
if (ret < 0) 
if (!(flags & EXT4_GET_BLOCKS_NO_LOCK)) 
if (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) { 
if (ret != 0) 
if ((flags & EXT4_GET_BLOCKS_CREATE) == 0) 
if the blocks have already allocated 
if blocks have been preallocated 
if (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) 
if (!(flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN)) 
if the caller is from delayed allocation writeout path 
for allocation 
if (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) 
for EXT4 here because migrate 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) { 
if (retval > 0 && map->m_flags & EXT4_MAP_NEW) { 
format changing.  Force the migrate 
for non extent files. So we can update 
if ((retval > 0) && 
if (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) 
if (retval > 0) { 
if (unlikely(retval != map->m_len)) { 
for inode " 
if ((flags & EXT4_GET_BLOCKS_PRE_IO) && 
if (ext4_es_is_written(&es)) 
if (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) && 
if (ret < 0) 
if (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) { 
if (ret != 0) 
for direct IO at once. */ 
if (ext4_has_inline_data(inode)) 
if (flags && !(flags & EXT4_GET_BLOCKS_NO_LOCK) && !handle) { 
if (map.m_len > DIO_MAX_BLOCKS) 
if (IS_ERR(handle)) { 
if (ret > 0) { 
if (io_end && io_end->flag & EXT4_IO_END_UNWRITTEN) 
if (started) 
if create is zero 
if (create && err == 0) 
if (err < 0) 
if (err <= 0) 
if (unlikely(!bh)) { 
if (map.m_flags & EXT4_MAP_NEW) { 
if (!fatal && !buffer_uptodate(bh)) { 
if (!fatal) 
if (fatal) { 
if (!bh) 
if (buffer_uptodate(bh)) 
if (buffer_uptodate(bh)) 
for (bh = head, block_start = 0; 
if (block_end <= from || block_start >= to) { 
if (!ret) 
while thus 
if another thread had a 
for the tiny quotafile 
if (!buffer_mapped(bh) || buffer_freed(bh)) 
fore releasing a page lock and thus writeback cannot 
if (dirty) 
if (!ret && dirty) 
for addition to orphan list in case 
if (ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA)) { 
if (ret < 0) 
if (ret == 1) 
if the 
fore we start 
if needed) without using GFP_NOFS. 
if (!page) 
if (IS_ERR(handle)) { 
if (page->mapping != mapping) { 
while the page was unlocked */ 
if (ext4_should_dioread_nolock(inode)) 
if (!ret && ext4_should_journal_data(inode)) { 
if (ret) { 
fore 
if (pos + len > inode->i_size && ext4_can_truncate(inode)) 
if (pos + len > inode->i_size) { 
if (inode->i_nlink) 
if (ret == -ENOSPC && 
if (!buffer_mapped(bh) || buffer_freed(bh)) 
if (ext4_test_inode_state(inode, EXT4_STATE_ORDERED_MODE)) { 
if (ret) { 
if (ext4_has_inline_data(inode)) { 
if (ret < 0) 
while still holding page lock: 
if (pos + copied > inode->i_size) { 
if (pos + copied > EXT4_I(inode)->i_disksize) { 
forces lock 
if (i_size_changed) 
if (pos + len > inode->i_size && ext4_can_truncate(inode)) 
if (!ret) 
if (pos + len > inode->i_size) { 
if (inode->i_nlink) 
if (ext4_has_inline_data(inode)) 
if (copied < len) { 
if (!partial) 
if (new_i_size > inode->i_size) 
if (new_i_size > EXT4_I(inode)->i_disksize) { 
if (!ret) 
if (pos + len > inode->i_size && ext4_can_truncate(inode)) 
if (!ret) 
if (pos + len > inode->i_size) { 
if (inode->i_nlink) 
for a single block located at lblock 
if we fail to claim space. 
ford to run out of free blocks. 
if (ext4_claim_free_clusters(sbi, md_needed, 0)) { 
for data. 
if (ret) 
if we fail to claim space. 
ford to run out of free blocks. 
if (ext4_claim_free_clusters(sbi, md_needed + 1, 0)) { 
if (!to_free) 
if (unlikely(to_free > ei->i_reserved_data_blocks)) { 
if there aren't enough reserved blocks, then the 
if (ei->i_reserved_data_blocks == 0) { 
if (next_off > stop) 
if ((offset <= curr_off) && (buffer_delay(bh))) { 
while ((bh = bh->b_this_page) != head); 
for that cluster. */ 
while (num_clusters > 0) { 
if (sbi->s_cluster_ratio == 1 || 
if (mpd->first_page >= mpd->next_page) 
if (invalidate) { 
while (index <= end) { 
if (nr_pages == 0) 
for (i = 0; i < nr_pages; i++) { 
if (page->index > end) 
if (invalidate) { 
ifdef ES_AGGRESSIVE_TEST 
if 
if (ext4_es_lookup_extent(inode, iblock, &es)) { 
if (ext4_es_is_hole(&es)) { 
if (ext4_es_is_delayed(&es) && !ext4_es_is_unwritten(&es)) { 
if (retval > map->m_len) 
if (ext4_es_is_written(&es)) 
if (ext4_es_is_unwritten(&es)) 
ifdef ES_AGGRESSIVE_TEST 
if 
if we can get the block without requesting a new 
if (ext4_has_inline_data(inode)) { 
for this page, and let 
if ((EXT4_SB(inode->i_sb)->s_cluster_ratio > 1) && 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if (retval == 0) { 
for every block we're going to write. 
if (!(map->m_flags & EXT4_MAP_FROM_CLUSTER)) { 
if (ret) { 
if (ret) { 
if (ret) { 
if (retval > 0) { 
if (unlikely(retval != map->m_len)) { 
for inode " 
if (ret != 0) 
for a single block. 
if (ret <= 0) 
if (buffer_unwritten(bh)) { 
for partial write. 
if (inline_data) { 
if (inode_bh == NULL) 
if (!page_bufs) { 
if (IS_ERR(handle)) { 
if (inline_data) { 
if (ret == 0) 
if (!ret) 
if (!ext4_has_inline_data(inode)) 
if 
ified via mmap(), no one guarantees in which 
if we do with blocksize 1K 
ify 
if we have any buffer_heads that is either delay or 
if (page->index == size >> PAGE_CACHE_SHIFT) 
if (ext4_walk_page_buffers(NULL, page_bufs, 0, len, NULL, 
for_writepage(wbc, page); 
if we came here 
if (PageChecked(page) && ext4_should_journal_data(inode)) 
if (!io_submit.io_end) { 
for_writepage(wbc, page); 
if (page->index == size >> PAGE_CACHE_SHIFT) 
for_io(page); 
if (!err) 
for writeback and we haven't started the 
if the block has been added to the extent, false if the block couldn't be 
for writeback? */ 
if (map->m_len == 0) 
if (map->m_len == 0) { 
if (map->m_len >= MAX_WRITEPAGES_EXTENT_LEN) 
if (lblk == map->m_lblk + map->m_len && 
for IO or add them to extent 
if all buffers in this page were mapped and there's no 
if the caller can continue 
if (lblk >= blocks || !mpage_add_bh_to_extent(mpd, lblk, bh)) { 
if (mpd->map.m_len) 
while (lblk++, (bh = bh->b_this_page) != head); 
if (mpd->map.m_len == 0) { 
if (err < 0) 
for IO 
form writes to unwritten extents 
for IO. 
while (start <= end) { 
if (nr_pages == 0) 
for (i = 0; i < nr_pages; i++) { 
if (page->index > end) 
if (lblk < mpd->map.m_lblk) 
if (lblk >= mpd->map.m_lblk + mpd->map.m_len) { 
for IO. 
if (err > 0) 
if (buffer_delay(bh)) { 
while (lblk++, (bh = bh->b_this_page) != head); 
if dioread_nolock 
if (err < 0) { 
if 
if the blocks 
ifferent parts of the allocation call path.  This flag exists 
if (dioread_nolock) 
if (map->m_flags & (1 << BH_Delay)) 
if (err < 0) 
if (dioread_nolock && (map->m_flags & EXT4_MAP_UNWRITTEN)) { 
if (map->m_flags & EXT4_MAP_NEW) { 
for (i = 0; i < map->m_len; i++) 
for IO 
iff there is a fatal error and there 
if it is unwritten, we may need to convert 
forward progress is 
if (err < 0) { 
if (EXT4_SB(sb)->s_mount_flags & EXT4_MF_FS_ABORTED) 
if ext4_count_free_blocks() 
if ((err == -ENOMEM) || 
for " 
if (err == -ENOSPC) 
if (err < 0) 
while (map->m_len); 
if (disksize > EXT4_I(inode)->i_disksize) { 
if (disksize > i_size) 
if (disksize > EXT4_I(inode)->i_disksize) 
if (err2) 
if (!err) 
for one writepages 
ifferent extents. 
for pages 
formed by 
if (mpd->wbc->sync_mode == WB_SYNC_ALL || mpd->wbc->tagged_writepages) 
while (index <= end) { 
if (nr_pages == 0) 
for (i = 0; i < nr_pages; i++) { 
if (page->index > end) 
if (mpd->wbc->sync_mode == WB_SYNC_NONE && left <= 0) 
if (mpd->map.m_len > 0 && mpd->next_page != page->index) 
if (!PageDirty(page) || 
if (mpd->map.m_len == 0) 
if (err <= 0) 
for special inodes like journal inode on last iput() 
if (!mapping->nrpages || !mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) 
if (ext4_should_journal_data(inode)) { 
if the filesystem is mounted 
if that ever happens, we would want 
if (unlikely(sbi->s_mount_flags & EXT4_MF_FS_ABORTED)) { 
if (ext4_should_dioread_nolock(inode)) { 
for the 1st page, so 
if (ext4_has_inline_data(inode)) { 
if (IS_ERR(handle)) { 
if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX) 
if (wbc->range_cyclic) { 
if (writeback_index) 
if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages) 
for_writeback(mapping, mpd.first_page, mpd.last_page); 
while (!done && mpd.first_page <= mpd.last_page) { 
if (!mpd.io_submit.io_end) { 
ifference when 
if (IS_ERR(handle)) { 
if (!ret) { 
if (ret == -ENOSPC && sbi->s_journal) { 
force_commit_nested(sbi->s_journal); 
if (ret) 
if (!ret && !cycled && wbc->nr_to_write > 0) { 
if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0)) 
if we are running low 
if (dirty_clusters && (free_clusters < 2 * dirty_clusters)) 
if (2 * free_clusters < 3 * dirty_clusters || 
if (ext4_nonda_switch(inode->i_sb)) { 
if (ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA)) { 
if (ret < 0) 
if (ret == 1) 
if the 
fore we start 
if needed) without using GFP_NOFS. 
if (!page) 
if there is delayed block allocation. But we still need 
if (IS_ERR(handle)) { 
if (page->mapping != mapping) { 
while the page was unlocked */ 
if (ret < 0) { 
if (pos + len > inode->i_size) 
if (ret == -ENOSPC && 
if we should update i_disksize 
for (i = 0; i < idx; i++) 
if (!buffer_mapped(bh) || (buffer_delay(bh)) || buffer_unwritten(bh)) 
if (write_mode == FALL_BACK_TO_NONDELALLOC) 
if i_size 
if (copied && new_i_size > EXT4_I(inode)->i_disksize) { 
if (new_i_size > EXT4_I(inode)->i_disksize) 
if 
if (write_mode != CONVERT_INLINE_DATA && 
if (ret2 < 0) 
if (!ret) 
if (!page_has_buffers(page)) 
for a given inode. 
if (!EXT4_I(inode)->i_reserved_data_blocks && 
for now.  The filemap_flush() will 
for users of 
for 
for_writepage() but that 
ifying them because we wouldn't actually intend to 
for the I/O to complete. 
ific piece of data. 
if we see any bmap calls here on a modified, data-journaled file, 
for an inline file via the FIBMAP ioctl 
if (ext4_has_inline_data(inode)) 
if (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY) && 
for file 
if (EXT4_JOURNAL(inode) && 
if we run lilo or swapon on a freshly made file 
if mortal users could trigger this path at 
if (err) 
if (ext4_has_inline_data(inode)) 
if (ret == -EAGAIN) 
if (ext4_has_inline_data(inode)) 
forget about the pending dirtying 
if (offset == 0 && length == PAGE_CACHE_SIZE) 
for aops... */ 
if (PageChecked(page)) 
if (journal) 
for a DIO write or buffer write. 
if not async direct IO just return */ 
for inode %lu, iocb 0x%p, offset %llu, size %zd\n", 
if the machine crashes during the write. 
for reads and writes beyond i_size. */ 
for direct IO properly wait also for extent 
if (rw == WRITE) 
if (overwrite) { 
fore DIO complete the data IO. 
for async 
if (!is_sync_kiocb(iocb)) { 
if (!io_end) { 
for DIO. Will be dropped in ext4_end_io_dio() 
for current async direct 
if (overwrite) { 
form extent 
if (io_end) { 
if (ret <= 0 && ret != -EIOCBQUEUED && iocb->private) { 
if (ret > 0 && !overwrite && ext4_test_inode_state(inode, 
for non AIO case, since the IO is already 
if (err < 0) 
if (rw == WRITE) 
if we do a ovewrite dio */ 
if (ext4_should_journal_data(inode)) 
if (ext4_has_inline_data(inode)) 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if (test_opt(inode->i_sb, DELALLOC)) 
ified range exceeds 
if (!page) 
if it does not fall between 
if (length > max || length < 0) 
if (!page_has_buffers(page)) 
while (offset >= pos) { 
if (buffer_freed(bh)) { 
if (!buffer_mapped(bh)) { 
if (!buffer_mapped(bh)) { 
if (PageUptodate(page)) 
if (!buffer_uptodate(bh)) { 
if (!buffer_uptodate(bh)) 
if (ext4_should_journal_data(inode)) { 
if (err) 
if (ext4_should_journal_data(inode)) { 
if (ext4_test_inode_state(inode, EXT4_STATE_ORDERED_MODE)) 
if the file is later grown. 
if (start == end && 
if (partial_start) { 
if (err) 
if (partial_end != sb->s_blocksize - 1) 
if (S_ISREG(inode->i_mode)) 
if (S_ISDIR(inode->i_mode)) 
if (S_ISLNK(inode->i_mode)) 
if (!S_ISREG(inode->i_mode)) 
if (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) { 
if (ret) 
if (offset >= inode->i_size) 
if (offset + length > inode->i_size) { 
if (offset & (sb->s_blocksize - 1) || 
if we do any zeroing of 
if (ret < 0) 
if (last_block_offset > first_block_offset) 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
for_truncate(inode); 
if (IS_ERR(handle)) { 
if (ret) 
if (first_block >= stop_block) 
if (ret) { 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if (IS_SYNC(inode)) 
if (last_block_offset > first_block_offset) 
if (ei->jinode || !EXT4_SB(inode->i_sb)->s_journal) 
if (!ei->jinode) { 
if (unlikely(jinode != NULL)) 
fore* the restart of 
if (!(inode->i_state & (I_NEW|I_FREEING))) 
if (!ext4_can_truncate(inode)) 
if (inode->i_size == 0 && !test_opt(inode->i_sb, NO_AUTO_DA_ALLOC)) 
if (ext4_has_inline_data(inode)) { 
if (has_inline) 
for jbd2 */ 
if (ext4_inode_attach_jinode(inode) < 0) 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
for_truncate(inode); 
if (IS_ERR(handle)) { 
if (inode->i_size & (inode->i_sb->s_blocksize - 1)) 
if this 
while each transaction commits. 
if (ext4_orphan_add(handle, inode)) 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if (IS_SYNC(inode)) 
if this was a real unlink then we were called by 
for us. 
if (inode->i_nlink) 
if (!ext4_valid_inum(sb, inode->i_ino)) 
if (!gdp) 
if (unlikely(!bh)) 
if (!buffer_uptodate(bh)) { 
if (buffer_write_io_error(bh) && !buffer_uptodate(bh)) 
if (buffer_uptodate(bh)) { 
while we waited */ 
formation of the inode in memory and this 
if (in_mem) { 
if (unlikely(!bitmap_bh)) 
forming two reads instead 
if (!buffer_uptodate(bitmap_bh)) { 
for (i = start; i < start + inodes_per_block; i++) { 
if (ext4_test_bit(i, bitmap_bh->b_data)) 
if (i == start + inodes_per_block) { 
if (EXT4_SB(sb)->s_inode_readahead_blks) { 
if (table > b) 
if (ext4_has_group_desc_csum(sb)) 
if (end > table) 
while (b <= end) 
if (!buffer_uptodate(bh)) { 
if (flags & EXT4_SYNC_FL) 
if (flags & EXT4_APPEND_FL) 
if (flags & EXT4_IMMUTABLE_FL) 
if (flags & EXT4_NOATIME_FL) 
if (flags & EXT4_DIRSYNC_FL) 
if (vfs_fl & S_SYNC) 
if (vfs_fl & S_APPEND) 
if (vfs_fl & S_IMMUTABLE) 
if (vfs_fl & S_NOATIME) 
if (vfs_fl & S_DIRSYNC) 
while (cmpxchg(&ei->i_flags, old_fl, new_fl) != old_fl); 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (ext4_test_inode_flag(inode, EXT4_INODE_HUGE_FILE)) { 
if (*magic == cpu_to_le32(EXT4_XATTR_MAGIC)) { 
if (!inode) 
if (!(inode->i_state & I_NEW)) 
if (ret < 0) 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) { 
if (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize > 
for inode metadata */ 
if (!ext4_inode_csum_verify(inode, raw_inode, ei)) { 
if (!(test_opt(inode->i_sb, NO_UID32))) { 
if the inode was active or not. 
if (inode->i_nlink == 0) { 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_64BIT)) 
ifdef CONFIG_QUOTA 
if 
for (block = 0; block < EXT4_N_BLOCKS; block++) 
if (journal) { 
if (journal->j_running_transaction) 
if (transaction) 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) { 
if (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) { 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) { 
if (ei->i_file_acl && 
if (!ext4_has_inline_data(inode)) { 
if ((S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) || 
if (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) || 
if (ret) 
if (S_ISREG(inode->i_mode)) { 
if (S_ISDIR(inode->i_mode)) { 
if (S_ISLNK(inode->i_mode)) { 
if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode) || 
if (raw_inode->i_block[0]) 
if (ino == EXT4_BOOT_LOADER_INO) { 
if (i_blocks <= ~0U) { 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_HUGE_FILE)) 
if (i_blocks <= 0xffffffffffffULL) { 
for new inodes. */ 
if (!(test_opt(inode->i_sb, NO_UID32))) { 
if (!ei->i_dtime) { 
if (ext4_inode_blocks_set(handle, raw_inode, ei)) { 
if (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) 
if (ei->i_disksize != ext4_isize(raw_inode)) { 
if (ei->i_disksize > 0x7fffffffULL) { 
if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode)) { 
if (!ext4_has_inline_data(inode)) { 
for (block = 0; block < EXT4_N_BLOCKS; block++) 
if (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) { 
if (ei->i_extra_isize) { 
if (!err) 
if (set_large_file) { 
if (err) 
for O_SYNC files. 
if told to. 
if told to. 
for us to return without doing anything, 
for WB_SYNC_ALL 
for them to not do this.  The code: 
while `stuff()' is running, 
if (WARN_ON_ONCE(current->flags & PF_MEMALLOC)) 
if (EXT4_SB(inode->i_sb)->s_journal) { 
force transaction in WB_SYNC_NONE mode. Also 
if (wbc->sync_mode != WB_SYNC_ALL || wbc->for_sync) 
force_commit(inode->i_sb); 
if (err) 
for each inode. 
if (wbc->sync_mode == WB_SYNC_ALL && !wbc->for_sync) 
if (buffer_req(iloc.bh) && !buffer_uptodate(iloc.bh)) { 
for commit to finish and try again. 
for_tail_page_commit(struct inode *inode) 
if (offset > PAGE_CACHE_SIZE - (1 << inode->i_blkbits)) 
while (1) { 
if (!page) 
if (ret != -EBUSY) 
if (journal->j_committing_transaction) 
if (commit_tid) 
ify_change. 
ify 
if we are in ordered mode 
for pages under 
if (error) 
if (is_quota_modification(inode, attr)) 
if ((ia_valid & ATTR_UID && !uid_eq(attr->ia_uid, inode->i_uid)) || 
if (IS_ERR(handle)) { 
if (error) { 
if (attr->ia_valid & ATTR_UID) 
if (attr->ia_valid & ATTR_GID) 
if (attr->ia_valid & ATTR_SIZE && attr->ia_size != inode->i_size) { 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) { 
if (attr->ia_size > sbi->s_bitmap_maxbytes) 
if (IS_I_VERSION(inode) && attr->ia_size != inode->i_size) 
if (S_ISREG(inode->i_mode) && 
if (ext4_should_order_data(inode)) { 
if (error) 
if (IS_ERR(handle)) { 
if (ext4_handle_valid(handle)) { 
if (!error) 
if (!error) 
if (error) { 
for dio in flight.  Temporarily disable 
if (orphan) { 
for_tail_page_commit(inode); 
for commit 
if attr->ia_size == 
for cases like truncation of fallocated space 
if (attr->ia_valid & ATTR_SIZE) 
if (!rc) { 
if (orphan && inode->i_nlink) 
if (!rc && (ia_valid & ATTR_MODE)) 
if (!error) 
for such files, so tools like tar, rsync, 
if (unlikely(ext4_has_inline_data(inode))) 
if the block allocation is delayed 
fore the real block 
for this file. 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) 
for index blocks, block groups bitmaps and block group 
ifferent block groups 
ifferent block groups too. If they are contiguous, with flexbg, 
for superblock, inode, quota and xattr blocks 
if (groups > ngroups) 
if (groups > EXT4_SB(inode->i_sb)->s_gdb_count) 
for super block, inode, quota and xattr blocks */ 
ification of a single pages into a single transaction, 
for data blocks for journalled mode */ 
ification. 
for data blocks are not included here, as DIO 
if (IS_I_VERSION(inode)) 
if (!err) { 
if (err) { 
if (EXT4_I(inode)->i_extra_isize >= new_extra_isize) 
if (!ext4_test_inode_state(inode, EXT4_STATE_XATTR) || 
form any I/O.  This is a very good thing, 
ifferent journal. 
if (ext4_handle_valid(handle) && 
for that inode. 
force a large enough s_min_extra_isize. 
if ((jbd2_journal_extend(handle, 
if (ret) { 
if (mnt_count != 
if (!err) 
if (IS_ERR(handle)) 
if 0 
if (handle) { 
if (!err) { 
if (!err) 
if 
forgetting to revoke the old log record 
if (!journal) 
if (is_journal_aborted(journal)) 
for delalloc blocks 
if (val && test_opt(inode->i_sb, DELALLOC)) { 
if (err < 0) 
for all existing dio workers */ 
ify 
if (val) 
if (IS_ERR(handle)) 
if (test_opt(inode->i_sb, DELALLOC) && 
while (ret == -ENOSPC && 
if (page->mapping != mapping || page_offset(page) > size) { 
if (page->index == size >> PAGE_CACHE_SHIFT) 
if we have all the buffers mapped. This avoids the need to do 
if (page_has_buffers(page)) { 
for_stable_page(page); 
if (ext4_should_dioread_nolock(inode)) 
if (IS_ERR(handle)) { 
if (!ret && ext4_should_journal_data(inode)) { 
if (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries)) 
file : ./test/kernel/fs/ext4/namei.c 
[ OK ] open : 4 ok... 
forward compatibility hooks 
while searching them. 
if (unlikely(EXT4_SB(inode->i_sb)->s_max_dir_size_kb && 
if (!bh) 
if (err) { 
ify(struct inode *inode, 
if (!bh) { 
if (is_dx(inode)) { 
if (ext4_rec_len_from_disk(dirent->rec_len, 
if (!is_dx_block && type == INDEX) { 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
ified(bh)) 
for a index block; for 
if (is_dx_block && type == INDEX) { 
ified(bh); 
if (!is_dx_block) { 
ified(bh); 
ifndef assert 
if 
if 
if it should somehow get overlaid by a 
fore, the 
ifdef PARANOID 
while (d < top && d->rec_len) 
if (d != top) 
if 
for_csum(struct inode *inode) 
ify(struct inode *inode, struct ext4_dir_entry *dirent) 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!t) { 
for_csum(inode); 
if (t->det_checksum != ext4_dirent_csum(inode, dirent, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!t) { 
for_csum(inode); 
if (le16_to_cpu(dirent->rec_len) == EXT4_BLOCK_SIZE(inode->i_sb)) 
if (le16_to_cpu(dirent->rec_len) == 12) { 
if (le16_to_cpu(dp->rec_len) != 
if (root->reserved_zero || 
if (offset) 
ify(struct inode *inode, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!c) { 
if (count_offset + (limit * sizeof(struct dx_entry)) > 
for_csum(inode); 
if (t->dt_checksum != ext4_dx_csum(inode, dirent, count_offset, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!c) { 
if (count_offset + (limit * sizeof(struct dx_entry)) > 
for_csum(inode); 
fore the end of page 
for coalesce-on-delete flags 
if (EXT4_HAS_RO_COMPAT_FEATURE(dir->i_sb, 
if (EXT4_HAS_RO_COMPAT_FEATURE(dir->i_sb, 
ifdef DX_DEBUG 
for (i = 0; i < n; i++) { 
while ((char *) de < base + size) 
if (show_names) 
while (len--) printk("%c", *name++); 
for (i = 0; i < count; i++, entries++) 
if (!(bh = ext4_bread (NULL,dir, block, 0,&err))) continue; 
if (bcount) 
if /* DX_DEBUG */ 
for a directory leaf block to search. 
for this error code, and make sure it never gets reflected 
if (IS_ERR(bh)) { 
if (root->info.hash_version != DX_HASH_TEA && 
if (hinfo->hash_version <= DX_HASH_TEA) 
if (d_name) 
if (root->info.unused_flags & 1) { 
if ((indirect = root->info.indirect_levels) > 1) { 
if (dx_get_limit(entries) != dx_root_limit(dir, 
while (1) 
if (!count || count > dx_get_limit(entries)) { 
while (p <= q) 
if (dx_get_hash(m) > hash) 
if (0) // linear search cross check 
while (n--) 
if (dx_get_hash(++at) > hash) 
if (!indirect--) return frame; 
if (IS_ERR(bh)) { 
if (dx_get_limit(entries) != dx_node_limit (dir)) { 
while (frame >= frame_in) { 
if (*err == ERR_BAD_DX_DIR) 
if (frames[0].bh == NULL) 
if (((struct dx_root *) frames[0].bh->b_data)->info.indirect_levels) 
if the search 
if the next block starts with that 
for a specific file. 
if the caller should continue to search, 
while (1) { 
if (p == frames) 
if the next page has a 
for readdir 
if (start_hash) 
if ((hash & 1) == 0) { 
while (num_frames--) { 
if (IS_ERR(bh)) 
formation from a 
if (IS_ERR(bh)) 
for (; de < top; de = ext4_next_entry(de, dir->i_sb->s_blocksize)) { 
if ((hinfo->hash < start_hash) || 
if (de->inode == 0) 
if ((err = ext4_htree_store_dirent(dir_file, 
formation from a 
if (!(ext4_test_inode_flag(dir, EXT4_INODE_INDEX))) { 
if (hinfo.hash_version <= DX_HASH_TEA) 
if (ext4_has_inline_data(dir)) { 
if (has_inline_data) { 
if (!frame) 
if (!start_hash && !start_minor_hash) { 
if ((err = ext4_htree_store_dirent(dir_file, 0, 0, de)) != 0) 
if (start_hash < 2 || (start_hash ==2 && start_minor_hash==0)) { 
if ((err = ext4_htree_store_dirent(dir_file, 2, 0, de)) != 0) 
while (1) { 
if (ret < 0) { 
if (ret < 0) { 
if:  (a) there are no more entries, or 
if ((ret == 0) || 
while ((char *) de < base + blocksize) { 
while (count > 2) { 
if (count - 9 < 2) /* 9, 10 -> 11 */ 
for (p = top, q = p - count; q >= map; p--, q--) 
while (q-- > map) { 
while(more); 
for success, 0 for failure. 
if (len != de->name_len) 
if (!de->inode) 
if not found, -1 on failure, and 1 on success 
while ((char *) de < dlimit) { 
if ((char *) de + namelen <= dlimit && 
if (ext4_check_dir_entry(dir, NULL, de, bh, bh->b_data, 
if (de_len <= 0) 
if (!is_dx(dir)) 
if (block == 0) 
if (de->inode == 0 && 
ified directory with the wanted name. It 
if you want to. 
if (namelen > EXT4_NAME_LEN) 
if (ext4_has_inline_data(dir)) { 
if (has_inline_data) { 
if ((namelen <= 2) && (name[0] == '.') && 
if (is_dx(dir)) { 
if the error was file not found, 
if (bh || (err != ERR_BAD_DX_DIR)) 
if (start >= nblocks) 
if (ra_ptr >= ra_max) { 
for (ra_max = 0; ra_max < NAMEI_RA_SIZE; ra_max++) { 
if we reach the end of the 
if (b >= nblocks || (num && block == start)) { 
if (bh) 
if ((bh = bh_use[ra_ptr++]) == NULL) 
if (!buffer_uptodate(bh)) { 
for the best */ 
if (!buffer_verified(bh) && 
ify(dir, 
ified(bh); 
if (i == 1) { 
if (i < 0) 
if (++block >= nblocks) 
while (block != start); 
while we were searching, then 
if (block < nblocks) { 
for (; ra_ptr < ra_max; ra_ptr++) 
if (!(frame = dx_probe(d_name, dir, &hinfo, frames, err))) 
if (IS_ERR(bh)) { 
if (retval == 1) { 	/* Success! */ 
if (retval == -1) { 
if we should continue to search */ 
if (retval < 0) { 
while (retval == 1); 
if (dentry->d_name.len > EXT4_NAME_LEN) 
if (bh) { 
if (!ext4_valid_inum(dir->i_sb, ino)) { 
if (unlikely(ino == dir->i_ino)) { 
if (inode == ERR_PTR(-ESTALE)) { 
if (!bh) 
if (!ext4_valid_inum(child->d_inode->i_sb, ino)) { 
while (count--) { 
while ((char*)de < base + blocksize) { 
if (de->inode && de->name_len) { 
if (de > to) 
for a new dir entry. 
if (EXT4_HAS_RO_COMPAT_FEATURE(dir->i_sb, 
if (IS_ERR(bh2)) { 
if (err) 
if (err) 
for (i = count-1; i >= 0; i--) { 
if (size + map[i].size/2 > blocksize/2) 
if (csum_size) { 
if (hinfo->hash >= hash2) 
if (err) 
if (err) 
while ((char *) de <= top) { 
if (ext4_match(namelen, name, de)) 
if ((de->inode ? rlen - nlen : rlen) >= reclen) 
if ((char *) de > top) 
if (de->inode) { 
for new directory entry.  If de is NULL, then 
if no space is available, and -EIO 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!de) { 
if (err) 
if (err) { 
for journaling */ 
ifferent from the directory change time. 
if (err) 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (retval) { 
if ((char *) de >= (((char *) root) + blocksize)) { 
for '..'"); 
for the 0th block's dirents */ 
if (IS_ERR(bh2)) { 
while ((char *)(de2 = ext4_next_entry(de, blocksize)) < top) 
if (csum_size) { 
for dx_probe */ 
if (hinfo.hash_version <= DX_HASH_TEA) 
if (!de) { 
if the block split failed, we have to properly write 
ified directory, using the same 
while you slept. 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!dentry->d_name.len) 
if (ext4_has_inline_data(dir)) { 
if (retval < 0) 
if (retval == 1) { 
if (is_dx(dir)) { 
if (!retval || (retval != ERR_BAD_DX_DIR)) 
for (block = 0; block < blocks; block++) { 
if (IS_ERR(bh)) 
if (retval != -ENOSPC) { 
if (blocks == 1 && !dx_fallback && 
if (IS_ERR(bh)) 
if (csum_size) { 
if (retval == 0) 
for success, or a negative error value 
if (!frame) 
if (IS_ERR(bh)) { 
if (err) 
if (err != -ENOSPC) 
for now just split */ 
if (dx_get_count(entries) == dx_get_limit(entries)) { 
if (levels && (dx_get_count(frames->entries) == 
if (IS_ERR(bh2)) { 
if (err) 
if (levels) { 
if (err) 
if (at - entries >= icount1) { 
if (err) 
if (err) 
if (err) { 
if (!de) 
while (i < buf_size - csum_size) { 
if (de == de_del)  { 
if (ext4_has_inline_data(dir)) { 
if (has_inline_data) 
if (EXT4_HAS_RO_COMPAT_FEATURE(dir->i_sb, 
if (unlikely(err)) 
if (err) 
if (unlikely(err)) 
if (err != -ENOENT) 
if 1) nlinks > EXT4_LINK_MAX or 2) nlinks == 2, 
if (is_dx(inode) && inode->i_nlink > 1) { 
if (inode->i_nlink >= EXT4_LINK_MAX || inode->i_nlink == 2) { 
if (!S_ISDIR(inode->i_mode) || inode->i_nlink > 2) 
if (!err) { 
for the new file, but it 
formation 
if (!IS_ERR(inode)) { 
if (!err && IS_DIRSYNC(dir)) 
if (handle) 
if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries)) 
if (!new_valid_dev(rdev)) 
if (!IS_ERR(inode)) { 
if (!err && IS_DIRSYNC(dir)) 
if (handle) 
if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries)) 
if (!IS_ERR(inode)) { 
if (err) 
if (handle) 
if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries)) 
if (!dotdot_real_len) 
if (EXT4_HAS_RO_COMPAT_FEATURE(dir->i_sb, 
if (ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA)) { 
if (err < 0 && err != -ENOSPC) 
if (!err) 
if (IS_ERR(dir_block)) 
if (err) 
if (csum_size) { 
if (err) 
ified(dir_block); 
if (EXT4_DIR_LINK_MAX(dir)) 
if (IS_ERR(inode)) 
if (err) 
if (!err) 
if (err) { 
if (err) 
if (IS_DIRSYNC(dir)) 
if (handle) 
if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries)) 
ified directory is empty (for rmdir) 
if (ext4_has_inline_data(inode)) { 
if (has_inline_data) 
if (inode->i_size < EXT4_DIR_REC_LEN(1) + EXT4_DIR_REC_LEN(2)) { 
if (IS_ERR(bh)) 
if (le32_to_cpu(de->inode) != inode->i_ino || 
while (offset < inode->i_size) { 
if (IS_ERR(bh)) 
if (ext4_check_dir_entry(inode, NULL, de, bh, 
if (le32_to_cpu(de->inode)) { 
fore the 
if (!sbi->s_journal) 
if inode already is on orphan list. This is a big speedup 
if (!list_empty(&EXT4_I(inode)->i_orphan)) 
for files with data blocks 
if (err) 
if (err) 
ification. 
if (!NEXT_ORPHAN(inode) || NEXT_ORPHAN(inode) > 
if (dirty) { 
if (!err) 
if (err) { 
if 
if (!sbi->s_journal && !(sbi->s_mount_state & EXT4_ORPHAN_FS)) 
fore taking global s_orphan_lock. */ 
if (handle) { 
fore taking global s_orphan_lock */ 
if (!handle || err) { 
if (prev == &sbi->s_orphan) { 
if (err) { 
if (err) { 
if (err) 
fore so that eventual writes go in 
if (!bh) 
if (le32_to_cpu(de->inode) != inode->i_ino) 
if (!empty_dir(inode)) 
if (IS_ERR(handle)) { 
if (IS_DIRSYNC(dir)) 
if (retval) 
if (!EXT4_DIR_LINK_EMPTY(inode)) 
if (handle) 
fore so that eventual writes go 
if (!bh) 
if (le32_to_cpu(de->inode) != inode->i_ino) 
if (IS_ERR(handle)) { 
if (IS_DIRSYNC(dir)) 
if (!inode->i_nlink) { 
if (retval) 
if (!inode->i_nlink) 
if (handle) 
if (l > dir->i_sb->s_blocksize) 
if (l > EXT4_N_BLOCKS * 4) { 
if (IS_ERR(inode)) 
if (l > EXT4_N_BLOCKS * 4) { 
if we are running out of space 
if (err) 
if (err) 
ified 
if (IS_ERR(handle)) { 
if (err) { 
format for fast symlink */ 
if (!err && IS_DIRSYNC(dir)) 
if (handle) 
if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries)) 
if (inode->i_nlink >= EXT4_LINK_MAX) 
if (IS_ERR(handle)) 
if (IS_DIRSYNC(dir)) 
if (!err) { 
for tmpfile being 
if (inode->i_nlink == 1) 
if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries)) 
if it is inlined or the 1st block 
if (!ext4_has_inline_data(inode)) { 
if (IS_ERR(bh)) { 
for "dentry" */ 
if it's a directory */ 
if (!ent->dir_bh) 
if (le32_to_cpu(ent->parent_de->inode) != ent->dir->i_ino) 
if (!ent->dir_inlined) { 
if (retval) { 
if (retval) 
if (EXT4_HAS_INCOMPAT_FEATURE(ent->dir->i_sb, 
if (!ent->inlined) { 
if (unlikely(retval)) { 
if (bh) { 
if (le32_to_cpu(ent->de->inode) != ent->inode->i_ino || 
if (retval == -ENOENT) { 
if (retval) { 
if (ent->dir_nlink_delta) { 
while new_{dentry,inode) refers to the destination dentry/inode 
fore so that eventual writes go 
if (new.inode) 
for inode number is _not_ due to possible IO errors. 
if (!old.bh || le32_to_cpu(old.de->inode) != old.inode->i_ino) 
if (new.bh) { 
if (new.inode && !test_opt(new.dir->i_sb, NO_AUTO_DA_ALLOC)) 
if (IS_ERR(handle)) 
if (IS_DIRSYNC(old.dir) || IS_DIRSYNC(new.dir)) 
if (S_ISDIR(old.inode->i_mode)) { 
if (!empty_dir(new.inode)) 
if (new.dir != old.dir && EXT4_DIR_LINK_MAX(new.dir)) 
if (retval) 
if (!new.bh) { 
if (retval) 
if (retval) 
for inodes on a 
if (new.inode) { 
if (old.dir_bh) { 
if (retval) 
if (new.inode) { 
for many-linked dirs */ 
if (new.inode) { 
if (!new.inode->i_nlink) 
if (handle) 
for inode number is _not_ due to possible IO errors. 
if (!old.bh || le32_to_cpu(old.de->inode) != old.inode->i_ino) 
if (!new.bh || le32_to_cpu(new.de->inode) != new.inode->i_ino) 
if (IS_ERR(handle)) 
if (IS_DIRSYNC(old.dir) || IS_DIRSYNC(new.dir)) 
if (S_ISDIR(old.inode->i_mode)) { 
if (retval) 
if (S_ISDIR(new.inode->i_mode)) { 
if (retval) 
ified if this is a cross directory rename. 
if (old.dir != new.dir && old.is_dir != new.is_dir) { 
if ((old.dir_nlink_delta > 0 && EXT4_DIR_LINK_MAX(old.dir)) || 
if (retval) 
if (retval) 
for inodes on a 
if (old.dir_bh) { 
if (retval) 
if (new.dir_bh) { 
if (retval) 
if (handle) 
if (flags & ~(RENAME_NOREPLACE | RENAME_EXCHANGE)) 
if (flags & RENAME_EXCHANGE) { 
file : ./test/kernel/fs/ext4/balloc.c 
[ OK ] open : 4 ok... 
for a given block number 
if (test_opt2(sb, STD_GROUP_SIZE)) 
if (offsetp) 
if (blockgrpp) 
if so 
for file system metadata; this 
if the block is in the block group.  If it 
for in the clusters used for the base metadata cluster, or 
for the allocation bitmap or inode table explicitly. 
for *very* 
if (ext4_block_in_group(sb, ext4_block_bitmap(sb, gdp), block_group)) { 
if (block_cluster < num_clusters) 
if (block_cluster == num_clusters) { 
if (ext4_block_in_group(sb, ext4_inode_bitmap(sb, gdp), block_group)) { 
if (inode_cluster < num_clusters) 
if (inode_cluster == num_clusters) { 
for (i = 0; i < sbi->s_itb_per_group; i++) { 
if ((c < num_clusters) || (c == inode_cluster) || 
if (c == num_clusters) { 
if (block_cluster != -1) 
if (inode_cluster != -1) 
if (block_group == ext4_get_groups_count(sb) - 1) { 
if (!ext4_group_desc_csum_verify(sb, block_group, gdp)) { 
for group %u", block_group); 
if (!EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
if (!EXT4_MB_GRP_IBITMAP_CORRUPT(grp)) { 
for (bit = 0; bit < bit_max; bit++) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG)) 
for block and inode bitmaps, and inode table */ 
if (!flex_bg || ext4_block_in_group(sb, tmp, block_group)) 
if (!flex_bg || ext4_block_in_group(sb, tmp, block_group)) 
for (; tmp < ext4_inode_table(sb, gdp) + 
if (!flex_bg || ext4_block_in_group(sb, tmp, block_group)) 
if the number of blocks within the group is less than 
for blocks, 1 bitmap 
if (block_group >= ngroups) { 
if (!sbi->s_group_desc[group_desc]) { 
if (bh) 
if 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG)) { 
for those groups 
ify they are set. 
if (!ext4_test_bit(EXT4_B2C(sbi, offset), bh->b_data)) 
if (!ext4_test_bit(EXT4_B2C(sbi, offset), bh->b_data)) 
if (next_zero_bit < 
for inode tables */ 
if (buffer_verified(bh)) 
if (unlikely(blk != 0)) { 
if (!EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
if (unlikely(!ext4_block_bitmap_csum_verify(sb, block_group, 
if (!EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
ified(bh); 
for a given block_group,and validate the 
if (!desc) 
if (unlikely(!bh)) { 
for block bitmap - " 
if (bitmap_uptodate(bh)) 
if (bitmap_uptodate(bh)) { 
ify; 
if (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) { 
if (buffer_uptodate(bh)) { 
if not uninit if bh is uptodate, 
ify; 
for reading 
ify: 
if (buffer_verified(bh)) 
if (!buffer_new(bh)) 
if (!desc) 
if (!buffer_uptodate(bh)) { 
if block bitmap is invalid */ 
for error just in case errors=continue. */ 
if (!bh) 
if (ext4_wait_block_bitmap(sb, block_group, bh)) { 
if filesystem has nclusters free & available for allocation. 
ift only. 
if (free_clusters - (nclusters + rsv + dirty_clusters) < 
for current 
if (free_clusters >= (rsv + nclusters + dirty_clusters)) 
if (uid_eq(sbi->s_resuid, current_fsuid()) || 
if (free_clusters >= (nclusters + dirty_clusters + 
if we can dip into reserved pool */ 
if (free_clusters >= (nclusters + dirty_clusters)) 
if (ext4_has_free_clusters(sbi, nclusters, flags)) { 
if 
for the current or committing transaction to complete, and then 
if the total number of retries exceed three times, return FALSE. 
if (!ext4_has_free_clusters(EXT4_SB(sb), 1, 0) || 
force_commit_nested(EXT4_SB(sb)->s_journal); 
if (count) 
for the allocated meta blocks.  We will never 
if (!(*errp) && 
ifdef EXT4FS_DEBUG 
for (i = 0; i < ngroups; i++) { 
if (!gdp) 
if (EXT4_SB(sb)->s_group_info) 
if (!grp || !EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
if (bitmap_bh == NULL) 
for (i = 0; i < ngroups; i++) { 
if (!gdp) 
if (EXT4_SB(sb)->s_group_info) 
if (!grp || !EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
if 
while (1) { 
if (a == b) 
if ((a % b) != 0) 
for filesystem 
if (group == 0) 
if (EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_SPARSE_SUPER2)) { 
if ((group <= 1) || !EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (!(group & 1)) 
if (test_root(group, 3) || (test_root(group, 5)) || 
if (group == first || group == first + 1 || group == last) 
if (!ext4_bg_has_super(sb, group)) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb,EXT4_FEATURE_INCOMPAT_META_BG)) 
for filesystem 
ifferent number of descriptor blocks in each group. 
if (!EXT4_HAS_INCOMPAT_FEATURE(sb,EXT4_FEATURE_INCOMPAT_META_BG) || 
for superblock and gdt backups in this group */ 
if (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_META_BG) || 
if (num) { 
for block allocation 
for a 
if (flex_size >= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) { 
for directories and special files.  Regular 
if (S_ISREG(inode->i_mode)) 
if (test_opt(inode->i_sb, DELALLOC)) 
if (bg_start + EXT4_BLOCKS_PER_GROUP(inode->i_sb) <= last_block) 
file : ./test/kernel/fs/ext4/mballoc.c 
[ OK ] open : 4 ok... 
ify 
for more details. 
if not, write to the Free Software 
ifdef CONFIG_EXT4_DEBUG 
for ext4's mballoc"); 
for metadata in few groups 
if no free space left (policy?) 
for superuser 
for better group selection 
for multiple number of blocks 
for having small file use group preallocation is to 
for this particular inode. The inode prealloc space is 
for this prealloc space 
for this prealloc space (in clusters) 
ify the values associated to inode prealloc space except 
if we 
for having a per cpu locality group is to reduce the contention 
ifferent 
formation is attached to buddy cache inode so that 
formation involve 
for bitmap and buddy information.  So for each group we 
formation regarding groups_per_page 
for count number of blocks in the buddy cache. If we were able 
fore allocating blocks via buddy cache we normalize the request 
for non-bigalloc file systems, it is 
if the request len is power of 
for contiguous block in 
ific group using bitmap for best extents. The 
for a best 
for the blocks starts with 
for allocation. ext4_mb_good_group explains how the groups are 
for the first 
for the 
ific inode and can be used for this inode only. 
fore taking some block from descriptor, one must 
ific locality group which does not translate to 
for any inode. thus 
if we follow this strict logic, then all operations above should be atomic. 
formance on high-end SMP hardware. let's try to relax it using 
if buddy is referenced, it's already initialized 
while block is used in buddy and the buddy is referenced, 
if on-disk has 
if buddy has same bit set or/and PA covers corresponded 
for PA are allocated in the buddy, buddy must be referenced 
ifferent PAs covering different blocks 
while it is no discard is possible 
fore 
ify buddy 
for given object (inode, locality group): 
for given group: 
for groupinfo data structures based on the 
if BITS_PER_LONG == 64 
if BITS_PER_LONG == 32 
if 
if (ret > max) 
if (ret > max) 
if (order > e4b->bd_blkbits + 1) { 
if (order == 0) { 
ifdef DOUBLE_CHECK 
if (unlikely(e4b->bd_info->bb_bitmap == NULL)) 
for (i = 0; i < count; i++) { 
if (unlikely(e4b->bd_info->bb_bitmap == NULL)) 
for (i = 0; i < count; i++) { 
if (memcmp(e4b->bd_info->bb_bitmap, bitmap, e4b->bd_sb->s_blocksize)) { 
for (i = 0; i < e4b->bd_sb->s_blocksize; i++) { 
if 
if (!(assert)) {						\ 
while (0) 
if (mb_check_counter++ % 100 != 0) 
while (order > 1) { 
for (i = 0; i < max; i++) { 
if (!mb_test_bit(i << 1, buddy2)) { 
if (!mb_test_bit((i << 1) + 1, buddy2)) { 
for (j = 0; j < (1 << order); j++) { 
for (i = 0; i < max; i++) { 
if (fstart == -1) { 
for (j = 0; j < e4b->bd_blkbits + 1; j++) { 
for_each(cur, &grp->bb_prealloc_list) { 
for (i = 0; i < pa->pa_len; i++) 
if 
for corresponded chunk size. 
while (len > 0) { 
if (max < min) 
if (min > 0) 
for (i = bits; i >= 0; i--) { 
for_stack 
while (i < max) { 
if (len > 1) 
if (i < max) 
if (free != grp->bb_free) { 
if (!EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
while ((buddy = mb_find_buddy(e4b, order++, &count))) { 
formation is attached the buddy cache inode 
formation involve 
for bitmap and buddy information. 
formation regarding groups_per_page which 
for this page; do not hold this lock when calling this routine! 
if (groups_per_page == 0) 
if (groups_per_page > 1) { 
if (bh == NULL) { 
for (i = 0, group = first_group; i < groups_per_page; i++, group++) { 
if (PageUptodate(page) && !EXT4_MB_GRP_NEED_INIT(grinfo)) { 
if (!(bh[i] = ext4_read_block_bitmap_nowait(sb, group))) { 
for group %u\n", group); 
for (i = 0, group = first_group; i < groups_per_page; i++, group++) { 
for (i = 0; i < blocks_per_page; i++) { 
if (group >= ngroups) 
if (!bh[group - first_group]) 
formation regarding this 
if ((first_block + i) & 1) { 
for group %u in page %lu/%x\n", 
for group %u in page %lu/%x\n", 
formation can be 
if (bh) { 
for (i = 0; i < groups_per_page; i++) 
if (bh != &bhs) 
formation in consecutive blocks. 
if (!page) 
if (blocks_per_page >= 2) { 
if (!page) 
if (e4b->bd_bitmap_page) { 
if (e4b->bd_buddy_page) { 
for this page; do not hold the BG lock when 
for_stack 
if (ret || !EXT4_MB_GRP_NEED_INIT(this_grp)) { 
if (ret) 
if (!PageUptodate(page)) { 
if (e4b.bd_buddy_page == NULL) { 
force 
if (ret) 
if (!PageUptodate(page)) { 
for this page; do not hold the BG lock when 
for_stack int 
if (unlikely(EXT4_MB_GRP_NEED_INIT(grp))) { 
if (ret) 
formation in consecutive blocks. 
if (page == NULL || !PageUptodate(page)) { 
for it to initialize. 
if (page) { 
if (!PageUptodate(page)) { 
if (ret) { 
if (page == NULL) { 
if (!PageUptodate(page)) { 
if (page == NULL || !PageUptodate(page)) { 
if (page) { 
if (!PageUptodate(page)) { 
if (ret) { 
if (page == NULL) { 
if (!PageUptodate(page)) { 
if (page) 
if (e4b->bd_bitmap_page) 
if (e4b->bd_buddy_page) 
if (e4b->bd_bitmap_page) 
if (e4b->bd_buddy_page) 
for_block(struct ext4_buddy *e4b, int block) 
while (order <= e4b->bd_blkbits + 1) { 
if (!mb_test_bit(block, bb)) { 
while (cur < len) { 
if any, -1 otherwise 
while (cur < len) { 
if (*addr != (__u32)(-1) && zero_bit == -1) 
if (!mb_test_and_clear_bit(cur, bm) && zero_bit == -1) 
while (cur < len) { 
if (mb_test_bit(*bit + side, bitmap)) { 
while (buddy) { 
form buddies on 
ift range to [0; 2], go up and do the same. 
if (first & 1) 
if (!(last & 1)) 
if (first > last) 
if (first == last || !(buddy2 = mb_find_buddy(e4b, order, &max))) { 
if the block group is corrupt. */ 
if (first < e4b->bd_info->bb_first_free) 
if (first != 0) 
if (last + 1 < EXT4_SB(sb)->s_mb_maxs[0]) 
if (unlikely(block != -1)) { 
if (!EXT4_MB_GRP_BBITMAP_CORRUPT(e4b->bd_info)) 
if (left_is_free && right_is_free) 
if (!left_is_free && !right_is_free) 
if neighbours are to be coaleasced, 
if (first & 1) { 
if (!(last & 1)) { 
if (first <= last) 
if (mb_test_bit(block, buddy)) { 
for_block(e4b, block); 
ifference from given start */ 
while (needed > ex->fe_len && 
if (block + 1 >= max) 
if (mb_test_bit(next, e4b->bd_bitmap)) 
for_block(e4b, next); 
if (e4b->bd_info->bb_first_free == start) 
if (start != 0) 
if (start + len < EXT4_SB(e4b->bd_sb)->s_mb_maxs[0]) 
if (mlen && max) 
if (!mlen && !max) 
while (len) { 
if (((start >> ord) << ord) == start && len >= (1 << ord)) { 
for history */ 
for history */ 
for this 
for subsequent stream allocation */ 
for general purposes allocation 
if (ac->ac_status == AC_STATUS_FOUND) 
for a whole year 
if (ac->ac_found > sbi->s_mb_max_to_scan && 
if (bex->fe_len < gex->fe_len) 
if ((finish_group || ac->ac_found > sbi->s_mb_min_to_scan) 
if (max >= gex->fe_len) { 
if new one is better, then it's stored 
if (unlikely(ac->ac_flags & EXT4_MB_HINT_FIRST)) { 
if (ex->fe_len == gex->fe_len) { 
if (bex->fe_len == 0) { 
if (bex->fe_len < gex->fe_len) { 
if (ex->fe_len > bex->fe_len) 
if (ex->fe_len > gex->fe_len) { 
if (ex->fe_len < bex->fe_len) 
for_stack 
if (err) 
if (max > 0) { 
for_stack 
if (!(ac->ac_flags & EXT4_MB_HINT_TRY_GOAL)) 
if (grp->bb_free == 0) 
if (err) 
if (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(e4b->bd_info))) { 
if (max >= ac->ac_g_ex.fe_len && ac->ac_g_ex.fe_len == sbi->s_stripe) { 
if (do_div(start, sbi->s_stripe) == 0) { 
if (max >= ac->ac_g_ex.fe_len) { 
if (max > 0 && (ac->ac_flags & EXT4_MB_HINT_MERGE)) { 
for_stack 
for (i = ac->ac_2order; i <= sb->s_blocksize_bits + 1; i++) { 
if (EXT4_SB(sb)->s_mb_stats) 
for_stack 
while (free && ac->ac_status == AC_STATUS_CONTINUE) { 
if (i >= EXT4_CLUSTERS_PER_GROUP(sb)) { 
if (free < ex.fe_len) { 
iffers. This mostly 
for storages like raid5 
for_stack 
while (i < EXT4_CLUSTERS_PER_GROUP(sb)) { 
if (max >= sbi->s_stripe) { 
if (free == 0) 
if (cr <= 2 && free < ac->ac_g_ex.fe_len) 
if (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(grp))) 
if the grp has never been initialized */ 
if (ret) 
if (fragments == 0) 
for data files */ 
if ((ac->ac_2order > ac->ac_sb->s_blocksize_bits+1) || 
if (grp->bb_largest_free_order < ac->ac_2order) 
if ((free / fragments) >= ac->ac_g_ex.fe_len) 
if (free >= ac->ac_g_ex.fe_len) 
for_stack int 
if (!(ext4_test_inode_flag(ac->ac_inode, EXT4_INODE_EXTENTS))) 
if (err || ac->ac_status == AC_STATUS_FOUND) 
if (unlikely(ac->ac_flags & EXT4_MB_HINT_GOAL_ONLY)) 
if the fe_len is a power of 2 
if the order of the request 
if (i >= sbi->s_mb_order2_reqs) { 
if fe_len is exactly power of 2 
if ((ac->ac_g_ex.fe_len & (~(1 << (i - 1)))) == 0) 
if stream allocation is enabled, use global goal */ 
for (; cr < 4 && ac->ac_status == AC_STATUS_CONTINUE; cr++) { 
for the right group start 
for (i = 0; i < ngroups; group++, i++) { 
ificially restricted ngroups for non-extent 
if (group >= ngroups) 
if (!ext4_mb_good_group(ac, group, cr)) 
if (err) 
if (!ext4_mb_good_group(ac, group, cr)) { 
if (cr == 0 && ac->ac_2order < sb->s_blocksize_bits+2) 
if (cr == 1 && sbi->s_stripe && 
if (ac->ac_status != AC_STATUS_CONTINUE) 
if (ac->ac_b_ex.fe_len > 0 && ac->ac_status != AC_STATUS_FOUND && 
if (ac->ac_status != AC_STATUS_FOUND) { 
if (*pos < 0 || *pos >= ext4_get_groups_count(sb)) 
if (*pos < 0 || *pos >= ext4_get_groups_count(sb)) 
if (group == 0) 
if not already loaded. */ 
if (err) { 
if (buddy_loaded) 
for (i = 0; i <= 13; i++) 
if (rc == 0) { 
ified number 
if (size <= sbi->s_group_info_size) 
if (!new_groupinfo) { 
if (sbi->s_group_info) { 
for %d meta_bg's\n",  
for the given group. */ 
if this group is the first of a reserved block. 
if (group % EXT4_DESC_PER_BLOCK(sb) == 0) { 
if (meta_group_info == NULL) { 
for a buddy group"); 
if (meta_group_info[i] == NULL) { 
if (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) { 
ifdef DOUBLE_CHECK 
if 
if (group % EXT4_DESC_PER_BLOCK(sb) == 0) { 
if (err) 
if (sbi->s_buddy_cache == NULL) { 
for the buddy cache inode number.  This inode is 
if it ever shows up during debugging. */ 
for (i = 0; i < ngroups; i++) { 
if (desc == NULL) { 
if (ext4_mb_add_groupinfo(sb, i, desc) != 0) 
while (i-- > 0) 
while (i-- > 0) 
for (i = 0; i < NR_GRPINFO_CACHES; i++) { 
if (cache_index >= NR_GRPINFO_CACHES) 
if (unlikely(cache_index < 0)) 
if (ext4_groupinfo_caches[cache_index]) { 
if (!cachep) { 
for groupinfo slab cache\n"); 
if (sbi->s_mb_offsets == NULL) { 
if (sbi->s_mb_maxs == NULL) { 
if (ret < 0) 
while (i <= sb->s_blocksize_bits + 1); 
for 4k block 
if the cluster size 
for cluster sizes up to 64k, and after 
if (sbi->s_stripe > 1) { 
if (sbi->s_locality_groups == NULL) { 
for_each_possible_cpu(i) { 
for (j = 0; j < PREALLOC_TB_SIZE; j++) 
for buddy data */ 
if (ret != 0) 
if (sbi->s_proc) 
for_each_safe(cur, tmp, &grp->bb_prealloc_list) { 
if (count) 
if (sbi->s_proc) 
if (sbi->s_group_info) { 
for (i = 0; i < ngroups; i++) { 
ifdef DOUBLE_CHECK 
if 
for (i = 0; i < num_meta_group_infos; i++) 
if (sbi->s_buddy_cache) 
if (sbi->s_mb_stats) { 
if (test_opt(sb, DISCARD)) { 
if (err && err != -EOPNOTSUPP) 
for the group so that the next 
if (!test_opt(sb, DISCARD)) 
if (!db->bb_free_root.rb_node) { 
if (ext4_pspace_cachep == NULL) 
if (ext4_ac_cachep == NULL) { 
if (ext4_free_data_cachep == NULL) { 
for completion of call_rcu()'s on ext4_pspace_cachep 
if success or error code 
for_stack int 
if (!bitmap_bh) 
if (err) 
if (!gdp) 
if (err) 
if (!ext4_data_block_valid(sbi, block, len)) { 
if (!err) 
ifdef AGGRESSIVE_CHECK 
for (i = 0; i < ac->ac_b_ex.fe_len; i++) { 
if 
if (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) { 
if (!(ac->ac_flags & EXT4_MB_DELALLOC_RESERVED)) 
if (sbi->s_log_groups_per_flex) { 
if (err) 
for locality group 
if we set the same via mount option. 
for locality group\n", 
for_stack void 
if (!(ac->ac_flags & EXT4_MB_HINT_DATA)) 
if (unlikely(ac->ac_flags & EXT4_MB_HINT_GOAL_ONLY)) 
for example) */ 
if (ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC) { 
if (size < i_size_read(ac->ac_inode)) 
if (size <= 16 * 1024) { 
if (size <= 32 * 1024) { 
if (size <= 64 * 1024) { 
if (size <= 128 * 1024) { 
if (size <= 256 * 1024) { 
if (size <= 512 * 1024) { 
if (size <= 1024 * 1024) { 
if (NRL_CHECK_SIZE(size, 4 * 1024 * 1024, max, 2 * 1024)) { 
if (NRL_CHECK_SIZE(size, 8 * 1024 * 1024, max, 4 * 1024)) { 
if (NRL_CHECK_SIZE(ac->ac_o_ex.fe_len, 
if (ar->pleft && start <= ar->lleft) { 
if (ar->pright && start + size - 1 >= ar->lright) 
for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) { 
if (pa->pa_deleted) 
if (pa->pa_deleted) { 
if (pa->pa_lstart >= end || pa_end <= start) { 
if (pa_end <= ac->ac_o_ex.fe_logical) { 
if (pa->pa_lstart > ac->ac_o_ex.fe_logical) { 
for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) { 
if (pa->pa_deleted == 0) { 
if (start + size <= ac->ac_o_ex.fe_logical && 
if (ar->pright && (ar->lright == (start + size))) { 
if (ar->pleft && (ar->lleft + 1 == start)) { 
if (sbi->s_mb_stats && ac->ac_g_ex.fe_len > 1) { 
if (ac->ac_b_ex.fe_len >= ac->ac_o_ex.fe_len) 
if (ac->ac_g_ex.fe_start == ac->ac_b_ex.fe_start && 
if (ac->ac_found > sbi->s_mb_max_to_scan) 
if (ac->ac_op == EXT4_MB_HISTORY_ALLOC) 
for this 
if (pa && pa->pa_type == MB_INODE_PA) 
if (cpa == NULL) { 
if (cur_distance <= new_distance) 
for_stack int 
if (!(ac->ac_flags & EXT4_MB_HINT_DATA)) 
for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) { 
for them */ 
if (!(ext4_test_inode_flag(ac->ac_inode, EXT4_INODE_EXTENTS)) && 
if (pa->pa_deleted == 0 && pa->pa_free) { 
if (!(ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC)) 
for some reason */ 
if (lg == NULL) 
if (order > PREALLOC_TB_SIZE - 1) 
for the prealloc space that is having 
for (i = order; i < PREALLOC_TB_SIZE; i++) { 
for_each_entry_rcu(pa, &lg->lg_prealloc_list[i], 
if (pa->pa_deleted == 0 && 
if (cpa) { 
while (n) { 
for_stack 
form of preallocation discards first load group, 
for 
for_each(cur, &grp->bb_prealloc_list) { 
if (unlikely(len == 0)) 
for group %u\n", preallocated, group); 
if this was the last reference and the space is consumed 
if (!atomic_dec_and_test(&pa->pa_count) || pa->pa_free != 0) { 
if (pa->pa_deleted == 1) { 
if (pa->pa_type == MB_GROUP_PA) 
for given inode 
for_stack int 
if (pa == NULL) 
if (ac->ac_b_ex.fe_len < ac->ac_g_ex.fe_len) { 
if (offs && offs < win) 
for history */ 
for %u\n", pa, 
for locality group inodes belongs to 
for_stack int 
if (pa == NULL) 
for history */ 
for %u\n", pa, 
if (ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC) 
for_stack int 
while (bit < end) { 
if (bit >= end) 
if (free != pa->pa_free) { 
for_stack int 
for_stack int 
for group %u\n", group); 
if (bitmap_bh == NULL) { 
for %u", group); 
if (err) { 
formation for %u", group); 
if (needed == 0) 
for_each_entry_safe(pa, tmp, 
if (atomic_read(&pa->pa_count)) { 
if (pa->pa_deleted) { 
if we still need more blocks and some PAs were used, try again */ 
if (list_empty(&list)) { 
for_each_entry_safe(pa, tmp, &list, u.pa_tmp_list) { 
if (pa->pa_type == MB_GROUP_PA) 
for given inode 
if (!S_ISREG(inode->i_mode)) { 
for inode %lu\n", inode->i_ino); 
while (!list_empty(&ei->i_prealloc_list)) { 
if (atomic_read(&pa->pa_count)) { 
while we're discarding it */ 
while discarding"); 
if (pa->pa_deleted == 0) { 
if this happens too often, we can 
force wait only in case 
for_each_entry_safe(pa, tmp, &list, u.pa_tmp_list) { 
if (err) { 
formation for %u", 
if (bitmap_bh == NULL) { 
for %u", 
ifdef CONFIG_EXT4_DEBUG 
if (!ext4_mballoc_debug || 
for (i = 0; i < ngroups; i++) { 
for_each(cur, &grp->bb_prealloc_list) { 
if (grp->bb_free == 0) 
if 
for small size file. The size of the 
if (!(ac->ac_flags & EXT4_MB_HINT_DATA)) 
if (unlikely(ac->ac_flags & EXT4_MB_HINT_GOAL_ONLY)) 
if ((size == isize) && 
if (sbi->s_mb_group_prealloc <= 0) { 
for large files */ 
if (size > sbi->s_mb_stream_request) { 
for having 
for_stack int 
if (len >= EXT4_CLUSTERS_PER_GROUP(sb)) 
if (goal < le32_to_cpu(es->s_first_data_block) || 
for_stack void 
for_each_entry_rcu(pa, &lg->lg_prealloc_list[order], 
if (atomic_read(&pa->pa_count)) { 
for block allocation. So don't 
if (pa->pa_deleted) { 
if (total_entries <= 5) { 
for this list. 
for_each_entry_safe(pa, tmp, &discard_list, u.pa_tmp_list) { 
if (ext4_mb_load_buddy(sb, group, &e4b)) { 
formation for %u", 
if (order > PREALLOC_TB_SIZE - 1) 
for_each_entry_rcu(tmp_pa, &lg->lg_prealloc_list[order], 
if (tmp_pa->pa_deleted) { 
if (!added && pa->pa_free < tmp_pa->pa_free) { 
if (!added) 
if (lg_prealloc_count > 8) { 
if (pa) { 
if (pa) { 
while adding 
if ((pa->pa_type == MB_GROUP_PA) && likely(pa->pa_free)) { 
if (ac->ac_bitmap_page) 
if (ac->ac_buddy_page) 
if (ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC) 
for (i = 0; i < ngroups && needed > 0; i++) { 
for quota file */ 
if (ext4_test_inode_state(ar->inode, EXT4_STATE_DELALLOC_RESERVED)) 
ify 
ify allocation doesn't exceed the quota limits. 
while (ar->len && 
if (!ar->len) { 
if (ar->flags & EXT4_MB_USE_ROOT_BLOCKS) { 
while (ar->len && 
if (ar->len == 0) { 
if (!ac) { 
if (*errp) { 
if (!ext4_mb_use_preallocated(ac)) { 
if (*errp) 
if (ac->ac_status == AC_STATUS_FOUND && 
if (*errp) { 
if (likely(ac->ac_status == AC_STATUS_FOUND)) { 
if (*errp == -EAGAIN) { 
if (*errp) { 
if (freed) 
if (*errp) { 
if (ac) 
if (inquota && ar->len < inquota) 
if (!ar->len) { 
if non delalloc */ 
if the physical blocks 
if ((entry1->efd_tid == entry2->efd_tid) && 
for_stack int 
if (!*n) { 
while (*n) { 
if (cluster < entry->efd_start_cluster) 
if (cluster >= (entry->efd_start_cluster + entry->efd_count)) 
if (node) { 
if (can_merge(entry, new_entry) && 
if (node) { 
if (can_merge(new_entry, entry) && 
for this transaction 
if (bh) { 
if (!(flags & EXT4_FREE_BLOCKS_VALIDATED) && 
if (flags & EXT4_FREE_BLOCKS_FORGET) { 
for (i = 0; i < count; i++) { 
if (!bh) 
if (!tbh) 
forget(handle, flags & EXT4_FREE_BLOCKS_METADATA, 
if the inode is to be written in writeback mode 
if (!ext4_should_writeback_data(inode)) 
if (overflow) { 
if (count > overflow) 
if (overflow) { 
if (count > overflow) 
if (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT( 
if we are freeing blocks across a group 
if (EXT4_C2B(sbi, bit) + count > EXT4_BLOCKS_PER_GROUP(sb)) { 
if (!bitmap_bh) { 
if (!gdp) { 
if (in_range(ext4_block_bitmap(sb, gdp), block, count) || 
if (err) 
ify some metadata.  Call the journal APIs 
if (err) 
ifdef AGGRESSIVE_CHECK 
for (i = 0; i < count_clusters; i++) 
if 
if (err) 
if ((flags & EXT4_FREE_BLOCKS_METADATA) && ext4_handle_valid(handle)) { 
if (!new_entry) { 
if (test_opt(sb, DISCARD)) { 
if (err && err != -EOPNOTSUPP) 
if (sbi->s_log_groups_per_flex) { 
if (flags & EXT4_FREE_BLOCKS_RESERVE && ei->i_reserved_data_blocks) { 
if (flags & EXT4_FREE_BLOCKS_METADATA) 
if (!(flags & EXT4_FREE_BLOCKS_NO_QUOT_UPDATE)) 
if (!(flags & EXT4_FREE_BLOCKS_NO_QUOT_UPDATE)) 
if (!err) 
if (overflow && !err) { 
if (count == 0) 
if we are freeing blocks across a group 
if (bit + count > EXT4_BLOCKS_PER_GROUP(sb)) { 
if (!bitmap_bh) { 
if (!desc) { 
if (in_range(ext4_block_bitmap(sb, desc), block, count) || 
if (err) 
ify some metadata.  Call the journal APIs 
if (err) 
for (i = 0, blocks_freed = 0; i < count; i++) { 
if (!mb_test_bit(bit + i, bitmap_bh->b_data)) { 
for block %llu", 
if (err) 
if (sbi->s_log_groups_per_flex) { 
if (!err) 
for the file system 
for the group 
while 
for file system 
for free 
for free 
if (ret) { 
formation for %u", group); 
if (EXT4_MB_GRP_WAS_TRIMMED(e4b.bd_info) && 
while (start <= max) { 
if (start > max) 
if ((next - start) >= minblocks) { 
if (ret && ret != -EOPNOTSUPP) 
if (fatal_signal_pending(current)) { 
if (need_resched()) { 
if ((e4b.bd_info->bb_free - free_count) < minblocks) 
if (!ret) { 
for filesystem 
if (minlen > EXT4_CLUSTERS_PER_GROUP(sb) || 
if (end >= max_blks) 
if (end <= first_data_blk) 
if (start < first_data_blk) 
for (group = first_group; group <= last_group; group++) { 
if the grp has never been initialized */ 
if (ret) 
for the last group, note that last_cluster is 
if (group == last_group) 
if (grp->bb_free >= minlen) { 
if (cnt < 0) { 
if (!ret) 
file : ./test/kernel/fs/ext4/block_validity.c 
[ OK ] open : 4 ok... 
if (ext4_system_zone_cachep == NULL) 
if ((entry1->start_blk + entry1->count) == entry2->start_blk) 
while (*n) { 
if (start_blk < entry->start_blk) 
if (start_blk >= (entry->start_blk + entry->count)) 
if (start_blk + count > (entry->start_blk + 
if (!new_entry) { 
if (!new_entry) 
if (node) { 
if (can_merge(entry, new_entry)) { 
if (node) { 
if (can_merge(new_entry, entry)) { 
while (node) { 
if (!test_opt(sb, BLOCK_VALIDITY)) { 
if (EXT4_SB(sb)->system_blks.rb_node) 
for (i=0; i < ngroups; i++) { 
if (ret) 
if (ret) 
if (ret) 
if (test_opt(sb, DEBUG)) 
for_each_entry_safe(entry, n, 
if the passed-in block region (start_blk, 
if ((start_blk <= le32_to_cpu(sbi->s_es->s_first_data_block)) || 
while (n) { 
if (start_blk + count - 1 < entry->start_blk) 
if (start_blk >= (entry->start_blk + entry->count)) 
while (bref < p+max) { 
if (blk && 
file : ./test/kernel/fs/ext4/xattr_trusted.c 
[ OK ] open : 4 ok... 
for trusted extended attributes. 
if (!capable(CAP_SYS_ADMIN)) 
if (list && total_len <= list_size) { 
if (strcmp(name, "") == 0) 
if (strcmp(name, "") == 0) 
file : ./test/kernel/fs/ext4/symlink.c 
[ OK ] open : 4 ok... 
file : ./test/kernel/fs/ext4/hash.c 
[ OK ] open : 4 ok... 
form(__u32 buf[4], __u32 const in[]) 
while (--n); 
while (len--) { 
if (hash & 0x80000000) 
while (len--) { 
if (hash & 0x80000000) 
if (len > num*4) 
for (i = 0; i < len; i++) { 
if ((i % 4) == 3) { 
if (--num >= 0) 
while (--num >= 0) 
if (len > num*4) 
for (i = 0; i < len; i++) { 
if ((i % 4) == 3) { 
if (--num >= 0) 
while (--num >= 0) 
ify a hash.  If the seed is all zero's, then some default seed 
ifies whether or not the seed is 
for the minor hash. 
for the hash checksum functions */ 
if the seed is all zero's */ 
for (i = 0; i < 4; i++) { 
while (len > 0) { 
form(buf, in); 
while (len > 0) { 
form(buf, in); 
if (hash == (EXT4_HTREE_EOF_32BIT << 1)) 
file : ./test/kernel/fs/ext4/bitmap.c 
[ OK ] open : 4 ok... 
ify(struct super_block *sb, ext4_group_t group, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (sbi->s_desc_size >= EXT4_BG_INODE_BITMAP_CSUM_HI_END) { 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (sbi->s_desc_size >= EXT4_BG_INODE_BITMAP_CSUM_HI_END) 
ify(struct super_block *sb, ext4_group_t group, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (sbi->s_desc_size >= EXT4_BG_BLOCK_BITMAP_CSUM_HI_END) { 
if (provided == calculated) 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (sbi->s_desc_size >= EXT4_BG_BLOCK_BITMAP_CSUM_HI_END) 
file : ./test/kernel/fs/ext4/ialloc.c 
[ OK ] open : 4 ok... 
for blocks, 1 bitmap 
for the rest of the bitmap as there are no other users. 
if (start_bit >= end_bit) 
for (i = start_bit; i < ((start_bit + 7) & ~7UL); i++) 
if (i < end_bit) 
if (!ext4_group_desc_csum_verify(sb, block_group, gdp)) { 
for group %u", block_group); 
if (!EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
if (!EXT4_MB_GRP_IBITMAP_CORRUPT(grp)) { 
if (uptodate) { 
for a given block_group, reading 
if (!desc) 
if (unlikely(!bh)) { 
if (bitmap_uptodate(bh)) 
if (bitmap_uptodate(bh)) { 
ify; 
if (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) { 
ified(bh); 
if (buffer_uptodate(bh)) { 
if not uninit if bh is uptodate, 
ify; 
for reading 
if (!buffer_uptodate(bh)) { 
ify: 
if (!buffer_verified(bh) && 
if (!EXT4_MB_GRP_IBITMAP_CORRUPT(grp)) { 
ified(bh); 
fore_ we mark the inode not in use in the inode 
if (!sb) { 
if (atomic_read(&inode->i_count) > 1) { 
if (inode->i_nlink) { 
fore locking the superblock, 
if (ino < EXT4_FIRST_INO(sb) || ino > le32_to_cpu(es->s_inodes_count)) { 
if the inode bitmap is corrupt. */ 
if (unlikely(EXT4_MB_GRP_IBITMAP_CORRUPT(grp)) || !bitmap_bh) 
if (fatal) 
if (gdp) { 
if (fatal || !cleared) { 
if (is_directory) { 
if (sbi->s_log_groups_per_flex) { 
if (is_directory) 
if (cleared) { 
if (!fatal) 
for inode %lu", ino); 
for Orlov's allocator; returns critical information 
if (flex_size > 1) { 
if (desc) { 
for directories. 
if it doesn't satisfy these 
for a group with more 
if (flex_size > 1) { 
if (S_ISDIR(mode) && 
if (qstr) { 
for (i = 0; i < ngroups; i++) { 
if (!stats.free_inodes) 
if (stats.used_dirs >= best_ndir) 
if (stats.free_inodes < avefreei) 
if (stats.free_clusters < avefreec) 
if (ret) 
if (flex_size == 1) { 
for (i = 0; i < flex_size; i++) { 
if (desc && ext4_free_inodes_count(sb, desc)) { 
if (min_inodes < 1) 
for this parent directory 
if (EXT4_I(parent)->i_last_alloc_group != ~0) { 
if (flex_size > 1) 
for (i = 0; i < ngroups; i++) { 
if (stats.used_dirs >= max_dirs) 
if (stats.free_inodes < min_inodes) 
if (stats.free_clusters < min_clusters) 
for (i = 0; i < ngroups; i++) { 
if (desc) { 
if (grp_free && grp_free >= avefreei) { 
if (avefreei) { 
for really small 
formation in the 
for future allocations. 
if (flex_size > 1) { 
if (last > ngroups) 
for  (i = parent_group; i < last; i++) { 
if (desc && ext4_free_inodes_count(sb, desc)) { 
if (!retry && EXT4_I(parent)->i_last_alloc_group != ~0) { 
if (*group > ngroups) 
if (desc && ext4_free_inodes_count(sb, desc) && 
ifferent blockgroup from its 
ifferent 
ifferent blockgroup. 
for the hash. 
for (i = 1; i < ngroups; i <<= 1) { 
if (*group >= ngroups) 
if (desc && ext4_free_inodes_count(sb, desc) && 
if that group 
for (i = 0; i < ngroups; i++) { 
if (desc && ext4_free_inodes_count(sb, desc)) 
if an inode has recently been deleted, we want 
if (unlikely(!gdp)) 
if (unlikely(!bh) || !buffer_uptodate(bh)) 
if (buffer_dirty(bh)) 
if (dtime && (dtime < now) && (now < dtime + recentcy)) 
for allocating an inode.  If the new inode is 
if that fails, then of 
forward from the parent directory's block 
if (!dir || !dir->i_nlink) 
if (!inode) 
for quota initialization worst case in standard inode creating 
if (owner) { 
if (test_opt(sb, GRPID)) { 
if (!goal) 
if (goal && goal <= le32_to_cpu(sbi->s_es->s_inodes_count)) { 
if (S_ISDIR(mode)) 
if (ret2 == -1) 
for (i = 0; i < ngroups; i++, ino = 0) { 
if (!gdp) 
fore loading bitmap. 
if (ext4_free_inodes_count(sb, gdp) == 0) { 
if (EXT4_MB_GRP_IBITMAP_CORRUPT(grp)) { 
if (EXT4_MB_GRP_IBITMAP_CORRUPT(grp) || !inode_bitmap_bh) { 
if (ino >= EXT4_INODES_PER_GROUP(sb)) 
if (group == 0 && (ino+1) < EXT4_FIRST_INO(sb)) { 
if ((EXT4_SB(sb)->s_journal == NULL) && 
if (!handle) { 
if (IS_ERR(handle)) { 
if (err) { 
if (!ret2) 
if (ino < EXT4_INODES_PER_GROUP(sb)) 
if (++group == ngroups) 
if (err) { 
if it isn't already */ 
if (err) { 
if we still need to */ 
if (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) { 
if (err) { 
if (err) { 
if (ext4_has_group_desc_csum(sb)) { 
ify the bg desc */ 
if (gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) { 
if it is greater 
if (ino > free) 
if (S_ISDIR(mode)) { 
if (sbi->s_log_groups_per_flex) { 
if (ext4_has_group_desc_csum(sb)) { 
if (err) { 
if (S_ISDIR(mode)) 
if (sbi->s_log_groups_per_flex) { 
for stat), not the fs block size */ 
if (IS_DIRSYNC(inode)) 
if (insert_inode_locked(inode) < 0) { 
for inode metadata */ 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_INLINE_DATA)) 
if (err) 
if (err) 
if (err) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) { 
for directory, file and normal symlink*/ 
if (ext4_handle_valid(handle)) { 
if (err) { 
ify that we are loading a valid orphan from disk */ 
for us */ 
if (!bitmap_bh) { 
for orphan %lu", ino); 
if (!ext4_test_bit(bit, bitmap_bh->b_data)) 
if (IS_ERR(inode)) 
if (inode->i_nlink && !ext4_can_truncate(inode)) 
if (NEXT_ORPHAN(inode) > max_ino) 
if (inode) { 
if we got a bad deleted inode */ 
ifdef EXT4FS_DEBUG 
for (i = 0; i < ngroups; i++) { 
if (!gdp) 
if (!bitmap_bh) 
for (i = 0; i < ngroups; i++) { 
if (!gdp) 
if 
for (i = 0; i < ngroups; i++) { 
if (!gdp) 
if (sb->s_flags & MS_RDONLY) { 
if (!gdp) 
if (gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)) 
if (IS_ERR(handle)) { 
if (!(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT))) 
if ((used_blks < 0) || (used_blks > sbi->s_itb_per_group)) { 
if (ret) 
if the inode table is full. But we set the ZEROED 
if (unlikely(num == 0)) 
if (ret < 0) 
if (barrier) 
file : ./test/kernel/fs/ext4/fsync.c 
[ OK ] open : 4 ok... 
for little endian machines 
if it was freshly created) since 
for 
if (!ext4_test_inode_state(inode, EXT4_STATE_NEWENTRY)) 
while (ext4_test_inode_state(inode, EXT4_STATE_NEWENTRY)) { 
if (!dentry) 
if (!next) 
if (ret) 
if (ret) 
for ext4_sync_file(). 
if (inode->i_sb->s_flags & MS_RDONLY) { 
if (EXT4_SB(inode->i_sb)->s_mount_flags & EXT4_MF_FS_ABORTED) 
if (!journal) { 
if (!ret && !hlist_empty(&inode->i_dentry)) 
if (ret) 
for proper transaction to 
force_commit will write the file data into the journal and 
if (ext4_should_journal_data(inode)) { 
force_commit(inode->i_sb); 
if (journal->j_flags & JBD2_BARRIER && 
if (needs_barrier) { 
if (!ret) 
file : ./test/kernel/fs/ext4/inline.c 
[ OK ] open : 4 ok... 
ify it 
for more details. 
if (EXT4_I(inode)->i_inline_off) 
if (!ext4_test_inode_state(inode, EXT4_STATE_XATTR)) 
for (; !IS_LAST_ENTRY(entry); entry = EXT4_XATTR_NEXT(entry)) { 
if (offs < min_offs) 
if (EXT4_I(inode)->i_inline_off) { 
if (free > EXT4_XATTR_ROUND) 
for a xattr entry, don't use the space 
if (EXT4_I(inode)->i_extra_isize == 0) 
if (error) { 
if (!max_inline_size) 
form ext4_iget, before 
if (EXT4_I(inode)->i_extra_isize == 0) 
if (error) 
if (error) 
if (!is.s.not_found) { 
if (!len) 
if (!len) 
if (pos < EXT4_MIN_INLINE_DATA_SIZE) { 
if (!len) 
if (error) 
if (error) 
if (len > EXT4_MIN_INLINE_DATA_SIZE) { 
if (error) 
if (error) { 
if (len <= EXT4_I(inode)->i_inline_size) 
if (error) 
if (error) 
if (!value) 
if (error == -ENODATA) 
if (error) 
if (error) 
if (!ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA)) 
if (size < len) 
if (ei->i_inline_off) 
if (!ei->i_inline_off) 
if (error) 
if (error) 
if (error) 
if (error) 
if (EXT4_HAS_INCOMPAT_FEATURE(inode->i_sb, 
if (S_ISDIR(inode->i_mode) || 
if (error == -ENODATA) 
if (!EXT4_I(inode)->i_inline_off) { 
if (ret) 
if (!ext4_has_inline_data(inode)) { 
for all the other pages, just set them uptodate. 
if (!page->index) 
if (!PageUptodate(page)) { 
if (!ext4_has_inline_data(inode)) { 
if (ret) 
if (IS_ERR(handle)) { 
if (!page) { 
for us, just exit. */ 
if (!PageUptodate(page)) { 
if (ret < 0) 
if (ret) 
if (ext4_should_dioread_nolock(inode)) 
if (!ret && ext4_should_journal_data(inode)) { 
if (ret) { 
if (inode->i_nlink) 
if (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries)) 
if (page) { 
if (sem_held) 
if (handle) 
for it. 
if (pos + len > ext4_get_max_inline_size(inode)) 
if (ret) 
if (IS_ERR(handle)) { 
if (ret && ret != -ENOSPC) 
if (ret == -ENOSPC) { 
if (!page) { 
if (!ext4_has_inline_data(inode)) { 
if (!PageUptodate(page)) { 
if (ret < 0) 
if (handle) 
if (unlikely(copied < len)) { 
if (ret) { 
for us. */ 
if (ret) { 
for the inline data case. 
if (!page) 
if (!ext4_has_inline_data(inode)) { 
if (!PageUptodate(page)) { 
if (ret < 0) 
if (ret) { 
if (page) { 
for the inline data. 
if (ret) 
if (IS_ERR(handle)) { 
if (inline_size >= pos + len) { 
if (ret && ret != -ENOSPC) 
if (ret == -ENOSPC) { 
if (ret == -ENOSPC && 
if (!page) { 
if (!ext4_has_inline_data(inode)) { 
if (!PageUptodate(page)) { 
if (ret < 0) 
while still holding page lock: 
if (pos+copied > inode->i_size) { 
forces lock 
if (i_size_changed) 
ifdef INLINE_DIR_DEBUG 
while ((void *)de < dlimit) { 
if (ext4_check_dir_entry(dir, NULL, de, bh, 
if 
if no space is available, and -EIO 
if (err) 
if (err) 
ifferent from the directory change time. 
if (old_size) { 
while (de_buf < limit); 
if (new_size - old_size <= EXT4_DIR_REC_LEN(1)) 
if (ret) 
formation 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (csum_size) { 
if (err) 
ified(dir_block); 
if (!buf) { 
if (error < 0) 
if (error) 
if (error < 0) 
if (!(map.m_flags & EXT4_MAP_MAPPED)) { 
if (!data_bh) { 
if (error) { 
if (!S_ISDIR(inode->i_mode)) { 
if (error) 
if (ret) 
if (!ext4_has_inline_data(dir)) 
if (ret != -ENOSPC) 
if (!inline_size) { 
if (ret && ret != -ENOSPC) 
if (inline_size) { 
if (ret != -ENOSPC) 
for it. 
formation from an 
if (ret) 
if (!ext4_has_inline_data(inode)) { 
if (!dir_buf) { 
if (ret < 0) 
while (pos < inline_size) { 
formation about '.' and 
ifferently. 
if (pos == 0) { 
if (pos == EXT4_INLINE_DOTDOT_OFFSET) { 
if (ext4_check_dir_entry(inode, dir_file, de, 
if ((hinfo->hash < start_hash) || 
if (de->inode == 0) 
if (err) { 
if '.' and '..' really take place. 
if (ret) 
if (!ext4_has_inline_data(inode)) { 
if (!dir_buf) { 
if (ret < 0) 
if the dir is block based while 
for them are only EXT4_INLINE_DOTDOT_SIZE. 
if (file->f_version != inode->i_version) { 
for (i = 0; i < extra_size && i < offset;) { 
if (!i) { 
if (i == dotdot_offset) { 
for other entry, the real offset in 
if (ext4_rec_len_from_disk(de->rec_len, extra_size) 
while (ctx->pos < extra_size) { 
if (!dir_emit(ctx, ".", 1, inode->i_ino, DT_DIR)) 
if (ctx->pos == dotdot_offset) { 
if (ext4_check_dir_entry(inode, file, de, iloc.bh, dir_buf, 
if (le32_to_cpu(de->inode)) { 
if (*retval) 
for the new dir. 
if (ret) 
if (ret) 
formation for the ".." 
if (ext4_get_inode_loc(dir, &iloc)) 
if (!ext4_has_inline_data(dir)) { 
if (ret == 1) 
if (ret < 0) 
if (ext4_get_inline_size(dir) == EXT4_MIN_INLINE_DATA_SIZE) 
if (ret == 1) 
if (err) 
if (!ext4_has_inline_data(dir)) { 
if ((void *)de_del - ((void *)ext4_raw_inode(&iloc)->i_block) < 
if (err) 
if (err) 
if (unlikely(err)) 
if (err != -ENOENT) 
if (offset < EXT4_MIN_INLINE_DATA_SIZE) { 
if (inline_start) 
if (err) { 
if (!ext4_has_inline_data(dir)) { 
if (!le32_to_cpu(de->inode)) { 
while (offset < dir->i_size) { 
if (ext4_check_dir_entry(dir, NULL, de, 
if (le32_to_cpu(de->inode)) { 
if (!ext4_has_inline_data(inode)) { 
if (error) 
if (physical) 
if we can sparse space 'needed', 
if (error) 
if (EXT4_XATTR_LEN(entry->e_name_len) + 
if (IS_ERR(handle)) 
if (!ext4_has_inline_data(inode)) { 
if (ext4_orphan_add(handle, inode)) 
if (ext4_get_inode_loc(inode, &is.iloc)) 
if (i_size < inline_size) { 
if (inline_size > EXT4_MIN_INLINE_DATA_SIZE) { 
if (!value) 
if (ext4_xattr_ibody_get(inode, i.name_index, i.name, 
if (ext4_xattr_ibody_inline_set(handle, inode, &i, &is)) 
if (i_size < EXT4_MIN_INLINE_DATA_SIZE) { 
if (inode->i_nlink) 
if (IS_SYNC(inode)) 
if (!ext4_has_inline_data(inode)) { 
if (error) 
if (IS_ERR(handle)) { 
if (!ext4_has_inline_data(inode)) { 
file : ./test/kernel/fs/ext4/xattr_security.c 
[ OK ] open : 4 ok... 
for storing security labels as extended attributes. 
if (list && total_len <= list_size) { 
if (strcmp(name, "") == 0) 
if (strcmp(name, "") == 0) 
for (xattr = xattr_array; xattr->name != NULL; xattr++) { 
if (err < 0) 
file : ./test/kernel/fs/ext4/acl.c 
[ OK ] open : 4 ok... 
if (!value) 
if (size < sizeof(ext4_acl_header)) 
if (((ext4_acl_header *)value)->a_version != 
if (count < 0) 
if (count == 0) 
if (!acl) 
for (n = 0; n < count; n++) { 
if ((char *)value + sizeof(ext4_acl_entry_short) > end) 
if ((char *)value > end) 
if ((char *)value > end) 
if (value != end) 
if (!ext_acl) 
for (n = 0; n < acl->a_count; n++) { 
if (retval > 0) { 
if (!value) 
if (retval > 0) 
if (retval == -ENODATA || retval == -ENOSYS) 
if (!IS_ERR(acl)) 
if (acl) { 
if (error < 0) 
if (error == 0) 
if (!S_ISDIR(inode->i_mode)) 
if (acl) { 
if (IS_ERR(value)) 
if (!error) 
if (IS_ERR(handle)) 
if (error == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries)) 
if (error) 
if (default_acl) { 
if (acl) { 
file : ./test/kernel/fs/ext4/migrate.c 
[ OK ] open : 4 ok... 
ify it 
if (lb->first_pblock == 0) 
if (IS_ERR(path)) { 
for_single_extent(inode, 
if (needed && ext4_handle_has_enough_credits(handle, 
if (retval) 
if (needed) { 
if (retval) { 
if (retval) 
if (path) { 
if we can add on to the existing range (if it exists) 
if (lb->first_pblock && 
if (!bh) 
for (i = 0; i < max_entries; i++) { 
if (retval) 
if (!bh) 
for (i = 0; i < max_entries; i++) { 
if (retval) 
if (!bh) 
for (i = 0; i < max_entries; i++) { 
if (retval) 
for_blkdel(handle_t *handle, struct inode *inode) 
if (ext4_handle_has_enough_credits(handle, EXT4_RESERVE_TRANS_BLOCKS+1)) 
if (ext4_journal_extend(handle, needed) != 0) 
if (!bh) 
for (i = 0; i < max_entries; i++) { 
for_blkdel(handle, inode); 
for_blkdel(handle, inode); 
if (!bh) 
for (i = 0; i < max_entries; i++) { 
if (retval) { 
for_blkdel(handle, inode); 
if (i_data[0]) { 
for_blkdel(handle, inode); 
if (i_data[1]) { 
if (retval) 
if (i_data[2]) { 
if (retval) 
for writing the 
if (retval) { 
if (retval) 
if EXT4_STATE_EXT_MIGRATE is cleared a block allocation 
if (!ext4_test_inode_state(inode, EXT4_STATE_EXT_MIGRATE)) { 
for extent index 
for extent blocks 
if (!bh) 
if (eh->eh_depth != 0) { 
for (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ix++) { 
if (retval) 
for_blkdel(handle, inode); 
if (eh->eh_depth == 0) 
for extent meta data 
for (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ix++) { 
if (retval) 
if (!EXT4_HAS_INCOMPAT_FEATURE(inode->i_sb, 
if (S_ISLNK(inode->i_mode) && inode->i_blocks == 0) 
ifying the quota inode. 
if (IS_ERR(handle)) { 
if (IS_ERR(tmp_inode)) { 
for 
if (IS_ERR(handle)) { 
for (i = 0; i < EXT4_NDIR_BLOCKS; i++) { 
if (retval) 
if (i_data[EXT4_IND_BLOCK]) { 
if (retval) 
if (i_data[EXT4_DIND_BLOCK]) { 
if (retval) 
if (i_data[EXT4_TIND_BLOCK]) { 
if (retval) 
if (retval) 
formation with the 
if (retval) 
if we fail to swap inode data free the extent 
if (ext4_journal_extend(handle, 1) != 0) 
if (!EXT4_HAS_INCOMPAT_FEATURE(inode->i_sb, 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (IS_ERR(handle)) 
if (ret) 
if (ext4_blocks_count(es) > EXT4_MAX_BLOCK_FILE_PHYS || 
if (eh->eh_entries == 0) 
if (len > EXT4_NDIR_BLOCKS) { 
for (i=0; i < len; i++) 
file : ./test/kernel/fs/ext4/resize.c 
[ OK ] open : 4 ok... 
for resizing an ext4 filesystem while it is mounted. 
if (!capable(CAP_SYS_RESOURCE)) 
if (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) { 
if (test_and_set_bit_lock(EXT4_RESIZING, &EXT4_SB(sb)->s_resize_flags)) 
if (ext4_bg_has_super(sb, group)) 
ify_group_input(struct super_block *sb, 
if (group != sbi->s_groups_count) { 
if (test_opt(sb, DEBUG)) 
if (offset != 0) 
if (input->reserved_blocks > input->blocks_count / 5) 
if (free_blocks_count < 0) 
if (!(bh = sb_bread(sb, end - 1))) 
if (outside(input->block_bitmap, start, end)) 
if (outside(input->inode_bitmap, start, end)) 
if (outside(input->inode_table, start, end) || 
if (input->inode_bitmap == input->block_bitmap) 
if (inside(input->block_bitmap, input->inode_table, itend)) 
if (inside(input->inode_bitmap, input->inode_table, itend)) 
if (inside(input->block_bitmap, start, metaend)) 
if (inside(input->inode_bitmap, start, metaend)) 
if (inside(input->inode_table, start, metaend) || 
for groups 
if (flex_gd == NULL) 
if (flexbg_size >= UINT_MAX / sizeof(struct ext4_new_flex_group_data)) 
if (flex_gd->groups == NULL) 
if (flex_gd->bg_flags == NULL) 
for a flex group. 
if (src_group >= group_data[0].group + flex_gd->count) 
for (; src_group <= last_group; src_group++) { 
if (overhead == 0) 
for (; bb_index < flex_gd->count; bb_index++) { 
for (; ib_index < flex_gd->count; ib_index++) { 
for (; it_index < flex_gd->count; it_index++) { 
if (start_blk + itb > last_blk) 
if (start_blk + itb > next_group_start) { 
if (test_opt(sb, DEBUG)) { 
for (i = 0; i < flex_gd->count; i++) { 
if (unlikely(!bh)) 
if ((err = ext4_journal_get_write_access(handle, bh))) { 
for the 
if (ext4_handle_has_enough_credits(handle, thresh)) 
if (err < 0) 
if (err) { 
if (err) 
for ext4_setup_new_group_blocks() which set . 
for (count2 = count; count > 0; count -= count2, block += count2) { 
if (count2 > count) 
if (flex_gd->bg_flags[group] & EXT4_BG_BLOCK_UNINIT) { 
if (err) 
if (unlikely(!bh)) 
if (err) 
if (unlikely(err)) 
for the new groups. 
if necessary. 
for blocks taken by 
for blocks taken by group tables. 
if (IS_ERR(handle)) 
for (i = 0; i < flex_gd->count; i++, group++) { 
if (meta_bg == 0 && !ext4_bg_has_super(sb, group)) 
if (meta_bg == 1) { 
if (first_group != group + 1 && 
for (j = 0; j < gdblocks; j++, block++) { 
if (err) 
if (unlikely(!gdb)) { 
if (err) { 
if (unlikely(err)) { 
if (ext4_bg_has_super(sb, group)) { 
if (err) 
if (!(bg_flags[i] & EXT4_BG_INODE_ZEROED)) 
if (err) 
if (bg_flags[i] & EXT4_BG_BLOCK_UNINIT) 
if (err) 
if (IS_ERR(bh)) { 
if (overhead != 0) { 
if (err) 
if (bg_flags[i] & EXT4_BG_INODE_UNINIT) 
if (err) 
if (IS_ERR(bh)) { 
if (err) 
for (j = 0; j < GROUP_TABLE_COUNT; j++) { 
for (i = 1; i < flex_gd->count; i++) { 
if (block == (&group_data[i].block_bitmap)[j]) { 
if (err) 
if (count) { 
if (err) 
if (err2 && !err) 
fore 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (*five < *min) { 
if (*seven < *min) { 
ify_reserved_gdb(struct super_block *sb, 
while ((grp = ext4_list_backups(sb, &three, &five, &seven)) < end) { 
if (++gdbackups > EXT4_ADDR_PER_BLOCK(sb)) 
ifying the data on disk, because JBD has no rollback. 
if (test_opt(sb, DEBUG)) 
if (EXT4_SB(sb)->s_sbh->b_blocknr != 
if (!gdb_bh) 
ify_reserved_gdb(sb, group, gdb_bh); 
if (!dind) { 
if (le32_to_cpu(data[gdb_num % EXT4_ADDR_PER_BLOCK(sb)]) != gdblock) { 
if (unlikely(err)) 
if (unlikely(err)) 
if (unlikely(err)) 
if (unlikely(err)) 
if (!n_group_desc) { 
for %lu groups", 
for use (which also "frees" the backup GDT blocks 
if (unlikely(err)) { 
if (unlikely(err)) { 
if (err) 
if (!gdb_bh) 
if (!n_group_desc) { 
for %lu groups", 
if (unlikely(err)) 
for future resizing and not allocated to files. 
ify it is pointing to the primary reserved 
if (!primary) 
if (!dind) { 
ify it holds backups */ 
for (res = 0; res < reserved_gdb; res++, blk++) { 
if (!primary[res]) { 
ify_reserved_gdb(sb, group, primary[res]); 
if (++data >= end) 
for (i = 0; i < reserved_gdb; i++) { 
if ((err = ext4_journal_get_write_access(handle, primary[i]))) 
if ((err = ext4_reserve_inode_write(handle, inode, &iloc))) 
for (i = 0; i < reserved_gdb; i++) { 
if (!err) 
while (--res >= 0) 
if there 
if possible, in case the primary gets trashed 
for some reason and we need to run e2fsck from a backup superblock.  The 
for this, because these 
if (IS_ERR(handle)) { 
if (meta_bg == 0) { 
while (group < sbi->s_groups_count) { 
if (ext4_handle_valid(handle) && 
if (meta_bg == 0) 
if (unlikely(!bh)) { 
if ((err = ext4_journal_get_write_access(handle, bh))) 
if (rest) 
if (unlikely(err)) 
if (meta_bg == 0) 
if (group == last) 
if ((err2 = ext4_journal_stop(handle)) && !err) 
if we got here we have a journal problem too, so we 
if not - we will simply wait until next fsck. 
if (err) { 
for group %u (err %d), " 
for (i = 0; i < count; i++, group++) { 
for the first group in a new group block. 
if (gdb_off) { 
if (!err && reserved_gdb && ext4_bg_num_gdb(sb, group)) 
if (meta_bg != 0) { 
if (err) 
if (unlikely(!bh)) 
if (!bh_uptodate_or_lock(bh)) { 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (!bh) 
if (!bh) 
for (i = 0; i < flex_gd->count; i++, group_data++, bg_flags++) { 
for new group */ 
if (err) { 
if (ext4_has_group_desc_csum(sb)) 
if (unlikely(err)) { 
for mb_alloc based on the new group 
if (err) 
fore 
fore the group is live won't actually let us 
for (i = 0; i < flex_gd->count; i++) { 
form a smp_wmb() after updating all 
form an smp_rmb() after reading the groups 
while freeing data, as we can only allocate from a block 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, 
formation 
if (test_opt(sb, DEBUG)) 
ifying the filesystem, because we cannot abort the 
if (err) 
ifying at least the superblock and  GDT 
ify the inode and the dindirect block.  If we 
ify each of the reserved GDT dindirect blocks. 
if (IS_ERR(handle)) { 
if (err) 
if (err) 
if (err) 
if (!err) 
if (!err) { 
for (; gdb_num <= gdb_num_end; gdb_num++) { 
if (old_gdb == gdb_bh->b_blocknr) 
if (o_blocks_count == n_blocks_count) 
if (last_group > n_group) 
for (i = 0; i < flex_gd->count; i++) { 
if (ext4_has_group_desc_csum(sb)) { 
if (!test_opt(sb, INIT_INODE_TABLE)) 
if (last_group == n_group && ext4_has_group_desc_csum(sb)) 
if ((last_group == n_group) && (last != blocks_per_group - 1)) { 
ifying 
for a sparse group. 
while we are actually adding 
ifying. 
if (gdb_off == 0 && !EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (ext4_blocks_count(es) + input->blocks_count < 
if (le32_to_cpu(es->s_inodes_count) + EXT4_INODES_PER_GROUP(sb) < 
if (reserved_gdb || gdb_off == 0) { 
if (IS_ERR(inode)) { 
ify_group_input(sb, input); 
if (err) 
if (err) 
if (IS_ERR(handle)) { 
if (err) { 
if (err) 
if (err2 && !err) 
if (!err) { 
ified.  This entry 
for emergencies (because it has no dependencies on reserved blocks). 
for arbitrary resizing, assuming enough 
if (test_opt(sb, DEBUG)) 
if (n_blocks_count == 0 || n_blocks_count == o_blocks_count) 
if (n_blocks_count > (sector_t)(~0ULL) >> (sb->s_blocksize_bits - 9)) { 
if (sizeof(sector_t) < 8) 
if (n_blocks_count < o_blocks_count) { 
if (last == 0) { 
if (o_blocks_count + add < o_blocks_count) { 
if (o_blocks_count + add > n_blocks_count) 
if (o_blocks_count + add < n_blocks_count) 
if the device is actually as big as what was requested */ 
if (!bh) { 
if there 
if (inode) { 
if (inode->i_blocks != 1 << (inode->i_blkbits - 9)) 
for (i = 0; i < EXT4_N_BLOCKS; i++) { 
if (ei->i_data[i]) 
if (ei->i_data[i]) 
if (IS_ERR(handle)) 
if (err) 
if (err) { 
if (inode) { 
if (err) 
if (!err) 
ified by @n_blocks_count 
if the device is actually as big as what was requested */ 
if (!bh) { 
if (n_blocks_count < o_blocks_count) { 
if (n_blocks_count == o_blocks_count) 
if (n_group > (0xFFFFFFFFUL / EXT4_INODES_PER_GROUP(sb))) { 
if (EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_RESIZE_INODE)) { 
if (n_desc_blocks > o_desc_blocks + 
if (!resize_inode) 
if (IS_ERR(resize_inode)) { 
if ((!resize_inode && !meta_bg) || n_blocks_count == o_blocks_count) { 
if (err) 
if (resize_inode) { 
if (n_blocks_count_retry) { 
if (n_group == o_group) 
if (add > 0) { 
if (err) 
if (ext4_blocks_count(es) == n_blocks_count) 
if (err) 
if (err) 
if (flex_gd == NULL) { 
while (ext4_setup_next_flex_gd(sb, flex_gd, n_blocks_count, 
if (jiffies - last_update_time > HZ * 10) { 
iffies; 
if (ext4_alloc_group_tables(sb, flex_gd, flexbg_size) != 0) 
if (unlikely(err)) 
if (!err && n_blocks_count_retry) { 
if (flex_gd) 
if (resize_inode != NULL) 
file : ./test/kernel/fs/ext4/super.c 
[ OK ] open : 4 ok... 
for trace points definition */ 
if !defined(CONFIG_EXT2_FS) && !defined(CONFIG_EXT2_FS_MODULE) && defined(CONFIG_EXT4_USE_FOR_EXT23) 
if 
if 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
ify(struct super_block *sb, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (!ret) 
if (!ret) 
if (is_vmalloc_addr(ptr)) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (!es->s_first_error_time) { 
if it hasn't been 
if (!es->s_error_count) 
ific data 
for example, by ext4_commit_super), will cause a kernel OOPS. 
form the VFS and file system layers. 
while (!list_empty(&txn->t_private_list)) { 
if (sb->s_flags & MS_RDONLY) 
if (!test_opt(sb, ERRORS_CONT)) { 
if (journal) 
if (test_opt(sb, ERRORS_RO)) { 
fore ->s_flags update 
if (test_opt(sb, ERRORS_PANIC)) 
forced after error\n", 
format vaf; 
if (ext4_error_ratelimit(sb)) { 
format vaf; 
if (ext4_error_ratelimit(inode->i_sb)) { 
if (block) 
format vaf; 
if (ext4_error_ratelimit(inode->i_sb)) { 
if (IS_ERR(path)) 
if (block) 
if (!sb || (EXT4_SB(sb)->s_journal && 
for unknown 
if (nbuf) { 
for truncated error codes... */ 
if the error is EROFS, and we're not already 
if (errno == -EROFS && journal_current_handle() == NULL && 
if (ext4_error_ratelimit(sb)) { 
force the filesystem into an ABORT|READONLY state, 
if ((sb->s_flags & MS_RDONLY) == 0) { 
fore ->s_flags update 
if (EXT4_SB(sb)->s_journal) 
if (test_opt(sb, ERRORS_PANIC)) 
format vaf; 
if (!___ratelimit(&(EXT4_SB(sb)->s_msg_ratelimit_state), "EXT4-fs")) 
format vaf; 
if (!___ratelimit(&(EXT4_SB(sb)->s_warning_ratelimit_state), 
format vaf; 
if (ext4_error_ratelimit(sb)) { 
if (ino) 
if (block) 
if (test_opt(sb, ERRORS_CONT)) { 
if (le32_to_cpu(es->s_rev_level) > EXT4_GOOD_OLD_REV) 
if empty */ 
if not it 
if (IS_ERR(bdev)) 
if (bdev) { 
for_each(l, &sbi->s_orphan) { 
if (sbi->s_journal) { 
if (err < 0) 
if (!(sb->s_flags & MS_RDONLY)) { 
if (!(sb->s_flags & MS_RDONLY)) 
if (sbi->s_proc) { 
for (i = 0; i < sbi->s_gdb_count; i++) 
ifdef CONFIG_QUOTA 
for (i = 0; i < MAXQUOTAS; i++) 
if 
if we've 
if (!list_empty(&sbi->s_orphan)) 
if (sbi->journal_bdev && sbi->journal_bdev != sb->s_bdev) { 
if (sbi->s_mb_cache) { 
if (sbi->s_mmp_tsk) 
for_completion(&sbi->s_kobj_unregister); 
if (!ei) 
ifdef CONFIG_QUOTA 
if 
if (!list_empty(&(EXT4_I(inode)->i_orphan))) { 
if (ext4_inode_cachep == NULL) 
fore we 
if (EXT4_I(inode)->jinode) { 
if (ino < EXT4_FIRST_INO(sb) && ino != EXT4_ROOT_INO) 
if (ino > le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count)) 
if the inode is currently unallocated!! 
if the inode had been 
for parent directory, so 
if (IS_ERR(inode)) 
if (generation && inode->i_generation != generation) { 
if (!page_has_buffers(page)) 
if (journal) 
ifdef CONFIG_QUOTA 
format_id, 
format_id); 
format_id, 
if 
ifdef CONFIG_QUOTA 
if 
ifdef CONFIG_QUOTA 
if 
if (!options || strncmp(options, "sb=", 3) != 0) 
if (*options && *options != ',') { 
if (*options == ',') 
if you think we should keep it.\n"; 
if (sb_any_quota_loaded(sb) && 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_QUOTA)) { 
if (!qname) { 
for storing quotafile name"); 
if (sbi->s_qf_names[qtype]) { 
ified", 
if (strchr(qname, '/')) { 
if (sb_any_quota_loaded(sb) && 
if 
ifdef CONFIG_QUOTA 
if 
ifdef CONFIG_EXT4_FS_POSIX_ACL 
if 
ifdef CONFIG_QUOTA 
if (token == Opt_grpjquota) 
if (token == Opt_offusrjquota) 
if (token == Opt_offgrpjquota) 
if 
for (m = ext4_mount_opts; m->token != Opt_err; m++) 
if (m->token == Opt_err) { 
if ((m->flags & MOPT_NO_EXT2) && IS_EXT2_SB(sb)) { 
if ((m->flags & MOPT_NO_EXT3) && IS_EXT3_SB(sb)) { 
if (args->from && !(m->flags & MOPT_STRING) && match_int(args, &arg)) 
if (args->from && (m->flags & MOPT_GTE0) && (arg < 0)) 
if (m->flags & MOPT_EXPLICIT) 
if (m->flags & MOPT_CLEAR_ERR) 
if (token == Opt_noquota && sb_any_quota_loaded(sb)) { 
if (m->flags & MOPT_NOSUPPORT) { 
if (token == Opt_commit) { 
if (token == Opt_max_batch_time) { 
if (token == Opt_min_batch_time) { 
if (token == Opt_inode_readahead_blks) { 
if (token == Opt_init_itable) { 
if (!args->from) 
if (token == Opt_max_dir_size_kb) { 
if (token == Opt_stripe) { 
if (token == Opt_resuid) { 
if (!uid_valid(uid)) { 
if (token == Opt_resgid) { 
if (!gid_valid(gid)) { 
if (token == Opt_journal_dev) { 
ify journal on remount"); 
if (token == Opt_journal_path) { 
if (is_remount) { 
ify journal on remount"); 
if (!journal_path) { 
if (error) { 
if (!S_ISBLK(journal_inode->i_mode)) { 
if (token == Opt_journal_ioprio) { 
if (m->flags & MOPT_DATAJ) { 
if (!sbi->s_journal) 
if (test_opt(sb, DATA_FLAGS) != m->mount_opt) { 
ifdef CONFIG_QUOTA 
if (sb_any_quota_loaded(sb) && 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if 
if (!args->from) 
if (m->flags & MOPT_CLEAR) 
if (unlikely(!(m->flags & MOPT_SET))) { 
if (arg != 0) 
if (!options) 
while ((p = strsep(&options, ",")) != NULL) { 
if (handle_mount_opt(sb, p, token, args, journal_devnum, 
ifdef CONFIG_QUOTA 
if (sbi->s_qf_names[USRQUOTA] || sbi->s_qf_names[GRPQUOTA]) { 
if (test_opt(sb, GRPQUOTA) && sbi->s_qf_names[GRPQUOTA]) 
if (test_opt(sb, GRPQUOTA) || test_opt(sb, USRQUOTA)) { 
format mixing"); 
if (!sbi->s_jquota_fmt) { 
format " 
if (sbi->s_jquota_fmt) { 
format " 
if 
if (blocksize < PAGE_CACHE_SIZE) { 
if block size != PAGE_SIZE"); 
if defined(CONFIG_QUOTA) 
if (sbi->s_jquota_fmt) { 
if (sbi->s_qf_names[USRQUOTA]) 
if (sbi->s_qf_names[GRPQUOTA]) 
if 
for (t = tokens; t->token != Opt_err; t++) 
if 
if the per-sb default is different from the global default 
if (sbi->s_sb_block != 1) 
for (m = ext4_mount_opts; m->token != Opt_err; m++) { 
if (((m->flags & (MOPT_SET|MOPT_CLEAR)) == 0) || 
if (!(m->mount_opt & (sbi->s_mount_opt ^ def_mount_opt))) 
if ((want_set && 
if (nodefs || !uid_eq(sbi->s_resuid, make_kuid(&init_user_ns, EXT4_DEF_RESUID)) || 
if (nodefs || !gid_eq(sbi->s_resgid, make_kgid(&init_user_ns, EXT4_DEF_RESGID)) || 
if (test_opt(sb, ERRORS_RO) && def_errors != EXT4_ERRORS_RO) 
if (test_opt(sb, ERRORS_CONT) && def_errors != EXT4_ERRORS_CONTINUE) 
if (test_opt(sb, ERRORS_PANIC) && def_errors != EXT4_ERRORS_PANIC) 
if (nodefs || sbi->s_commit_interval != JBD2_DEFAULT_MAX_COMMIT_AGE*HZ) 
if (nodefs || sbi->s_min_batch_time != EXT4_DEF_MIN_BATCH_TIME) 
if (nodefs || sbi->s_max_batch_time != EXT4_DEF_MAX_BATCH_TIME) 
if (sb->s_flags & MS_I_VERSION) 
if (nodefs || sbi->s_stripe) 
if (EXT4_MOUNT_DATA_FLAGS & (sbi->s_mount_opt ^ def_mount_opt)) { 
if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA) 
if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_WRITEBACK_DATA) 
if (nodefs || 
if (nodefs || (test_opt(sb, INIT_INODE_TABLE) && 
if (nodefs || sbi->s_max_dir_size_kb) 
if (le32_to_cpu(es->s_rev_level) > EXT4_MAX_SUPP_REV) { 
forcing read-only mode"); 
if (read_only) 
if (!(sbi->s_mount_state & EXT4_VALID_FS)) 
if (sbi->s_mount_state & EXT4_ERROR_FS) 
if ((__s16) le16_to_cpu(es->s_max_mnt_count) > 0 && 
if (le32_to_cpu(es->s_checkinterval) && 
if (!sbi->s_journal) 
if (!(__s16) le16_to_cpu(es->s_max_mnt_count)) 
if (sbi->s_journal) 
if (test_opt(sb, DEBUG)) 
if (!sbi->s_log_groups_per_flex) 
if (size <= sbi->s_flex_groups_allocated) 
if (!new_groups) { 
for %d flex groups", 
if (sbi->s_flex_groups) { 
if (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) { 
if (err) 
for (i = 0; i < sbi->s_groups_count; i++) { 
if ((sbi->s_es->s_feature_ro_compat & 
for checksum of struct ext4_group_desc do the rest...*/ 
ify(struct super_block *sb, __u32 block_group, 
if (ext4_has_group_desc_csum(sb) && 
if (!ext4_has_group_desc_csum(sb)) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG)) 
for (i = 0; i < sbi->s_groups_count; i++) { 
if (i == sbi->s_groups_count - 1 || flexbg_flag) 
if ((grp == sbi->s_groups_count) && 
if (block_bitmap < first_block || block_bitmap > last_block) { 
for group %u not in group " 
if (inode_bitmap < first_block || inode_bitmap > last_block) { 
for group %u not in group " 
if (inode_table < first_block || 
for group %u not in group " 
if (!ext4_group_desc_csum_verify(sb, i, gdp)) { 
for group %u failed (%u!=%u)", 
if (!(sb->s_flags & MS_RDONLY)) { 
if (!flexbg_flag) 
if (NULL != first_not_zeroed) 
for us). 
if 
for us, so we can safely abort without any further action. 
ifdef CONFIG_QUOTA 
if 
if (bdev_read_only(sb->s_bdev)) { 
if feature set would not allow a r/w mount */ 
if (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) { 
if (es->s_last_orphan && !(s_flags & MS_RDONLY)) { 
if (s_flags & MS_RDONLY) { 
ifdef CONFIG_QUOTA 
for iput() to work correctly and not trash data */ 
for (i = 0; i < MAXQUOTAS; i++) { 
if (ret < 0) 
if 
while (es->s_last_orphan) { 
if (IS_ERR(inode)) { 
if (inode->i_nlink) { 
if (test_opt(sb, DEBUG)) 
if (nr_orphans) 
if (nr_truncates) 
ifdef CONFIG_QUOTA 
for (i = 0; i < MAXQUOTAS; i++) { 
if 
format file size. 
format containers, within a sector_t, and within i_blocks 
form 
format containers as 
for vfs i_blocks. 
if (!has_huge_files || sizeof(blkcnt_t) < sizeof(u64)) { 
if (res > upper_limit) 
for a dense, block 
if (!has_huge_files || sizeof(blkcnt_t) < sizeof(u64)) { 
if (res > upper_limit) 
if (res > MAX_LFS_FILESIZE) 
if (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_META_BG) || 
if (ext4_bg_has_super(sb, bg)) 
if (sb->s_blocksize == 1024 && nr == 0 && 
ified it via mount option, then 
if (sbi->s_stripe && sbi->s_stripe <= sbi->s_blocks_per_group) 
if (stripe_width <= sbi->s_blocks_per_group) 
if (stride <= sbi->s_blocks_per_group) 
if (ret <= 1) 
if (!ret && *value > max) 
if (!sb->s_bdev->bd_part) 
ifetime_write_kbytes_show(struct ext4_attr *a, 
if (!sb->s_bdev->bd_part) 
if (ret) 
if (t && (!is_power_of_2(t) || t > 0x40000000)) 
if (ret) 
if (parse_strtoull(buf, -1ULL, &val)) 
if (!capable(CAP_SYS_ADMIN)) 
if (len && buf[len-1] == '\n') 
if (len) 
ify(_name), .mode = _mode },	\ 
ify(_name), .mode = 0444 },	\ 
ifetime_write_kbytes); 
ifetime_write_kbytes), 
if this filesystem can be mounted as requested, 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, ~EXT4_FEATURE_INCOMPAT_SUPP)) { 
if (readonly) 
for a read-write mount */ 
if kernel is built with CONFIG_LBDAF 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_HUGE_FILE)) { 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_BIGALLOC) && 
ifndef CONFIG_QUOTA 
if  /* CONFIG_QUOTA */ 
if we have errors logged 
if (es->s_error_count) 
if (es->s_first_error_time) { 
if (es->s_first_error_ino) 
if (es->s_first_error_block) 
if (es->s_last_error_time) { 
if (es->s_last_error_ino) 
if (es->s_last_error_block) 
iffies + 24*60*60*HZ);  /* Once a day */ 
for (group = elr->lr_next_group; group < ngroups; group++) { 
if (!gdp) { 
if (!(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED))) 
if (group >= ngroups) 
if (!ret) { 
if (elr->lr_timeout == 0) { 
iffies + elr->lr_timeout; 
if (!elr) 
if (!ext4_li_info) { 
for next scheduled filesystem. 
while (true) { 
if (list_empty(&eli->li_request_list)) { 
for_each_safe(pos, n, &eli->li_request_list) { 
if (time_after_eq(jiffies, elr->lr_next_sched)) { 
if (time_before(elr->lr_next_sched, next_wakeup)) 
iffies; 
if (kthread_should_stop()) { 
if (!list_empty(&eli->li_request_list)) { 
for_each_safe(pos, n, &ext4_li_info->li_request_list) { 
if (IS_ERR(ext4_lazyinit_task)) { 
for (group = 0; group < ngroups; group++) { 
if (!gdp) 
if (!(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED))) 
if (!eli) 
if (!elr) 
iffies + (prandom_u32() % 
if (sbi->s_li_request != NULL) { 
if (first_not_zeroed == ngroups || 
if (!elr) { 
if (NULL == ext4_li_info) { 
if (ret) 
if (!(ext4_li_info->li_state & EXT4_LAZYINIT_RUNNING)) { 
if (ret) 
if (ret) 
if (!ext4_li_info || !ext4_lazyinit_task) 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (test_opt(sb, JOURNAL_ASYNC_COMMIT)) { 
if (test_opt(sb, JOURNAL_CHECKSUM)) { 
ifficult in the face of 
ifferent block group can end up in the same allocation cluster. 
for 
for very large cluster sizes --- but for newer 
for non-bigalloc file systems), we will use it. 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_BIGALLOC)) 
for (i = 0; i < ngroups; i++) { 
if (b >= first_block && b <= last_block) { 
if (b >= first_block && b <= last_block) { 
if (b >= first_block && b + sbi->s_itb_per_group <= last_block) 
for (j = 0; j < sbi->s_itb_per_group; j++, b++) { 
if (i != grp) 
if (ext4_bg_has_super(sb, grp)) { 
for (j = ext4_bg_num_gdb(sb, grp); j > 0; j--) { 
if (!count) 
if (!buf) 
for a given filesystem unless the number of block groups 
fore first_data_block are overhead 
for (i = 0; i < ngroups; i++) { 
if (blks) 
if (sbi->s_journal) 
if (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) 
ford to run 
if (count >= clusters) 
if (!sbi) 
if (!sbi->s_blockgroup_lock) { 
if (sb->s_bdev->bd_part) 
for (cp = sb->s_id; (cp = strchr(cp, '/'));) 
if (!blocksize) { 
for other than 1kB 
if (blocksize != EXT4_MIN_BLOCK_SIZE) { 
if (!(bh = sb_bread(sb, logical_sb_block))) { 
if (sb->s_magic != EXT4_SUPER_MAGIC) 
if metadata_csum and gdt_csum are both set. */ 
for a known checksum algorithm */ 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (IS_ERR(sbi->s_chksum_driver)) { 
if (!ext4_superblock_csum_verify(sb, es)) { 
for all metadata */ 
fore we parse the mount options */ 
if (def_mount_opts & EXT4_DEFM_DEBUG) 
if (def_mount_opts & EXT4_DEFM_BSDGROUPS) 
if (def_mount_opts & EXT4_DEFM_UID16) 
ifdef CONFIG_EXT4_FS_POSIX_ACL 
if 
if ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_ORDERED) 
if ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_WBACK) 
if (le16_to_cpu(sbi->s_es->s_errors) == EXT4_ERRORS_PANIC) 
if (le16_to_cpu(sbi->s_es->s_errors) == EXT4_ERRORS_CONTINUE) 
if (def_mount_opts & EXT4_DEFM_BLOCK_VALIDITY) 
if (def_mount_opts & EXT4_DEFM_DISCARD) 
if ((def_mount_opts & EXT4_DEFM_NOBARRIER) == 0) 
if (!IS_EXT3_SB(sb) && !IS_EXT2_SB(sb) && 
for lazyinit, for the case there is 
if (!parse_options((char *) sbi->s_es->s_mount_opts, sb, 
if (!parse_options((char *) data, sb, &journal_devnum, 
if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA) { 
if (test_opt2(sb, EXPLICIT_DELALLOC)) { 
if (test_opt(sb, DIOREAD_NOLOCK)) { 
if (test_opt(sb, DELALLOC)) 
if (le32_to_cpu(es->s_rev_level) == EXT4_GOOD_OLD_REV && 
if (es->s_creator_os == cpu_to_le32(EXT4_OS_HURD)) { 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, 
if (IS_EXT2_SB(sb)) { 
if (IS_EXT3_SB(sb)) { 
if (!ext4_feature_set_ok(sb, (sb->s_flags & MS_RDONLY))) 
if (blocksize < EXT4_MIN_BLOCK_SIZE || 
if (sb->s_blocksize != blocksize) { 
if (!sb_set_blocksize(sb, blocksize)) { 
if (!bh) { 
if (es->s_magic != cpu_to_le16(EXT4_SUPER_MAGIC)) { 
if (le32_to_cpu(es->s_rev_level) == EXT4_GOOD_OLD_REV) { 
if ((sbi->s_inode_size < EXT4_GOOD_OLD_INODE_SIZE) || 
if (sbi->s_inode_size > EXT4_GOOD_OLD_INODE_SIZE) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_64BIT)) { 
if (EXT4_INODE_SIZE(sb) == 0 || EXT4_INODES_PER_GROUP(sb) == 0) 
if (sbi->s_inodes_per_block == 0) 
for (i = 0; i < 4; i++) 
if (EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_DIR_INDEX)) { 
if (i & EXT2_FLAGS_UNSIGNED_HASH) 
if ((i & EXT2_FLAGS_SIGNED_HASH) == 0) { 
if (!(sb->s_flags & MS_RDONLY)) 
if (!(sb->s_flags & MS_RDONLY)) 
if 
if (has_bigalloc) { 
if (sbi->s_clusters_per_group > blocksize * 8) { 
if (sbi->s_blocks_per_group != 
if (clustersize != blocksize) { 
if (sbi->s_blocks_per_group > blocksize * 8) { 
if (sbi->s_inodes_per_group > blocksize * 8) { 
if (sbi->s_blocks_per_group == clustersize << 3) 
if (err) { 
if (sizeof(sector_t) < 8) 
if (EXT4_BLOCKS_PER_GROUP(sb) == 0) 
if (blocks_count && ext4_blocks_count(es) > blocks_count) { 
for the first data block to be beyond the end 
if (le32_to_cpu(es->s_first_data_block) >= ext4_blocks_count(es)) { 
if (blocks_count > ((uint64_t)1<<32) - EXT4_DESC_PER_BLOCK(sb)) { 
if (sbi->s_group_desc == NULL) { 
if (ext4_proc_root) 
if (sbi->s_proc) 
for (i = 0; i < db_count; i++) { 
if (!sbi->s_group_desc[i]) { 
for buddy allocator 
if (!test_opt(sb, NOLOAD) && 
if (err) { 
if (!ext4_check_descriptors(sb, &first_not_zeroed)) { 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG)) 
if (!err) { 
if (!err) { 
if (!err) { 
if (!err) { 
if (err) { 
ifdef CONFIG_QUOTA 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_QUOTA)) 
if 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_MMP) && 
if (ext4_multi_mount_protect(sb, le64_to_cpu(es->s_mmp_block))) 
ified in the journal! 
if (!test_opt(sb, NOLOAD) && 
if (ext4_load_journal(sb, es, journal_devnum)) 
if (test_opt(sb, NOLOAD) && !(sb->s_flags & MS_RDONLY) && 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_64BIT) && 
if (!set_journal_csum_feature_set(sb)) { 
if required, so we can 
if the journal can 
if (jbd2_journal_check_available_features 
if (!jbd2_journal_check_available_features 
if (ext4_mballoc_ready) { 
if (!sbi->s_mb_cache) { 
if present. 
if (es->s_overhead_clusters) 
if (err) 
if (!EXT4_SB(sb)->rsv_conversion_wq) { 
if (IS_ERR(root)) { 
if (!S_ISDIR(root->i_mode) || !root->i_blocks || !root->i_size) { 
if (!sb->s_root) { 
if (ext4_setup_super(sb, es, sb->s_flags & MS_RDONLY)) 
if present */ 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (sbi->s_want_extra_isize < 
if (sbi->s_want_extra_isize < 
if enough inode space is available */ 
if (err) { 
for " 
if (err) { 
if (err) 
if (err) 
ifdef CONFIG_QUOTA 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_QUOTA) && 
if (err) 
if  /* CONFIG_QUOTA */ 
if (needs_recovery) { 
if (EXT4_SB(sb)->s_journal) { 
if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA) 
if (test_opt(sb, DISCARD)) { 
if (!blk_queue_discard(q)) 
if (es->s_error_count) 
if (!silent) 
ifdef CONFIG_QUOTA 
if 
if (EXT4_SB(sb)->rsv_conversion_wq) 
if (sbi->s_journal) { 
if (sbi->s_flex_groups) 
if (sbi->s_mmp_tsk) 
for (i = 0; i < db_count; i++) 
if (sbi->s_chksum_driver) 
if (sbi->s_proc) { 
ifdef CONFIG_QUOTA 
for (i = 0; i < MAXQUOTAS; i++) 
if 
fore we've 
if (test_opt(sb, BARRIER)) 
if (test_opt(sb, DATA_ERR_ABORT)) 
for the existence of a valid inode on disk.  Bad 
if (IS_ERR(journal_inode)) { 
if (!journal_inode->i_nlink) { 
if (!S_ISREG(journal_inode->i_mode)) { 
if (!journal) { 
if (bdev == NULL) 
if (blocksize < hblock) { 
for journal device"); 
if (!(bh = __bread(bdev, sb_block, blocksize))) { 
if ((le16_to_cpu(es->s_magic) != EXT4_SUPER_MAGIC) || 
if (memcmp(EXT4_SB(sb)->s_es->s_journal_uuid, es->s_uuid, 16)) { 
if (!journal) { 
if (!buffer_uptodate(journal->j_sb_buffer)) { 
if (be32_to_cpu(journal->j_superblock->s_nr_users) != 1) { 
if (journal_devnum && 
forming recovery after a 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER)) { 
if (really_read_only) { 
if (journal_inum && journal_dev) { 
if (journal_inum) { 
if (!(journal = ext4_get_dev_journal(sb, journal_dev))) 
if (!(journal->j_flags & JBD2_BARRIER)) 
if (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER)) 
if (!err) { 
if (save) 
if (save) 
if (err) { 
if (!really_read_only && journal_devnum && 
if (!sbh || block_device_ejected(sb)) 
if (buffer_write_io_error(sbh)) { 
for the best. 
for people who are east of GMT and who make their clock 
force a full file system check. 
if (!(sb->s_flags & MS_RDONLY)) 
if (sb->s_bdev->bd_part) 
if (sync) { 
if (error) 
if (error) { 
while writing " 
if we are mounting (or 
if (!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL)) { 
if (jbd2_journal_flush(journal) < 0) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER) && 
ifetime, move that error to the 
for any error status which may have been recorded in the 
if (j_errno) { 
force_commit(struct super_block *sb) 
if (sb->s_flags & MS_RDONLY) 
force_commit(journal); 
if 
for us. 
if (wait && sbi->s_journal->j_flags & JBD2_BARRIER && 
if (jbd2_journal_start_commit(sbi->s_journal, &target)) { 
if (needs_barrier) { 
if (!ret) 
if (wait && test_opt(sb, BARRIER)) 
fore a (read-only) snapshot is created.  This 
ifications. 
if (sb->s_flags & MS_RDONLY) 
if we failed to flush 
if (error < 0) 
if (sb->s_flags & MS_RDONLY) 
fore the fs is unlocked. */ 
for ext4_remount's benefit 
ifdef CONFIG_QUOTA 
if 
ifdef CONFIG_QUOTA 
if 
ifdef CONFIG_QUOTA 
for (i = 0; i < MAXQUOTAS; i++) 
if (!old_opts.s_qf_names[i]) { 
for (j = 0; j < i; j++) 
if 
if (!parse_options(data, sb, NULL, &journal_ioprio, 1)) { 
if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA) { 
if (test_opt(sb, DIOREAD_NOLOCK)) { 
if (sbi->s_mount_flags & EXT4_MF_FS_ABORTED) 
forced by user"); 
if (sbi->s_journal) { 
if ((*flags & MS_RDONLY) != (sb->s_flags & MS_RDONLY)) { 
if (*flags & MS_RDONLY) { 
if (err < 0) 
if (err < 0) 
if we are remounting a valid rw partition 
if (!(es->s_state & cpu_to_le16(EXT4_VALID_FS)) && 
if (sbi->s_journal) 
if (!ext4_feature_set_ok(sb, 0)) { 
for (g = 0; g < sbi->s_groups_count; g++) { 
if (!ext4_group_desc_csum_verify(sb, g, gdp)) { 
for group %u failed (%u!=%u)", 
for now. 
if (es->s_last_orphan) { 
if (sbi->s_journal) 
if (!ext4_setup_super(sb, es, 0)) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, 
if (ext4_multi_mount_protect(sb, 
if ((sb->s_flags & MS_RDONLY) || !test_opt(sb, INIT_INODE_TABLE)) 
if (sbi->s_journal == NULL && !(old_sb_flags & MS_RDONLY)) 
ifdef CONFIG_QUOTA 
for (i = 0; i < MAXQUOTAS; i++) 
if (enable_quota) { 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (err) 
if 
ifdef CONFIG_QUOTA 
for (i = 0; i < MAXQUOTAS; i++) { 
if 
if (!test_opt(sb, MINIX_DF)) 
if (buf->f_bfree < (ext4_r_blocks_count(es) + resv_blocks)) 
for writing quotas on sync - we need to start transaction 
ifdef CONFIG_QUOTA 
if (IS_ERR(handle)) 
if (!ret) 
if (IS_ERR(handle)) 
if (!ret) 
if (IS_ERR(handle)) { 
if (!ret) 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_QUOTA) || 
if (IS_ERR(handle)) 
if (!ret) 
format_id, 
if (!test_opt(sb, QUOTA)) 
if (path->dentry->d_sb != sb) 
if (EXT4_SB(sb)->s_qf_names[type]) { 
if (path->dentry->d_parent != sb->s_root) 
if (EXT4_SB(sb)->s_journal && 
if (err) 
format_id, path); 
if (!qf_inums[type]) 
if (IS_ERR(qf_inode)) { 
for quota files to avoid recursion */ 
format_id, flags); 
for all quota types. */ 
for (type = 0; type < MAXQUOTAS; type++) { 
if (err) { 
format_id) 
format_id, DQUOT_LIMITS_ENABLED); 
if (test_opt(sb, DELALLOC)) 
if (!inode) 
ification times of quota files when userspace can 
if (IS_ERR(handle)) 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_QUOTA)) 
ford 
if (off > i_size) 
if (off+len > i_size) 
while (toread > 0) { 
if (err) 
if (!bh)	/* A hole? */ 
if (EXT4_SB(sb)->s_journal && !handle) { 
if (sb->s_blocksize - offset < len) { 
if (!bh) 
if (err) { 
if (err) 
if (inode->i_size < off + len) { 
if 
if !defined(CONFIG_EXT2_FS) && !defined(CONFIG_EXT2_FS_MODULE) && defined(CONFIG_EXT4_USE_FOR_EXT23) 
if (err) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, ~EXT2_FEATURE_INCOMPAT_SUPP)) 
if (sb->s_flags & MS_RDONLY) 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, ~EXT2_FEATURE_RO_COMPAT_SUPP)) 
if 
if (err) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, ~EXT3_FEATURE_INCOMPAT_SUPP)) 
if (!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL)) 
if (sb->s_flags & MS_RDONLY) 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, ~EXT3_FEATURE_RO_COMPAT_SUPP)) 
if 
if (!ef) 
if (ret) { 
for_completion(&ext4_feat->f_kobj_unregister); 
for flags consistency */ 
for (i = 0; i < EXT4_WQ_HASH_SZ; i++) { 
if (err) 
if (err) 
if (err) 
if (!ext4_kset) { 
if (err) 
if (err) 
if (err) 
if (err) 
if (ext4_proc_root) 
file : ./test/kernel/fs/ext4/mmp.c 
[ OK ] open : 4 ok... 
ify(struct super_block *sb, struct mmp_struct *mmp) 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (unlikely(!buffer_uptodate(bh))) 
if (*bh) 
if (!*bh) 
if (!*bh) 
if (*bh) { 
if (!buffer_uptodate(*bh)) { 
if (unlikely(!*bh)) { 
while reading MMP block %llu", 
if (le32_to_cpu(mmp->mmp_magic) != EXT4_MMP_MAGIC || 
formation as possible to help the admin. 
iff; 
if 
while (!kthread_should_stop()) { 
iffies; 
if (retval) { 
if (!(le32_to_cpu(es->s_feature_incompat) & 
if (sb->s_flags & MS_RDONLY) { 
iff = jiffies - last_update_time; 
iff); 
if the MMP block is as we left it. 
iff = jiffies - last_update_time; 
if (retval) { 
if (mmp->mmp_seq != mmp_check->mmp_seq || 
while updating MMP info. " 
for the MMP block to be written. 
iff / HZ, 
while (new_seq > EXT4_MMP_SEQ_MAX); 
if (mmp_block < le32_to_cpu(es->s_first_data_block) || 
if (retval) 
if (mmp_check_interval < EXT4_MMP_MIN_CHECK_INTERVAL) 
if (le16_to_cpu(mmp->mmp_check_interval) > mmp_check_interval) 
if (seq == EXT4_MMP_SEQ_CLEAN) 
if (seq == EXT4_MMP_SEQ_FSCK) { 
if more than 20 secs. */ 
if (schedule_timeout_interruptible(HZ * wait_time) != 0) { 
if (retval) 
if (seq != le32_to_cpu(mmp->mmp_seq)) { 
if (retval) 
for MMP interval and check mmp_seq. 
if (schedule_timeout_interruptible(HZ * wait_time) != 0) { 
if (retval) 
if (seq != le32_to_cpu(mmp->mmp_seq)) { 
if (!mmpd_data) { 
for mmpd_data"); 
if (IS_ERR(EXT4_SB(sb)->s_mmp_tsk)) { 
for %s.", 
file : ./test/kernel/fs/ext4/move_extent.c 
[ OK ] open : 4 ok... 
ify it 
for more details. 
for designated logical block number. 
for output) 
if (IS_ERR(path)) 
if (path[ext_depth(inode)].p_ext == NULL) 
for getting initialize status 
if (ext4_ext_is_unwritten(src)) 
for the next extent and set it to "extent" 
for the next extent 
if 
if (EXT_LAST_EXTENT(path[ppos].p_hdr) > path[ppos].p_ext) { 
while (--ppos >= 0) { 
if (path[ppos+1].p_bh) 
if (!path[ppos+1].p_bh) 
while (++cur_ppos < leaf_ppos) { 
if (path[cur_ppos+1].p_bh) 
if (!path[cur_ppos+1].p_bh) 
if (le16_to_cpu(eh->eh_entries) == 0) 
if (first < second) { 
if (start_ext->ee_len && new_ext->ee_len && end_ext->ee_len) { 
if (start_ext->ee_len && new_ext->ee_len && 
if (!start_ext->ee_len && new_ext->ee_len && 
if new_ext was 
if (new_ext->ee_block) 
if (new_flag) { 
if (err) 
if (ext4_ext_insert_extent(handle, orig_inode, 
if (end_flag) { 
if (err) 
if (ext4_ext_insert_extent(handle, orig_inode, 
if (orig_path) { 
if (range_to_move && o_end < EXT_LAST_EXTENT(eh)) { 
if (start_ext->ee_len) 
if (new_ext->ee_len) { 
if (end_ext->ee_len) 
if (depth) { 
if (ret) 
if (range_to_move > 0 && 
if (ret < 0) 
fore, this function creates structures to save extents of the leaf 
if (le32_to_cpu(oext->ee_block) < le32_to_cpu(new_ext.ee_block) && 
if (oext > EXT_FIRST_EXTENT(orig_path[depth].p_hdr)) { 
if these are contiguous and same extent type. 
if (ext4_can_extents_be_merged(orig_inode, prev_ext, 
if (le32_to_cpu(oext->ee_block) + oext_alen - 1 < new_ext_end) { 
if (le32_to_cpu(oext->ee_block) <= new_ext_end && 
for extent swapping. 
iff, orig_diff; 
if (orig_off < le32_to_cpu(tmp_oext->ee_block) || 
if (orig_off < le32_to_cpu(tmp_dext->ee_block) || 
iff = donor_off - le32_to_cpu(tmp_dext->ee_block); 
iff); 
if (max_count < ext4_ext_get_actual_len(tmp_dext)) 
iff = orig_off - le32_to_cpu(tmp_oext->ee_block); 
if donor extent is larger than orig */ 
iff) 
iff); 
if all extents in range has expected type, and zero otherwise. 
while (from < last) { 
if (*err) 
if (unwritten != ext4_ext_is_unwritten(ext)) 
if (path) { 
formation of original and donor inodes into 
formation of original inode to point at the 
formation of donor inode to point at the saved 
if (*err) 
if (*err) 
for the block "orig_off" */ 
if (*err) 
for the head */ 
if (*err) 
if (unlikely(!dext)) 
if (*err) 
for the donor extents */ 
while (1) { 
if (unlikely(!dext)) { 
for donor must be found"); 
if (donor_off != le32_to_cpu(tmp_dext.ee_block)) { 
if (*err) 
if (*err) 
if (replaced_count >= count) 
if (orig_path) 
if (*err) 
if (donor_path) 
if (*err) 
if (*err) 
if (orig_path) { 
if (donor_path) { 
for inode's by inode order 
if (inode1 < inode2) { 
if (!page[0]) 
if (!page[1]) { 
if 
if (inode1 > inode2) { 
if (PageUptodate(page)) 
if (!page_has_buffers(page)) 
for (bh = head, block_start = 0; bh != head || !block_start; 
if (block_end <= from || block_start >= to) { 
if (buffer_uptodate(bh)) 
if (!buffer_mapped(bh)) { 
if (err) { 
if (!buffer_mapped(bh)) { 
if (!nr) 
for (i = 0; i < nr; i++) { 
if (!bh_uptodate_or_lock(bh)) { 
if (err) 
if (!partial) 
ifferent metadata blocks. 
if (IS_ERR(handle)) { 
if (segment_eq(get_fs(), KERNEL_DS)) 
if ((orig_blk_offset + block_len_in_page - 1) == 
if (tmp_data_size == 0) 
if (unlikely(*err < 0)) 
while we 
if (unwritten) { 
if (*err) 
if (*err) 
if (!unwritten) { 
if ((page_has_private(pagep[0]) && 
if (*err) 
if ((page_has_private(pagep[0]) && !try_to_release_page(pagep[0], 0)) || 
if (*err) { 
form all necessary steps similar write_begin()/write_end() 
if (!*err) 
if (unlikely(*err < 0)) 
force transaction commit may help to free it. */ 
if (replaced_count != block_len_in_page) { 
for orig 
if (donor_inode->i_mode & (S_ISUID|S_ISGID)) { 
if (IS_IMMUTABLE(donor_inode) || IS_APPEND(donor_inode)) 
if (IS_SWAPFILE(orig_inode) || IS_SWAPFILE(donor_inode)) { 
if (!(ext4_test_inode_flag(orig_inode, EXT4_INODE_EXTENTS))) { 
if (!(ext4_test_inode_flag(donor_inode, EXT4_INODE_EXTENTS))) { 
if ((!orig_inode->i_size) || (!donor_inode->i_size)) { 
if (orig_start != donor_start) { 
if ((orig_start >= EXT_MAX_BLOCKS) || 
if (orig_inode->i_size > donor_inode->i_size) { 
ificial restriction */ 
ificial restriction */ 
if (orig_start >= orig_blocks) { 
if (orig_start + *len > orig_blocks) { 
if (!*len) { 
ified range of a file 
for orig 
if succeed, otherwise returns error value. 
ified as arguments. 
ified with the ext_cur (initial value is holecheck_path) re-cursive, 
ified as arguments. 
for the 
for the command to calculate the file offset 
if (orig_inode->i_sb != donor_inode->i_sb) { 
ifferent inodes */ 
if (!S_ISREG(orig_inode->i_mode) || !S_ISREG(donor_inode->i_mode)) { 
for inodes with full 
if (ext4_should_journal_data(orig_inode) || 
for all existing dio workers */ 
if (ret) 
if (file_end < block_end) 
if (ret) 
if (ret) 
if block_start was 
if (le32_to_cpu(ext_cur->ee_block) + 
if (last_extent < 0) { 
if (last_extent < 0) { 
if (le32_to_cpu(ext_cur->ee_block) > block_start) 
ified range. */ 
ified range of file " 
while (!last_extent && le32_to_cpu(ext_cur->ee_block) <= block_end) { 
if (seq_start + seq_blocks - 1 > block_end) 
if (last_extent < 0) { 
if extents are contiguous. 
if (ext4_can_extents_be_merged(orig_inode, 
if (data_offset_in_page + seq_blocks > blocks_per_page) { 
while (orig_page_offset <= seq_end_page) { 
if (ret < 0) 
if (*moved_len > len) { 
if (rest_blocks > blocks_per_page) 
if (ret < 0) 
if (holecheck_path) 
if (ret) 
if (orig_path) 
if (ret) 
if (*moved_len) { 
if (orig_path) { 
if (holecheck_path) { 
file : ./test/kernel/fs/ext4/page-io.c 
[ OK ] open : 4 ok... 
for ext4 
if (io_end_cachep == NULL) 
ific 
ifetime, due to LKML politics... 
for_each_segment_all(bvec, bio, i) { 
if (!page) 
if (error) { 
if (bh_offset(bh) < bio_start || 
if (buffer_async_write(bh)) 
if (error) 
while ((bh = bh->b_this_page) != head); 
if (!under_io) 
if (atomic_dec_and_test(&EXT4_I(io_end->inode)->i_ioend_count)) 
for (bio = io_end->bio; bio; bio = next_bio) { 
if (atomic_dec_and_test(&EXT4_I(inode)->i_unwritten)) 
for all DIO to finish (thus exclusion from 
fore all IOs overlapping that range are 
if (ret < 0) { 
ifdef	EXT4FS_DEBUG 
fore, *after; 
if (list_empty(head)) 
for_each_entry(io, head, list) { 
fore = cur->prev; 
if 
if (list_empty(&ei->i_rsv_conversion_list)) 
while (!list_empty(&unwritten)) { 
if (unlikely(!ret && err)) 
if (io) { 
if (atomic_dec_and_test(&io_end->count)) { 
if (atomic_dec_and_test(&io_end->count)) { 
for page writeback */ 
if (test_bit(BIO_UPTODATE, &bio->bi_flags)) 
if (error) { 
if (io_end->flag & EXT4_IO_END_UNWRITTEN) { 
if (bio) { 
if (!bio) 
if (io->io_bio && bh->b_blocknr != io->io_next_block) { 
if (io->io_bio == NULL) { 
if (ret) 
if (ret != bh->b_size) 
if (keep_towrite) 
if (len < PAGE_CACHE_SIZE) 
fore submitting so that 
if (block_start >= len) { 
if (!buffer_dirty(bh) || buffer_delay(bh) || 
if (!buffer_mapped(bh)) 
if (io->io_bio) 
if (buffer_new(bh)) { 
while ((bh = bh->b_this_page) != head); 
if (!buffer_async_write(bh)) 
if (ret) { 
for_writepage(wbc, page); 
while ((bh = bh->b_this_page) != head); 
if (ret) { 
while (bh != head); 
if (!nr_submitted) 
file : ./test/kernel/fs/ext4/xattr.c 
[ OK ] open : 4 ok... 
for symlinks and special files added per 
if an inode uses an additional block. All 
ifferent header; the entries 
format: 
ific order. 
if they are exclusive to an inode, so 
ifdef EXT4_XATTR_DEBUG 
while (0) 
while (0) 
if 
ifdef CONFIG_EXT4_FS_POSIX_ACL 
if 
ifdef CONFIG_EXT4_FS_SECURITY 
if 
ifdef CONFIG_EXT4_FS_POSIX_ACL 
if 
if 
ify(struct inode *inode, 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (name_index > 0 && name_index < ARRAY_SIZE(ext4_xattr_handler_map)) 
while (!IS_LAST_ENTRY(entry)) { 
if ((void *)next >= end) 
if (buffer_verified(bh)) 
if (BHDR(bh)->h_magic != cpu_to_le32(EXT4_XATTR_MAGIC) || 
if (!ext4_xattr_block_csum_verify(inode, bh->b_blocknr, BHDR(bh))) 
if (!error) 
if (entry->e_value_block != 0 || value_size > size || 
if (name == NULL) 
for (; !IS_LAST_ENTRY(entry); entry = EXT4_XATTR_NEXT(entry)) { 
if (!cmp) 
if (!cmp) 
if (cmp <= 0 && (sorted || cmp == 0)) 
if (!cmp && ext4_xattr_check_entry(entry, size)) 
if (!EXT4_I(inode)->i_file_acl) 
if (!bh) 
if (ext4_xattr_check_block(inode, bh)) { 
if (error == -EIO) 
if (error) 
if (buffer) { 
if (size > buffer_size) 
if (!ext4_test_inode_state(inode, EXT4_STATE_XATTR)) 
if (error) 
if (error) 
if (error) 
if (buffer) { 
if (size > buffer_size) 
if (strlen(name) > 255) 
if (error == -ENODATA) 
for (; !IS_LAST_ENTRY(entry); entry = EXT4_XATTR_NEXT(entry)) { 
if (handler) { 
if (buffer) { 
if (!EXT4_I(inode)->i_file_acl) 
if (!bh) 
if (ext4_xattr_check_block(inode, bh)) { 
if (!ext4_test_inode_state(inode, EXT4_STATE_XATTR)) 
if (error) 
if (error) 
if (ret < 0) 
if (buffer) { 
if (ret < 0) 
if (EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_EXT_ATTR)) 
if (ext4_journal_get_write_access(handle, EXT4_SB(sb)->s_sbh) == 0) { 
if (error) 
if (BHDR(bh)->h_refcount == cpu_to_le32(1)) { 
if (ce) 
if (ce) 
ifferent inodes can race and so we have to protect 
fore we are done dirtying the buffer. In 
for that case 
if (ext4_handle_valid(handle)) 
if (!ext4_handle_valid(handle)) 
if (IS_SYNC(inode)) 
for EAs. This also returns the total number of 
for (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) { 
if (offs < *min_offs) 
if (total) 
for (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) { 
if (offs < min_offs) 
if (!s->not_found) { 
if (i->value) { 
if (i->value && s->not_found) { 
if (!s->here->e_value_block && s->here->e_value_size) { 
if (i->value && size == EXT4_XATTR_SIZE(i->value_len)) { 
if (i->value == EXT4_ZERO_XATTR_VALUE) { 
while (!IS_LAST_ENTRY(last)) { 
if (!last->e_value_block && 
if (!i->value) { 
if (i->value) { 
if (i->value_len) { 
if (i->value == EXT4_ZERO_XATTR_VALUE) { 
if (EXT4_I(inode)->i_file_acl) { 
if (!bs->bh) 
if (ext4_xattr_check_block(inode, bs->bh)) { 
if (error && error != -ENODATA) 
if (i->value && i->value_len > sb->s_blocksize) 
if (s->base) { 
if (error) 
if (header(s->base)->h_refcount == cpu_to_le32(1)) { 
ifying in-place"); 
if (!error) { 
if (error == -EIO) 
if (!error) 
if (error) 
if (ce) { 
if (s->base == NULL) 
if (s->base == NULL) 
if (error == -EIO) 
if (error) 
if (!IS_LAST_ENTRY(s->first)) 
if (!IS_LAST_ENTRY(s->first)) { 
if (new_bh) { 
if (new_bh == bs->bh) 
if (error) 
if (error) 
if (error) 
if (bs->bh && s->base == bs->bh->b_data) { 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) 
if (error) 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) 
if (unlikely(!new_bh)) { 
if (error) { 
if (error) 
if (bs->bh && bs->bh != new_bh) 
if (ce) 
if (!(bs->bh && s->base == bs->bh->b_data)) 
if (EXT4_I(inode)->i_extra_isize == 0) 
if (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) { 
if (error) 
if (error && error != -ENODATA) 
if (EXT4_I(inode)->i_extra_isize == 0) 
if (error) { 
if (error) 
if (error) 
if (error) 
if (!IS_LAST_ENTRY(s->first)) { 
if (EXT4_I(inode)->i_extra_isize == 0) 
if (error) 
if (!IS_LAST_ENTRY(s->first)) { 
for this inode.  Value 
ify that an extended attribute must exist and must not exist 
if (!name) 
if (strlen(name) > 255) 
if (error) 
if (ext4_test_inode_state(inode, EXT4_STATE_NEW)) { 
if (error) 
if (is.s.not_found) 
if (error) 
if (is.s.not_found && bs.s.not_found) { 
if (flags & XATTR_REPLACE) 
if (!value) 
if (flags & XATTR_CREATE) 
if (!value) { 
if (!bs.s.not_found) 
if (!error && !bs.s.not_found) { 
if (error == -ENOSPC) { 
if (error) 
if (error) 
if (!is.s.not_found) { 
if (!error) { 
if (!value) 
if (IS_SYNC(inode)) 
if (no_expand == 0) 
ification is a filesystem transaction by itself. 
if (IS_ERR(handle)) { 
if (error == -ENOSPC && 
if (error == 0) 
ift the EA entries in the inode to create space for the increased 
ift_entries(struct ext4_xattr_entry *entry, 
for (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) { 
ift; 
ift the entries by n bytes */ 
if (EXT4_I(inode)->i_extra_isize >= new_extra_isize) { 
if enough free space is available in the inode to shift the 
if (free >= new_extra_isize) { 
ift_entries(entry,	EXT4_I(inode)->i_extra_isize 
if 
if (EXT4_I(inode)->i_file_acl) { 
if (!bh) 
if (ext4_xattr_check_block(inode, bh)) { 
if (free < new_extra_isize) { 
while (new_extra_isize > 0) { 
ift_bytes; /* No. of bytes to shift EAs by? */ 
if (!is || !bs) { 
for (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) { 
if (total_size <= free && total_size < min_total_size) { 
if (entry == NULL) { 
if (!tried_min_extra_isize && 
if (!buffer || !b_entry_name) { 
if (error) 
if (error) 
if (error) 
if (entry_size + EXT4_XATTR_SIZE(size) >= new_extra_isize) 
ift_bytes = entry_size + size; 
ift_entries(entry, EXT4_I(inode)->i_extra_isize - 
ift_bytes, 
ift_bytes; 
if (error) 
if (error) 
if (is) 
fore an inode is freed. We have exclusive 
if (!EXT4_I(inode)->i_file_acl) 
if (!bh) { 
if (BHDR(bh)->h_magic != cpu_to_le32(EXT4_XATTR_MAGIC) || 
if (!ce) { 
if (error) { 
if (error == -EBUSY) { 
for equality. 
while (!IS_LAST_ENTRY(entry1)) { 
if (entry1->e_hash != entry2->e_hash || 
if (entry1->e_value_block != 0 || entry2->e_value_block != 0) 
if (memcmp((char *)header1 + le16_to_cpu(entry1->e_value_offs), 
if (!IS_LAST_ENTRY(entry2)) 
if such a block was 
if (!header->h_hash) 
for cached blocks [%x]", (int)hash); 
while (ce) { 
if (IS_ERR(ce)) { 
if (!bh) { 
if (le32_to_cpu(BHDR(bh)->h_refcount) >= 
if (ext4_xattr_cmp(header, BHDR(bh)) == 0) { 
for (n = 0; n < entry->e_name_len; n++) { 
if (entry->e_value_block == 0 && entry->e_value_size != 0) { 
for (n = (le32_to_cpu(entry->e_value_size) + 
while (!IS_LAST_ENTRY(here)) { 
if an entry's hash value == 0 */ 
if (cache) 
file : ./test/kernel/fs/afs/cell.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (namelen > AFS_MAXCELLNAME) { 
if (!cell) { 
if the ip address is invalid, try dns query */ 
if (ret < 0) { 
for user-space reply */ 
if (next) 
if (sscanf(_vllist, "%u.%u.%u.%u", &a, &b, &c, &d) != 4) 
if (a > 255 || b > 255 || c > 255 || d > 255) 
while (cell->vl_naddrs < AFS_CELL_MAX_ADDRS && (_vllist = next)); 
while (*cp++); 
if (IS_ERR(key)) { 
format. 
for_each_entry(cell, &afs_cells, link) { 
if (IS_ERR(cell)) { 
for this cell */ 
if (ret < 0) 
ifdef CONFIG_AFS_FSCACHE 
for caching (this never returns an error) */ 
if 
if (retref && !IS_ERR(cell)) 
if (retref) { 
formation 
if (!rootcell) { 
if (!cp) 
for the root cell */ 
if (IS_ERR(new_root)) { 
if (name) { 
for it in the cell record list */ 
if (strncmp(cell->name, name, namesz) == 0) { 
if (dns_cell) 
if (!cell) { 
for other reasons. 
if 0 
if (cell && !list_empty(&cell->link)) 
if  /*  0  */ 
if (!cell) 
if (likely(!atomic_dec_and_test(&cell->usage))) { 
for everyone to stop using the cell */ 
for cell %s", cell->name); 
while (atomic_read(&cell->usage) > 0) { 
ifdef CONFIG_AFS_FSCACHE 
if 
fore calling this 
while (!list_empty(&afs_cells)) { 
if (!list_empty(&afs_cells)) { 
if (cell) { 
file : ./test/kernel/fs/afs/vlocation.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
for (count = cell->vl_naddrs; count > 0; count--) { 
if (ret == -ENOMEM || ret == -ENONET) 
for (count = cell->vl_naddrs; count > 0; count--) { 
if (ret == -ENOMEM || ret == -ENONET) 
if (vl->upd_busy_cnt <= 3) { 
if (ret < 0 && vl->upd_rej_cnt > 0) { 
if (vl) { 
if we found it in the cache 
if (vl->vldb.vidmask & AFS_VOL_VTM_RW) { 
if (vl->vldb.vidmask & AFS_VOL_VTM_RO) { 
if (vl->vldb.vidmask & AFS_VOL_VTM_BAK) { 
if (strcmp(vldb->name, vl->vldb.name) != 0) 
ifdef CONFIG_AFS_FSCACHE 
if 
if we have an in-cache copy (will set vl->valid if there is) */ 
if 
if (ret < 0) { 
for updates 
for_updates(struct afs_vlocation *vl) 
fore updating... */ 
if (!list_empty(&afs_vlocation_updates)) { 
if (vl->update_at <= xvl->update_at) 
if not able to find on the VL server 
if (namesz >= sizeof(vl->vldb.name)) { 
if we have an in-memory copy first */ 
for_each_entry(vl, &cell->vl_list, link) { 
if (memcmp(vl->vldb.name, name, namesz) == 0) 
if (!vl) { 
if (ret < 0) 
ifdef CONFIG_AFS_FSCACHE 
if 
for regular updates */ 
if (!list_empty(&vl->grave)) { 
if it was an abandoned record that we might try filling in */ 
while (vl->state != AFS_VL_VALID) { 
if (state == AFS_VL_NEW || state == AFS_VL_NO_VOLUME) { 
for creation or update by someone else to 
if (ret < 0) 
if (!vl) 
if (likely(!atomic_dec_and_test(&vl->usage))) { 
if (atomic_read(&vl->usage) == 0) { 
if (!list_empty(&vl->update)) { 
ifdef CONFIG_AFS_FSCACHE 
if 
while (!list_empty(&afs_vlocation_graveyard)) { 
if (expiry > now) { 
if (atomic_read(&vl->usage) > 0) { 
while (!list_empty(&corpses)) { 
for rmmod 
for (;;) { 
if (atomic_read(&vl->usage) > 0) 
if (timeout > 0) { 
form the update */ 
if (!list_empty(&afs_vlocation_updates)) { 
if (vl->update_at <= xvl->update_at) 
if (timeout < 0) 
file : ./test/kernel/fs/afs/file.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (IS_ERR(key)) { 
if (ret < 0) { 
ifdef CONFIG_AFS_FSCACHE 
if the read completes with an error, we just unlock the page and let 
if (!error) 
if 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) 
ifdef CONFIG_AFS_FSCACHE 
if 
if (ret < 0) { 
ifdef CONFIG_AFS_FSCACHE 
if 
ifdef CONFIG_AFS_FSCACHE 
if 
if (file) { 
if (IS_ERR(key)) { 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) { 
ifdef CONFIG_AFS_FSCACHE 
if 
if offset is 0 (indicating 
if the entire page is being invalidated */ 
ifdef CONFIG_AFS_FSCACHE 
if 
if (wb && !PageWriteback(page)) { 
if (!page_private(page)) 
if it's not busy 
if page is being written to the cache and the caller hasn't 
ifdef CONFIG_AFS_FSCACHE 
if 
if (wb) { 
file : ./test/kernel/fs/afs/dir.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if any char of the name (inc 
if 0 
if (qty == 0) 
if (page->index == 0 && qty != ntohs(dbuf->blocks[0].pagehdr.npages)) { 
if 
if (latter >= PAGE_SIZE) 
for (tmp = 0; tmp < qty; tmp++) { 
if (!IS_ERR(page)) { 
if (!PageChecked(page)) 
if (PageError(page)) 
if (test_bit(AFS_VNODE_DELETED, &AFS_FS_I(inode)->flags)) 
for (offset = AFS_DIRENT_PER_BLOCK - block->pagehdr.nentries; 
if (!(block->pagehdr.bitmap[offset / 8] & 
if (offset >= curr) 
for (tmp = nlen; tmp > 15; tmp -= sizeof(union afs_dirent)) { 
if (!(block->pagehdr.bitmap[next / 8] & 
if starts before the current position */ 
if (!dir_emit(ctx, dire->u.name, nlen, 
if (test_bit(AFS_VNODE_DELETED, &AFS_FS_I(dir)->flags)) { 
while (ctx->pos < dir->i_size) { 
if (IS_ERR(page)) { 
if (ret != 1) { 
while (ctx->pos < dir->i_size && blkoff < limit); 
for a name 
ifier through dtype 
if (cookie->name.len != nlen || 
if found 
if (ret < 0) { 
if (!cookie.found) { 
if the autocell 
if (ret != -ENOENT || 
if (IS_ERR(inode)) { 
if (dentry->d_name.len >= AFSNAMEMAX) { 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) { 
if (IS_ERR(key)) { 
if (ret < 0) { 
if (ret < 0) { 
if (!IS_ERR(inode)) { 
if (ret == -ENOENT) { 
if (IS_ERR(inode)) { 
if (flags & LOOKUP_RCU) 
if (dentry->d_inode) 
if (IS_ERR(key)) 
if (test_bit(AFS_VNODE_MODIFIED, &dir->flags)) 
if (test_bit(AFS_VNODE_DELETED, &dir->flags)) { 
if (dentry->d_fsdata == dir_version) 
ified"); 
for this vnode */ 
if (!dentry->d_inode) 
if (is_bad_inode(dentry->d_inode)) { 
if the vnode ID has changed, then the dirent points to a 
if (fid.vnode != vnode->fid.vnode) { 
if the vnode ID uniqifier has changed, then the file has 
if (fid.unique != vnode->fid.unique) { 
if (dentry->d_inode) 
if it exists, now points to a different vnode */ 
if we have submounts */ 
if (dentry->d_flags & DCACHE_NFSFS_RENAMED) 
if (dentry->d_inode && 
if (IS_ERR(key)) { 
if (ret < 0) 
if (IS_ERR(inode)) { 
for the new vnode */ 
if (d_unhashed(dentry)) { 
if (IS_ERR(key)) { 
if (ret < 0) 
if (dentry->d_inode) { 
if (dentry->d_name.len >= AFSNAMEMAX) 
if (IS_ERR(key)) { 
if (dentry->d_inode) { 
if (ret < 0) 
if (ret < 0) 
if (dentry->d_inode) { 
if 
fore it returns to us, and if it was deleted, 
if we didn't have a callback promise outstanding, 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) 
if (test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) 
if (IS_ERR(key)) { 
if (ret < 0) 
if (IS_ERR(inode)) { 
for the new vnode */ 
if (d_unhashed(dentry)) { 
if (IS_ERR(key)) { 
if (ret < 0) 
if (strlen(content) >= AFSPATHMAX) 
if (IS_ERR(key)) { 
if (ret < 0) 
if (IS_ERR(inode)) { 
for the new vnode */ 
if (d_unhashed(dentry)) { 
if (IS_ERR(key)) { 
if (ret < 0) 
file : ./test/kernel/fs/afs/server.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
while (*pp) { 
if (server->addr.s_addr < xserver->addr.s_addr) 
if (server->addr.s_addr > xserver->addr.s_addr) 
if (server) { 
for a cell 
if we already have the server */ 
for_each_entry(server, &cell->servers, link) { 
if (!candidate) { 
for_each_entry(server, &cell->servers, link) { 
if (afs_install_server(server) < 0) 
if (!list_empty(&server->grave)) { 
while (p) { 
if (addr.s_addr < server->addr.s_addr) { 
if (addr.s_addr > server->addr.s_addr) { 
if (!server) 
if (likely(!atomic_dec_and_test(&server->usage))) { 
if (atomic_read(&server->usage) == 0) { 
while (!list_empty(&afs_server_graveyard)) { 
if (expiry > now) { 
if (atomic_read(&server->usage) > 0) { 
while (!list_empty(&corpses)) { 
for rmmod 
file : ./test/kernel/fs/afs/inode.c 
[ OK ] open : 4 ok... 
if not, write to the Free Software 
ifdef CONFIG_AFS_FSCACHE 
if 
if (vnode->status.type == AFS_FTYPE_SYMLINK) { 
if (test_bit(AFS_VNODE_MOUNTPOINT, &vnode->flags)) { 
for inode created by autocell operations 
for autocell 
if (!inode) { 
if (!inode) { 
if (!(inode->i_state & I_NEW)) { 
if (!status) { 
if (ret < 0) 
if (!cb) { 
fore mapping the status, as map-status reads the 
ifdef CONFIG_AFS_FSCACHE 
if 
if (ret < 0) 
ifdef CONFIG_AFS_FSCACHE 
if 
if (S_ISREG(vnode->vfs_inode.i_mode)) 
if (vnode->cb_promised && 
if (vnode->cb_expires < get_seconds() + 10) { 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) 
if the promise has expired, we need to check the server again to get 
ifferent and we may no longer have 
if (!vnode->cb_promised || 
if (ret < 0) 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) { 
if the vnode's data version number changed then its contents are 
if (test_and_clear_bit(AFS_VNODE_ZAP_DATA, &vnode->flags)) 
if (test_bit(AFS_VNODE_PSEUDODIR, &AFS_FS_I(inode)->flags)) 
if (vnode->server) { 
ifdef CONFIG_AFS_FSCACHE 
if 
if (permits) 
if (!(attr->ia_valid & (ATTR_SIZE | ATTR_MODE | ATTR_UID | ATTR_GID | 
if (S_ISREG(vnode->vfs_inode.i_mode)) { 
if (attr->ia_valid & ATTR_FILE) { 
if (IS_ERR(key)) { 
if (!(attr->ia_valid & ATTR_FILE)) 
file : ./test/kernel/fs/afs/netdevices.c 
[ OK ] open : 4 ok... 
if_arp.h> 
if (dev) { 
for_each_netdev(&init_net, dev) { 
if (!idev) 
ifa(idev) { 
ifa->ifa_mask; 
if (n >= maxbufs) 
ifa(idev); 
file : ./test/kernel/fs/afs/cache.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
for the index entry 
if (klen > bufmax) 
for the index entry 
if (klen > bufmax) 
if (dlen > bufmax) 
if (dlen != buflen) 
if what's on disk is more valid than what's in memory, then use the 
if (!vlocation->valid || vlocation->vldb.rtime == cvldb->rtime) { 
if the cached info differs */ 
if the volume IDs for this name differ */ 
for the volume index entry 
if (klen > bufmax) 
for the index entry 
if (klen > bufmax) 
if (dlen > bufmax) 
if (dlen != buflen) { 
if (memcmp(buffer, 
if (memcmp(buffer + sizeof(vnode->fid.unique), 
for any object that may have data 
for (;;) { 
if (!nr_pages) 
for (loop = 0; loop < nr_pages; loop++) 
file : ./test/kernel/fs/afs/security.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (IS_ERR(key)) { 
for (loop = permits->count - 1; loop >= 0; loop--) 
ified inode itself if it's a 
if (S_ISDIR(vnode->vfs_inode.i_mode)) { 
if (IS_ERR(auth_inode)) 
if (permits) 
for a vnode to its or its parent directory's cache 
if (IS_ERR(auth_vnode)) { 
for the 
if (memcmp(&auth_vnode->fid, &vnode->status.parent, 
for the status */ 
if (key == vnode->volume->cell->anonymous_key) 
if (xpermits) { 
if it is then we just amend the list 
for (loop = count; loop > 0; loop--) { 
if (!permits) 
if (xpermits) 
if (xpermits) 
if the directory or parent directory is 
if (IS_ERR(auth_vnode)) { 
if we've got one yet */ 
if (permits) { 
for (loop = permits->count; loop > 0; loop--) { 
if (!valid) { 
if (ret < 0) { 
if (mask & MAY_NOT_BLOCK) 
if (IS_ERR(key)) { 
if the promise has expired, we need to check the server again */ 
if (ret < 0) 
if we've got one yet */ 
if (ret < 0) 
if (S_ISDIR(inode->i_mode)) { 
if (!(access & AFS_ACE_LOOKUP)) 
if (mask & MAY_READ) { 
if (mask & MAY_WRITE) { 
if (!(access & AFS_ACE_LOOKUP)) 
if (mask & (MAY_EXEC | MAY_READ)) { 
if (mask & MAY_WRITE) { 
file : ./test/kernel/fs/afs/rxrpc.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
for_call_to_complete(struct afs_call *); 
for_call_to_complete(struct afs_call *); 
for_call_to_complete, 
for_call_to_complete, 
ifications 
if (!afs_async_calls) { 
if (ret < 0) { 
if (ret < 0) { 
if (!skb) { 
if (atomic_dec_return(&afs_outstanding_skbs) == -1) 
if (!skb) { 
if (atomic_dec_return(&afs_outstanding_skbs) == -1) 
if (atomic_dec_return(&afs_outstanding_calls) == -1) 
if (call->rxcall) { 
if (call->type->destructor) 
if (!call) 
if (request_size) { 
if (!call->request) 
if (reply_size) { 
if (!call->buffer) 
if (count > ARRAY_SIZE(pages)) 
if (first + loop >= last) 
fore* sending the last 
if (first + loop >= last) 
if (ret < 0) 
while (++loop < count); 
for (loop = 0; loop < count; loop++) 
if (ret < 0) 
while (first <= last); 
if (IS_ERR(rxcall)) { 
fore* sending the last packet as RxRPC 
if (!call->send_pages) 
if (ret < 0) 
if (call->send_pages) { 
if (ret < 0) 
while ((skb = skb_dequeue(&call->rx_queue))) 
if (!call) { 
for our callback service */ 
while ((call->state == AFS_CALL_AWAIT_REPLY || 
if (last && 
if (call->state != AFS_CALL_AWAIT_REPLY) 
if the call is done with (we might have 
if (call->state >= AFS_CALL_COMPLETE) { 
while ((skb = skb_dequeue(&call->rx_queue))) 
if (call->incoming) 
for a call to complete 
for_call_to_complete(struct afs_call *call) 
for (;;) { 
if (!skb_queue_empty(&call->rx_queue)) { 
if (call->state >= AFS_CALL_COMPLETE) 
if (signal_pending(current)) 
if (call->state < AFS_CALL_COMPLETE) { 
while ((skb = skb_dequeue(&call->rx_queue))) 
for_call_to_complete(struct afs_call *call) 
form processing on an asynchronous call 
if (!skb_queue_empty(&call->rx_queue)) 
if (call->state >= AFS_CALL_COMPLETE && call->wait_mode) { 
if (skb_copy_bits(skb, 0, call->buffer + call->reply_size, len) < 0) 
while ((skb = skb_dequeue(&afs_incoming_calls))) { 
ification */ 
if (!call) { 
if (!call) { 
if (!IS_ERR(rxcall)) { 
if (call) 
forms the first four bytes of the request data */ 
if (skb_copy_bits(skb, 0, oibuf + call->offset, len) < 0) 
if (!pskb_pull(skb, len)) 
if (call->offset < 4) { 
if successful) */ 
for the remainer of this message off to the 
if (n >= 0) { 
if (n == -ENOMEM) { 
if (skb_copy_bits(skb, 0, buf + call->offset, len) < 0 || 
if (call->offset < count) { 
file : ./test/kernel/fs/afs/volume.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
forces access of that type or nothing 
if not available 
if (IS_ERR(vlocation)) { 
if (params->force && !(vlocation->vldb.vidmask & (1 << params->type))) 
for (loop = 0; loop < vlocation->vldb.nservers; loop++) 
if (params->force) { 
if (srvtmask & AFS_VOL_VTM_RO) { 
if (srvtmask & AFS_VOL_VTM_RW) { 
if (vlocation->vols[params->type]) { 
if (!volume) 
force	= params->force; 
if (ret) 
for (loop = 0; loop < 8; loop++) { 
if (IS_ERR(server)) { 
ifdef CONFIG_AFS_FSCACHE 
if 
for (loop = volume->nservers - 1; loop >= 0; loop--) 
if (!volume) 
if (likely(!atomic_dec_and_test(&volume->usage))) { 
ifdef CONFIG_AFS_FSCACHE 
if 
for (loop = volume->nservers - 1; loop >= 0; loop--) 
if we can */ 
if (volume->nservers == 0) { 
for the first live server and use 
for (loop = 0; loop < volume->nservers; loop++) { 
if (ret == 0) 
if (ret == 0 || 
if (ret == 0 || 
if (ret == 0 || 
if okay or to issue error 
if = jiffies; 
if = jiffies; 
if it 
for (loop = 0; loop < volume->nservers; loop++) 
if (volume->nservers > 0) 
for volume information 
if (!server->fs_state) { 
if = jiffies; 
file : ./test/kernel/fs/afs/cmservice.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if 0 
if  /*  0  */ 
if supported, F if not 
if (call->unmarshall == 6) { 
if the cache manager is still alive 
fore* attempting to spam the AFS server 
if (call->count > AFSCBMAX) 
if (!call->buffer) 
if (!call->request) 
for (loop = call->count; loop > 0; loop--, cb++) { 
if (tmp != call->count && tmp != 0) 
if (tmp == 0) 
for (loop = call->count; loop > 0; loop--, cb++) { 
if (skb->len != 0) 
if the final ACK isn't received. 
if (!last) 
if (!server) 
if (skb->len > 0) 
if (!last) 
if (!server) 
if (!last) 
if (!server) 
if the cache manager is still alive 
if (skb->len > 0) 
if (!last) 
if the fileserver has been rebooted 
if (memcmp(r, &afs_uuid, sizeof(afs_uuid)) == 0) 
if (skb->len > 0) 
if (!last) 
if (!call->buffer) 
if (!call->request) 
for (loop = 0; loop < 6; loop++) 
if (skb->len != 0) 
if (!last) 
ifs; 
ifs; 
ifs; 
ifaddr[32]; 
ifs = 0; 
if (ifs) { 
if (nifs < 0) { 
ifs = NULL; 
ifs = htonl(nifs); 
for (loop = 0; loop < 6; loop++) 
if (ifs) { 
for (loop = 0; loop < nifs; loop++) { 
ifs[loop].netmask.s_addr; 
ifs); 
if (skb->len > 0) 
if (!last) 
file : ./test/kernel/fs/afs/main.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (ret < 0) 
if (ret < 0) 
if (!afs_wq) 
if (ret < 0) 
ifdef CONFIG_AFS_FSCACHE 
if (ret < 0) 
if 
if (ret < 0) 
if (ret < 0) 
if (ret < 0) 
if (ret < 0) 
if (ret < 0) 
ifdef CONFIG_AFS_FSCACHE 
if 
ifdef CONFIG_AFS_FSCACHE 
if 
file : ./test/kernel/fs/afs/proc.c 
[ OK ] open : 4 ok... 
for AFS 
ify it under the terms of the GNU General Public License 
if (!proc_afs) 
if (!proc_create("cells", 0644, proc_afs, &afs_proc_cells_fops) || 
if (ret < 0) 
ification */ 
if (v == &afs_proc_cells) { 
if (size <= 1 || size >= PAGE_SIZE) 
if (!kbuf) 
if (copy_from_user(kbuf, buf, size) != 0) 
if (name) 
if (!name) 
while(*name == ' '); 
if (!args) 
while(*args == ' '); 
form */ 
if (strcmp(kbuf, "add") == 0) { 
if (IS_ERR(cell)) { 
if (size <= 1 || size >= PAGE_SIZE) 
if (!kbuf) 
if (copy_from_user(kbuf, buf, size) != 0) 
if (s) 
form */ 
if (ret >= 0) 
if (!dir) 
if (!proc_create_data("servers", 0, dir, 
if (!cell) 
if (ret < 0) 
ification */ 
if (v == &cell->vl_list) { 
if (!cell) 
if (ret<0) 
ification */ 
for the header line */ 
if (pos >= cell->vl_naddrs) 
if (pos >= cell->vl_naddrs) 
if (v == (struct in_addr *) 1) { 
if (!cell) 
if (ret < 0) 
ification */ 
if (v == &cell->servers) { 
file : ./test/kernel/fs/afs/callback.c 
[ OK ] open : 4 ok... 
if not, write to the Free Software 
if 0 
if  /*  0  */ 
while (!RB_EMPTY_ROOT(&server->cb_promises)) { 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) 
if no-one else has dealt with it yet */ 
if (test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) { 
if (afs_vnode_fetch_status(vnode, NULL, NULL) < 0) 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) 
if the vnode's data version number changed then its contents 
if (test_and_clear_bit(AFS_VNODE_ZAP_DATA, &vnode->flags)) 
if (test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) { 
if (vnode->cb_promised) { 
if (vnode->cb_promised) { 
if (list_empty(&vnode->granted_locks) && 
while (p) { 
if (fid->vid < vnode->fid.vid) 
if (fid->vid > vnode->fid.vid) 
if (fid->vnode < vnode->fid.vnode) 
if (fid->vnode > vnode->fid.vnode) 
if (fid->unique < vnode->fid.unique) 
if (fid->unique > vnode->fid.unique) 
if (!igrab(AFS_VNODE_TO_I(vnode))) 
for (; count > 0; callbacks++, count--) { 
for breaking 
if (!vnode->cb_promised) { 
if (vnode->cb_promised) { 
for a vnode on the file server when the 
if (!vnode->cb_promised) { 
if (vnode->cb_promised && afs_breakring_space(server) == 0) { 
for (;;) { 
if (!vnode->cb_promised || 
for the server to break this vnode's 
if (vnode->cb_promised) 
forget that we 
if 0 
for (;;) { 
if (atomic_read(&vnode->usage) > 0) 
if (timeout > 0) { 
form the update */ 
if (!list_empty(&server->cb_promises)) { 
if (vnode->update_at <= xvnode->update_at) 
if (timeout < 0) 
if 
file : ./test/kernel/fs/afs/vnode.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if 0 
if (!node) 
if (node->rb_left) 
if (rb_parent(node) != parent) { 
if (node->rb_right) 
if (dump_tree_aux(server->cb_promises.rb_node, NULL, 0, '-')) 
if 
if (old_server) { 
while (*p) { 
if (vnode->fid.vid < xvnode->fid.vid) 
if (vnode->fid.vid > xvnode->fid.vid) 
if (vnode->fid.vnode < xvnode->fid.vnode) 
if (vnode->fid.vnode > xvnode->fid.vnode) 
if (vnode->fid.unique < xvnode->fid.unique) 
if (vnode->fid.unique > xvnode->fid.unique) 
if (vnode->cb_promised) { 
if (vnode->cb_promised) { 
if (vnode->server != server) 
while (*p) { 
if (vnode->cb_expires_at < xvnode->cb_expires_at) 
if (server) { 
if (vnode->cb_promised) { 
if (ret == -ENOENT) { 
if: 
if (!test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags) && 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) { 
if (auth_vnode) 
if (!test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags) && 
if (vnode->update_cnt > 0) { 
for the status to be updated */ 
if (!test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) 
if it got updated and invalidated all 
fore we saw it */ 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (auth_vnode) 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(dvnode, server, ret)); 
if (ret == 0) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (new_dvnode != orig_dvnode) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(orig_dvnode, server, ret)); 
if (ret == 0) { 
if (new_dvnode != orig_dvnode) 
if (new_dvnode != orig_dvnode) 
if (new_dvnode != orig_dvnode) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) 
file : ./test/kernel/fs/afs/super.c 
[ OK ] open : 4 ok... 
if not, write to the Free Software 
if (!afs_inode_cachep) { 
if (ret < 0) { 
if (atomic_read(&afs_count_active_inodes) != 0) { 
fore we 
while ((p = strsep(&options, ","))) { 
if (IS_ERR(cell)) 
if (!name) { 
if ((name[0] != '%' && name[0] != '#') || !name[1]) { 
for */ 
force = false; 
force = true; 
if there is one */ 
if (params->volname) { 
if (suffix) { 
force = true; 
force = true; 
if (cellname || !params->cell) { 
if (IS_ERR(cell)) { 
force ? " FORCE" : ""); 
if it's the one we're looking for 
if (IS_ERR(inode)) 
if (params->autocell) 
if (!sb->s_root) 
if (current->nsproxy->net_ns != &init_net) 
if (options) { 
if (ret < 0) 
if (ret < 0) 
if (IS_ERR(key)) { 
if (IS_ERR(vol)) { 
if (!as) { 
if (IS_ERR(sb)) { 
if (!sb->s_root) { 
if (ret < 0) { 
if (!vnode) 
formation about an AFS volume 
if (IS_ERR(key)) 
if (ret < 0) { 
if (vs.max_quota == 0) 
file : ./test/kernel/fs/afs/misc.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
file : ./test/kernel/fs/afs/vlclient.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (!last) 
if (call->reply_size != call->reply_max) 
for (loop = 0; loop < 64; loop++) 
for (loop = 0; loop < 8; loop++) 
for (loop = 0; loop < 8; loop++) { 
if (tmp & AFS_VLSF_RWVOL) 
if (tmp & AFS_VLSF_ROVOL) 
if (tmp & AFS_VLSF_BACKVOL) 
if (tmp & AFS_VLF_RWEXISTS) 
if (tmp & AFS_VLF_ROEXISTS) 
if (tmp & AFS_VLF_BACKEXISTS) 
if (!entry->vidmask) 
if (!call) 
if (padsz > 0) 
if (!call) 
file : ./test/kernel/fs/afs/flock.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if it isn't already running 
if (!afs_lock_manager) { 
if (!afs_lock_manager) { 
if (!afs_lock_manager) 
if it's running 
if (afs_lock_manager) 
if the callback is broken on this vnode, then the lock may now be available 
if the 
if (fl->fl_type == F_RDLCK) { 
for_each_entry_safe(p, _p, &vnode->pending_locks, 
if (p->fl_type == F_RDLCK) { 
for a lock, including: 
if (test_bit(AFS_VNODE_UNLOCKING, &vnode->flags)) { 
if it fails, we just 
if (ret < 0) 
if we've got a lock, then it must be time to extend that lock as AFS 
if (!list_empty(&vnode->granted_locks)) { 
if (test_and_set_bit(AFS_VNODE_LOCKING, &vnode->flags)) 
if we don't have a granted lock, then we must've been called back by 
for */ 
if (test_and_set_bit(AFS_VNODE_LOCKING, &vnode->flags)) 
if (type == AFS_LOCK_READ) 
if (list_entry(vnode->pending_locks.next, 
if (ret == AFS_LOCK_GRANTED) 
if (!list_empty(&vnode->pending_locks)) 
for the unlocking of a vnode on the server to the 
if (!test_and_clear_bit(AFS_VNODE_READLOCKED, &vnode->flags) && 
if (test_and_set_bit(AFS_VNODE_UNLOCKING, &vnode->flags)) 
if (fl->fl_start != 0 || fl->fl_end != OFFSET_MAX) 
if (ret < 0) 
if (ret < 0) 
if (vnode->status.lock_count != 0 && !(fl->fl_flags & FL_SLEEP)) { 
if we've already got a readlock on the server then we can instantly 
if (type == AFS_LOCK_READ && 
if there's no-one else with a lock on this vnode, then we need to 
for a lock */ 
for a local lock to become available */ 
if (!(fl->fl_flags & FL_SLEEP)) { 
for the lock manager thread to get the 
if (fl->fl_u.afs.state <= AFS_LOCK_GRANTED) { 
if (ret < 0) 
if (fl->fl_u.afs.state <= AFS_LOCK_GRANTED) { 
if (ret < 0) { 
if (list_empty(&vnode->granted_locks) && 
if (vnode->pending_locks.prev != &fl->fl_u.afs.link) { 
if (type == AFS_LOCK_READ) 
if (ret < 0) 
if (list_empty(&vnode->granted_locks)) 
if (fl->fl_start != 0 || fl->fl_end != OFFSET_MAX) 
if (ret < 0) { 
if all granted locks are gone */ 
if indeed we hold one 
if (fl->fl_type == F_UNLCK) { 
if (ret < 0) 
if (lock_count) { 
if (__mandatory_lock(&vnode->vfs_inode) && fl->fl_type != F_UNLCK) 
if (IS_GETLK(cmd)) 
if (fl->fl_type == F_UNLCK) 
if (!(fl->fl_flags & FL_FLOCK)) 
if (fl->fl_type == F_UNLCK) 
file : ./test/kernel/fs/afs/write.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (!list_empty(&vnode->writebacks)) { 
if (front->state == AFS_WBACK_SYNCING) { 
if (--wb->usage == 0) 
if (wb) 
for writing 
if (pos + PAGE_CACHE_SIZE > i_size) 
if (ret < 0) { 
form part of a write to a page 
if (!candidate) 
if (!page) { 
if (!PageUptodate(page) && len != PAGE_CACHE_SIZE) { 
if (ret < 0) { 
if this page is already pending a writeback under a suitable key 
if (wb) { 
if (index > 0) { 
for_each_entry(wb, &vnode->writebacks, link) { 
if (index == wb->first && from < wb->offset_first) 
if (index == wb->last && to > wb->to_last) 
if it's dirty we 
fore we can use the new context */ 
if (wb->state == AFS_WBACK_PENDING) 
if (PageDirty(page)) { 
if (ret < 0) { 
if (maybe_i_size > i_size) { 
if (maybe_i_size > i_size) 
if (PageDirty(page)) 
if (count > PAGEVEC_SIZE) 
for (loop = 0; loop < count; loop++) { 
if (error) 
while (first < last); 
if (!clear_page_dirty_for_io(primary_page)) 
if (test_set_page_writeback(primary_page)) 
if (start >= wb->last) 
if (n > ARRAY_SIZE(pages)) 
if (n == 0) 
if (pages[0]->index != start) { 
while (n > 0); 
for (loop = 0; loop < n; loop++) { 
if (page->index > wb->last) 
if (!trylock_page(page)) 
if (!PageDirty(page) || 
if (!clear_page_dirty_for_io(page)) 
if (test_set_page_writeback(page)) 
if (loop < n) { 
for (; loop < n; loop++) 
while (start <= wb->last && count < 65536); 
if (ret < 0) { 
for us 
if (ret < 0) { 
if (!n) 
if (page->index > end) { 
if (page->mapping != mapping) { 
if (wbc->sync_mode != WB_SYNC_NONE) 
if (PageWriteback(page) || !PageDirty(page)) { 
if (ret < 0) { 
while (index < end && wbc->nr_to_write > 0); 
if (wbc->range_cyclic) { 
if (start > 0 && wbc->nr_to_write > 0 && ret == 0) 
if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX) { 
if (wbc->nr_to_write > 0) 
if (count > PAGEVEC_SIZE) 
for (loop = 0; loop < count; loop++) { 
if (page_private(page) == (unsigned long) wb) { 
if (wb->usage == 0) { 
if (free_wb) { 
while (first <= last); 
if (IS_SWAPFILE(&vnode->vfs_inode)) { 
if (!count) 
if (IS_ERR_VALUE(result)) { 
for this process, and check for write errors. 
for this process. 
if (ret) 
if (!wb) { 
for_each_entry(xwb, &vnode->writebacks, link) { 
if (ret < 0) { 
for the preceding writes to actually complete */ 
ification that a previously read-only page is about to become writable 
for the page to be written to the cache before we allow it to 
ifdef CONFIG_AFS_FSCACHE 
if 
file : ./test/kernel/fs/afs/fsclient.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if ctime-type changes seen */ 
while (0) 
if (size != status->size) { 
if (vnode) { 
if (changed && !test_bit(AFS_VNODE_UNSET, &vnode->flags)) { 
if (store_version) 
if (expected_version != data_version) { 
if (vnode && !test_bit(AFS_VNODE_UNSET, &vnode->flags)) { 
if (store_version) { 
if (attr->ia_valid & ATTR_MTIME) { 
if (attr->ia_valid & ATTR_UID) { 
if (attr->ia_valid & ATTR_GID) { 
if (attr->ia_valid & ATTR_MODE) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (call->reply2) 
formation for a file 
if (!call) 
if (call->operation_ID != FSFETCHDATA64) { 
if (call->count > 0) 
if (call->count > PAGE_SIZE) 
if (call->count > 0) { 
if (call->reply2) 
if (skb->len != 0) 
if (!last) 
if (call->count < PAGE_SIZE) { 
if (!call) 
if (upper_32_bits(offset) || upper_32_bits(offset + length)) 
if (!call) 
if (skb->len > 0) 
if (ncallbacks == 0) 
if (ncallbacks > AFSCBMAX) 
if (!call) 
for (loop = ncallbacks; loop > 0; loop--) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (!call) 
if (padsz > 0) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (!call) 
if (padsz > 0) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (!call) 
if (padsz > 0) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (!call) 
if (padsz > 0) { 
if (c_padsz > 0) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (new_dvnode != orig_dvnode) 
if (!call) 
if (o_padsz > 0) { 
if (n_padsz > 0) { 
if (!last) { 
if (call->reply_size != call->reply_max) { 
if (!call) 
if (first != last) 
if (pos + size > i_size) 
if (pos >> 32 || i_size >> 32 || size >> 32 || (pos + size) >> 32) 
if (!call) 
if (!last) { 
if (call->reply_size != call->reply_max) { 
if (call->operation_ID == FSSTOREDATA) 
if (!call) 
if (attr->ia_size >> 32) 
if (!call) 
if there's a change in file 
if (attr->ia_valid & ATTR_SIZE) 
if (!call) 
if (call->count >= AFSNAMEMAX) 
if (call->count > 0) { 
if ((call->count & 3) == 0) { 
if (call->count >= AFSNAMEMAX) 
if (call->count > 0) { 
if ((call->count & 3) == 0) { 
if (call->count >= AFSNAMEMAX) 
if (call->count > 0) { 
if ((call->count & 3) == 0) { 
if (skb->len != 0) 
if (!last) 
if (!tmpbuf) 
if (!call) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (!call) 
if (!call) 
if (!call) 
file : ./test/kernel/fs/afs/mntpt.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (IS_ERR(page)) { 
if (PageError(page)) 
if (size > 2 && 
if (!devname) 
if (!options) 
if (test_bit(AFS_VNODE_PSEUDODIR, &vnode->flags)) { 
if (size < 2 || size > AFS_MAXCELLNAME) 
if (mntpt->d_name.name[0] == '.') { 
if (size > PAGE_SIZE - 1) 
if (IS_ERR(page)) { 
if (PageError(page)) 
if (super->volume->type == AFSVL_RWVOL || rwpath) 
if (IS_ERR(newmnt)) 
if (!list_empty(&afs_vfsmounts)) { 
for_expiry(&afs_vfsmounts); 
if it's still running 
file : ./test/kernel/fs/posix_acl.c 
[ OK ] open : 4 ok... 
for manipulating 
if (acl) { 
if (acl != ACL_NOT_CACHED) 
if (old != ACL_NOT_CACHED) 
forget_cached_acl(struct inode *inode, int type) 
if (old != ACL_NOT_CACHED) 
forget_cached_acl); 
if (old_access != ACL_NOT_CACHED) 
if (old_default != ACL_NOT_CACHED) 
forget_all_cached_acls); 
if (acl != ACL_NOT_CACHED) 
if (!IS_POSIXACL(inode)) 
force a ACL callback by just never filling the 
if (!inode->i_op->get_acl) { 
ified number of entries. 
if (acl) 
if (acl) { 
if (clone) 
if an acl is valid. Returns 0 if it is, or -E... otherwise. 
if (pa->e_perm & ~(ACL_READ|ACL_WRITE|ACL_EXECUTE)) 
if (state == ACL_USER_OBJ) { 
if (state != ACL_USER) 
if (!uid_valid(pa->e_uid)) 
if (state == ACL_USER) { 
if (state != ACL_GROUP) 
if (!gid_valid(pa->e_gid)) 
if (state != ACL_GROUP) 
if (state == ACL_OTHER || 
if (state == 0) 
if the acl can be exactly represented in the traditional 
if (!acl) 
if (mode_p) 
if (!acl) 
if current is granted want access to the inode 
if (uid_eq(inode->i_uid, current_fsuid())) 
if (uid_eq(pa->e_uid, current_fsuid())) 
if (in_group_p(inode->i_gid)) { 
if ((pa->e_perm & want) == want) 
if (in_group_p(pa->e_gid)) { 
if ((pa->e_perm & want) == want) 
if (found) 
for (mask_obj = pa+1; mask_obj != pe; mask_obj++) { 
if ((pa->e_perm & mask_obj->e_perm & want) == want) 
if ((pa->e_perm & want) == want) 
ify acl when creating a new inode. The caller must ensure the acl is 
if (mask_obj) { 
if (!group_obj) 
ify the ACL for the chmod syscall. 
if (mask_obj) { 
if (!group_obj) 
if (clone) { 
if (err < 0) { 
if (clone) { 
if (err) { 
if (!IS_POSIXACL(inode)) 
if (!inode->i_op->set_acl) 
if (IS_ERR_OR_NULL(acl)) { 
if (ret) 
if (S_ISLNK(*mode) || !IS_POSIXACL(dir)) 
if (IS_ERR(p)) { 
if (!p) 
if (!*acl) 
if (ret < 0) { 
if (ret == 0) { 
if (!S_ISDIR(*mode)) { 
if (!value) 
if (size < sizeof(posix_acl_xattr_header)) 
if (header->a_version != cpu_to_le32(POSIX_ACL_XATTR_VERSION)) 
if (count < 0) 
if (count == 0) 
for (end = entry + count; entry != end; entry++) { 
if (user_ns == &init_user_ns) 
if (user_ns == &init_user_ns) 
if (!value) 
if (size < sizeof(posix_acl_xattr_header)) 
if (header->a_version != cpu_to_le32(POSIX_ACL_XATTR_VERSION)) 
if (count < 0) 
if (count == 0) 
if (!acl) 
for (end = entry + count; entry != end; acl_e++, entry++) { 
if (!uid_valid(acl_e->e_uid)) 
if (!gid_valid(acl_e->e_gid)) 
if (!buffer) 
if (real_size > size) 
for (n=0; n < acl->a_count; n++, ext_entry++) { 
if (!IS_POSIXACL(dentry->d_inode)) 
if (S_ISLNK(dentry->d_inode->i_mode)) 
if (IS_ERR(acl)) 
if (acl == NULL) 
if (!IS_POSIXACL(inode)) 
if (!inode->i_op->set_acl) 
if (type == ACL_TYPE_DEFAULT && !S_ISDIR(inode->i_mode)) 
if (!inode_owner_or_capable(inode)) 
if (value) { 
if (IS_ERR(acl)) 
if (acl) { 
if (ret) 
if (!IS_POSIXACL(dentry->d_inode)) 
if (S_ISLNK(dentry->d_inode->i_mode)) 
if (type == ACL_TYPE_ACCESS) 
if (list && size <= list_size) 
if (type == ACL_TYPE_ACCESS) { 
if (error < 0) 
if (error == 0) 
if (error) 
if (default_acl) 
if (acl) 
file : ./test/kernel/fs/logfs/file.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
if (!page) 
if ((len == PAGE_CACHE_SIZE) || PageUptodate(page)) 
if ((pos & PAGE_CACHE_MASK) >= i_size_read(inode)) { 
if (copied < len) { 
if (!PageUptodate(page)) { 
if (copied == 0) 
if (i_size_read(inode) < (index << PAGE_CACHE_SHIFT) + end) { 
if (!PageDirty(page)) { 
for filesystems that don't have to wait 
if (err) 
if (level != 0) 
if (bix < end_index) 
if (bix > end_index || offset == 0) { 
if (block->reserved_bytes) { 
if (IS_RDONLY(inode)) 
if (!inode_owner_or_capable(inode)) 
if (err) 
if (ret) 
if (err) 
if (attr->ia_valid & ATTR_SIZE) { 
if (err) 
file : ./test/kernel/fs/logfs/compr.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
if (err != Z_OK) 
if (err != Z_STREAM_END) 
if (err != Z_OK) 
if (stream.total_out >= stream.total_in) 
if (err != Z_OK) 
if (err != Z_STREAM_END) 
if (err != Z_OK) 
if (!stream.workspace) 
file : ./test/kernel/fs/logfs/dir.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
fore we need to do 
if we were 
if we were interrupted, we delete the inode. 
if we were 
for the estimated amount of information 
for (i = 0; i < len; i++) 
for a given hash 
for 2x and 3x indirect 
for a reasonable amount of hash 
fore overflowing.  Oh and currently we don't overflow but return 
force collisions for a couple of days showed that on average the 
if all entries are carefully chosen. 
if (name->len > LOGFS_MAX_NAMELEN) 
for (round = 0; round < 20; round++) { 
if (beyond_eof(dir, index)) 
if (!logfs_exist_block(dir, index)) 
if (IS_ERR(page)) 
if (name->len != be16_to_cpu(dd->namelen) || 
if (logfs_inode(inode)->li_block) 
if (!ta) 
if (!page) { 
if (IS_ERR(page)) { 
if (!ret) 
if (ret) { 
if (!logfs_empty_dir(inode)) 
if (ctx->pos < 0) 
if (!dir_emit_dots(file, ctx)) 
for (;; pos++, ctx->pos++) { 
if (beyond_eof(dir, pos)) 
if (!logfs_exist_block(dir, pos)) { 
if (IS_ERR(page)) 
if (full) 
if (IS_ERR(page)) 
if (!page) { 
if (IS_ERR(inode)) 
for dentry (%lx, %lx)n", 
if (i_size_read(dir) < index) 
for (round = 0; round < 20; round++) { 
if (logfs_exist_block(dir, index)) 
if (!page) 
if (!err) 
for this particular hash and no fallback. 
if (!ta) { 
if (dest) { 
if (!ret) 
if (ret) { 
if (!ret) 
if (ret) { 
while the mode is 
for us but for some reason fails to do so. 
if (IS_ERR(inode)) 
if (IS_ERR(inode)) 
if (dentry->d_name.len > LOGFS_MAX_NAMELEN) 
if (IS_ERR(inode)) 
if (destlen > dir->i_sb->s_blocksize) 
if (IS_ERR(inode)) 
if (IS_ERR(page)) 
while taking care to remember our operation in the journal. 
if (err) 
if (!ta) 
if (!err) 
if (err) { 
if (!err) 
if (err) 
if (err) 
if (isdir) { 
if (err) 
if (!ta) 
if (err) { 
if (!err) 
if (new_dentry->d_inode) 
fore .get_sb() returns. */ 
if (super->s_victim_ino) { 
if (IS_ERR(inode)) 
if (err) { 
if (super->s_rename_dir) { 
if (IS_ERR(inode)) 
if (err) { 
file : ./test/kernel/fs/logfs/inode.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
fore also lacks a method to store the previous 
for new inodes.  Being just a 32bit counter, 
fore we have to increment 
for now... 
ife a bit 
for GC to make progress, naturally. 
fore we 
ific reference counting on top of what the vfs 
if GC accessed the inode, its 
if (!inode) 
if (!(inode->i_state & I_NEW)) 
if (err || inode->i_nlink == 0) { 
if (!err) 
if we hand out a cached inode, 0 otherwise. 
if (ino == LOGFS_INO_MASTER) 
if (ino == LOGFS_INO_SEGFILE) 
for_each_entry(li, &super->s_freeing_list, li_freeing_list) 
if (inode->i_ino < LOGFS_RESERVED_INOS) { 
if (li->li_refcount == 0) 
if (inode->i_ino == LOGFS_INO_MASTER) 
if (inode->i_ino == LOGFS_INO_SEGFILE) 
if (is_cached) { 
for (i = 0; i < LOGFS_EMBEDDED_FIELDS; i++) 
if (!li) 
fore all other 
while other inodes are still in use and dirty.  Not 
if such 
for 
for... 
if (!inode) 
if (IS_ERR(inode)) 
if (err) { 
if creat() failed.  Safe to skip. */ 
if (super->s_inos_till_wrap < 0) { 
if (!inode) 
for (i = 0; i < LOGFS_EMBEDDED_FIELDS; i++) 
if (!logfs_inode_cache) 
fore we 
file : ./test/kernel/fs/logfs/dev_mtd.c 
[ OK ] open : 4 ok... 
for MTD 
if (ret) 
if we should loop instead. */ 
if (super->s_flags & LOGFS_SB_FLAG_RO) 
ift) << super->s_writeshift); 
if (ret || (retlen != len)) 
for completion before returning 
for (index = ofs >> PAGE_SHIFT; index < (ofs + len) >> PAGE_SHIFT; index++) { 
if (!page) 
if (logfs_super(sb)->s_flags & LOGFS_SB_FLAG_RO) 
if (ret) 
for_completion(&complete); 
if (err == -EUCLEAN || err == -EBADMSG) { 
force GC this segment */ 
while (mtd_block_isbad(mtd, *ofs)) { 
if (*ofs >= mtd->size) 
while (mtd_block_isbad(mtd, *ofs)) { 
if (*ofs <= 0) 
for (i = 0; i < nr_pages; i++) { 
if (err) 
if (super->s_flags & LOGFS_SB_FLAG_RO) 
if (len == 0) { 
if (head) { 
if (!buf) 
if (err) 
if (memchr_inv(buf, 0xff, super->s_writesize)) 
if (IS_ERR(mtd)) 
file : ./test/kernel/fs/logfs/journal.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
for_each(i) 
for GC */ 
for with speed reserve - the filesystem 
if (free < 0) 
for_each(i) { 
for (i = 0; i < LOGFS_EMBEDDED_FIELDS; i++) 
for_each(i) 
if (a->gc_level >= LOGFS_NO_AREAS) 
if (a->vim != VIM_DEFAULT) 
if (area->a_segno) 
if (super->s_writesize > 1) 
if (jh->h_compr == COMPR_NONE) 
if (err) 
if (len > sb->s_blocksize) 
if ((type < JE_FIRST) || (type > JE_LAST)) 
if (datalen > bufsize) 
if (err) 
if (jh->h_crc != logfs_crc32(jh, len + sizeof(*jh), 4)) { 
forgot about the header length 
if (jh->h_crc == logfs_crc32(jh, len, 4)) 
if (err) 
if (err) 
for most recent commit */ 
if (err) 
if (jh->h_type != cpu_to_be16(JE_COMMIT)) 
if (err) 
if ((datalen > sizeof(super->s_je_array)) || 
if (last_ofs == 0) 
if (err) 
for (i = 0; i < super->s_no_je; i++) { 
if (err) 
if (!segno) 
if (err) 
if (crc != sh.crc) { 
for_each(i) { 
if (gec[i] > max) { 
if (max_i == -1) 
for_each(i) { 
if (i == LOGFS_JOURNAL_SEGS) 
while (!super->s_journal_seg[i]); 
if (err) 
for_each(i) 
if (li->li_block) { 
for (i = 0; i < LOGFS_EMBEDDED_FIELDS; i++) 
if (super->s_writesize > 1) 
if (len == 0) 
if (compr_len < 0 || type == JE_ANCHOR) { 
if (ret) 
if (must_pad) { 
if (jh->h_type == cpu_to_be16(JE_COMMIT)) 
if (ofs < 0) 
force u8)level; 
if (fill >= sb->s_blocksize / sizeof(*oa)) { 
if (err) 
if (super->s_je_fill) 
if (!(super->s_flags & LOGFS_SB_FLAG_DIRTY)) 
for_each_area(i) { 
if (err) 
if (err) 
if (err) 
if (err) 
if (err) 
fore 
if (err) 
for_each(i) 
for (i = 0; i < super->s_no_journal_segs; i++) { 
if (!super->s_je) 
if (!super->s_compressed_je) 
if (IS_ERR(super->s_master_inode)) 
if (ret) 
file : ./test/kernel/fs/logfs/gc.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
ifference between low erase 
for "too big" 
ific meaning? :) 
for wear leveling at the 
ifile layer distance 0 */ 
force u8)__gc_level; 
ifile_levels + super->s_iblock_levels - gc_level; 
ifile_levels - (gc_level - 6); 
ifile_levels + super->s_iblock_levels; 
if (reserved) 
for_each_area(i) { 
if (area->a_is_open && area->a_segno == segno) 
if (se.ec_level == cpu_to_be32(BADSEG) || 
if (sh.crc != logfs_crc32(&sh, sizeof(sh), 4)) { 
for (seg_ofs = LOGFS_SEGMENT_HEADERSIZE; 
if (!memchr_inv(&oh, 0xff, sizeof(oh))) 
if (oh.crc != logfs_crc32(&oh, sizeof(oh) - 4, 4)) { 
if (valid == 1) { 
if (valid == 2) { 
while (*p) { 
if (list->sort_by_ec) 
if (comp) 
if (list->count <= list->maxcount) { 
if (ec) 
for normal usage.  It usually gets the 
for wear 
forget 
while.  We have better candidates for each purpose. 
if (cand->valid == 0) { 
if (cand) { 
for Garbage Collection */ 
for wear leveling, 
if (cand) 
if (cand) 
if (!cand) 
if (cand) { 
if (segment_is_reserved(sb, segno)) 
if (valid == RESERVED) 
if (list->count == 0) 
for garbage collection.  Main criterion is 
fort segment on the lowest level first, 
fort.  Hence the LOGFS_MAX_OBJECTSIZE in the comparison. 
for (i = max_dist; i >= 0; i--) { 
if (!this) 
if (!cand) 
if (this->valid + LOGFS_MAX_OBJECTSIZE <= cand->valid) 
if (!cand) { 
ift, 
if (cand) 
if a wrap occurs, 0 otherwise */ 
for (i = SCAN_RATIO; i > 0; i--) { 
if (segno >= super->s_no_segs) { 
if 
forever, looking for GC candidates 
fore 
if (super->s_shadow_tree.no_shadowed_segments >= MAX_OBJ_ALIASES) 
if (no_free_segments(sb) >= target && 
for (round = 0; round < SCAN_ROUNDS; ) { 
if (no_free_segments(sb) >= target) 
if (progress) 
if (round - last_progress > 2) 
if necessary.  However, after 
if (super->s_no_object_aliases < MAX_OBJ_ALIASES) 
if (list_empty(&super->s_object_alias)) { 
for GC not making any progress and limited 
if (*next_event < super->s_gec) { 
if (wl_ratelimit(sb, &super->s_wl_gec_ostore)) 
if (!wl_cand) 
if (!free_cand) 
if (wl_cand->erase_count < free_cand->erase_count + WL_DELTA) { 
if we 
ificant improvement.  That means that a) the current journal segments 
if it is aging 
ifference, compared to ostore wear 
if (wl_ratelimit(sb, &super->s_wl_gec_journal)) 
if (super->s_reserve_list.count < super->s_no_journal_segs) { 
for_each(i) 
for (i = 0; i < 2; i++) { 
if (min_journal_ec > max_reserve_ec + 2 * WL_DELTA) { 
fore free space is getting saturated with dirty 
if (super->s_dirty_used_bytes + super->s_dirty_free_bytes 
if (!area->a_is_open) 
if (super->s_devops->can_write_buf(sb, ofs) == 0) 
fore the journal commit happened.  In that case we wouldn't have 
if (cleaned != valid) 
for_each_area(i) { 
if (err) 
for_each_area(i) 
while (list->count) { 
if (!super->s_free_list.count) 
for us, really. 
for_each_area(i) 
file : ./test/kernel/fs/logfs/segment.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
if (err) 
ift, 
if (use_filler) 
if (page) 
if (IS_ERR(page)) 
if (!PagePrivate(page)) { 
while (len); 
if (len % PAGE_SIZE) { 
if (!PagePrivate(page)) { 
while (no_indizes) { 
if (!PagePrivate(page)) { 
for final writeout. 
if (final) 
for_each_entry(item, &block->item_list, list) { 
if (err) 
for (i = 0; i < count; i++) { 
if (!item) 
if (!block) { 
if (test_and_set_bit(item->child_no, block->alias_map)) { 
while (!list_empty(&block->item_list)) { 
if (level == 0) { 
if (inode->i_ino == LOGFS_INO_MASTER) 
if (shadow->gc_level == 0) 
ifying the 
ifications _before_ 
if (compr_len >= 0) { 
if (shadow->gc_level != 0) { 
for indirect blocks */ 
if (do_compress) 
while (len) { 
if (IS_ERR(page)) 
fore comparing. 
if 0 
if (err) 
if (crc != sh->crc) { 
if 
if (err) 
if (crc != oh->crc) { 
if (!(super->s_flags & LOGFS_SB_FLAG_OBJ_ALIAS)) 
if (!block) 
for_each_entry_safe(item, next, &block->item_list, list) { 
if (!PagePrivate(page)) { 
if (super->s_flags & LOGFS_SB_FLAG_SHUTDOWN) { 
for (pos = 0; ; pos++) { 
if (pos >= LOGFS_BLOCK_FACTOR) 
if (PagePrivate(page)) { 
if (err) 
if (be64_to_cpu(oh.ino) != inode->i_ino 
if (err) 
if (crc != oh.data_crc) { 
if (err) { 
if (crc != oh.data_crc) { 
if (err) { 
if (PageUptodate(page)) 
if (!err) { 
if (!shadow->old_ofs) 
if (shadow->gc_level == 0) 
for (ofs = start; ofs < end; ofs += PAGE_SIZE) { 
if (!page) 
if (PagePrivate(page)) { 
if (area->a_is_open && area->a_used_bytes + bytes <= super->s_segsize) 
if (area->a_is_open) { 
if (err) { 
if (super->s_writesize) 
if (len == 0) 
for_each_area(i) 
for this area.  Effectively takes a 
if (super->s_free_list.count == 0) { 
if (err) 
force u8)area->a_level; 
if (area) 
for_each_area(i) 
if (!area) 
if (IS_ERR(inode)) 
if (!super->s_alias_pool) 
if (!super->s_journal_area) 
for_each_area(i) { 
if (!super->s_area[i]) 
for (i--; i >= 0; i--) 
file : ./test/kernel/fs/logfs/super.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
for 
if (page) 
if (err) { 
if (page == emergency_page) 
for (segno = 0; segno < super->s_no_segs; segno++) { 
if (++segno < super->s_no_segs) { 
if (++segno < super->s_no_segs) { 
if (++segno < super->s_no_segs) { 
formation to device 
formation as it can gather into the spare space. 
for root, similar to ext2. 
ifdef CONFIG_BLOCK 
if 
if (sb->s_mtd) 
if 
if (mtd && sb->s_mtd == mtd) 
if (super->s_bdev && sb->s_bdev == super->s_bdev) 
ifile_levels	= super->s_ifile_levels; 
ift	= super->s_segshift; 
ift	= super->s_writeshift; 
for_each(i) 
if (!page) 
if (err) 
if (err) 
iffer, so ignore them */ 
if (err) 
if (err) 
if (!valid0 && valid1) { 
if (valid0 && !valid1) { 
if (valid0 && valid1 && ds_cmp(ds0, ds1)) { 
fore?!? */ 
if (err) 
if (err) 
for trailing unaccounted data */ 
if (err) 
fore any data gets dirtied */ 
if necessary */ 
if (err) 
if (IS_ERR(rootdir)) 
if (!sb->s_root) 
if (!super->s_erase_page) 
for read-only mounts */ 
if (err) { 
if (ds->ds_magic != cpu_to_be64(LOGFS_MAGIC)) 
if (sh->crc != logfs_crc32(sh, LOGFS_SEGMENT_HEADERSIZE, 4)) 
if (ds->ds_crc != logfs_crc32(ds, sizeof(*ds), 
if (!first || IS_ERR(first)) 
if (!last || IS_ERR(last)) { 
if (!logfs_check_ds(page_address(first))) { 
if (!logfs_check_ds(page_address(last))) { 
if (!page) 
ift; 
ift = ds->ds_segment_shift; 
ift; 
ift = ds->ds_write_shift; 
for_each(i) 
ifile_levels = ds->ds_ifile_levels; 
ifile_levels + super->s_iblock_levels 
if (!super->s_btree_pool) 
if (ret) 
if (ret) 
if (super->s_feature_incompat & ~LOGFS_FEATURES_INCOMPAT) 
if ((super->s_feature_ro_compat & ~LOGFS_FEATURES_RO_COMPAT) && 
if (ret) 
if (ret) 
if (ret) 
if (ret) 
if (super->s_erase_page) 
if (IS_ERR(sb)) { 
if (sb->s_root) { 
for indirect blocks. 
if (err) 
if (err) { 
if (!super) 
if (!devname) 
if (strncmp(devname, "mtd", 3)) 
if (*garbage) 
if (err) { 
if (!emergency_page) 
if (ret) 
if (ret) 
if (!ret) 
file : ./test/kernel/fs/logfs/dev_bdev.c 
[ OK ] open : 4 ok... 
for block devices 
if (err) { 
for_each_segment_all(bvec, bio, i) { 
if (atomic_dec_and_test(&super->s_pending_writes)) 
for (i = 0; i < nr_pages; i++) { 
if (len == 0) { 
if (head) { 
if (atomic_dec_and_test(&super->s_pending_writes)) 
for (i = 0; i < nr_pages; i++) { 
if (super->s_flags & LOGFS_SB_FLAG_RO) 
if (ensure_write) { 
for the journal they are required.  Otherwise a scan 
for block devices. */ 
if (IS_ERR(bdev)) 
if (MAJOR(bdev->bd_dev) == MTD_BLOCK_MAJOR) { 
file : ./test/kernel/fs/logfs/readwrite.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
ifile) 
for 4KiB blocksize) are ignored 
if (level == 0) 
force long)level << LEVEL_SHIFT; 
if (!(index & INDIRECT_BIT)) { 
for (i = 0; i < LOGFS_EMBEDDED_FIELDS; i++) 
for (i = 0; i < LOGFS_EMBEDDED_FIELDS; i++) 
if (li->li_used_bytes >> sb->s_blocksize_bits < ULONG_MAX) 
if (lock) { 
if (PagePreLocked(page)) 
if (lock) 
if (super->s_lock_count) 
while holding 
iff the page in question 
for s_write_mutex.  We annotate this fact by setting PG_pre_locked 
if (page) 
if (lock) { 
for shadowed space 
if (page) 
fore releasing 
if (lock) 
while (unlikely(!trylock_page(page))) { 
if (PagePreLocked(page)) { 
for us, it 
for it to finish. 
if (!page) { 
if (!page) 
if (unlikely(err)) { 
if (err == -EEXIST) 
if (!PagePreLocked(page)) 
if (rw == READ) 
if (rw == READ) 
force int)skip, LOGFS_BLOCK_BITS); 
fore the actual work has been 
for now the line below isn't 
if (inode->i_ino == LOGFS_INO_MASTER) 
ifting generates good code, but must match the format 
for (pos = 0; ; pos++) { 
if (pos >= LOGFS_EMBEDDED_FIELDS + INODE_POINTER_OFS) 
if (err) 
for (pos = 0; ; pos++) { 
if (pos >= LOGFS_BLOCK_FACTOR) 
if (err) 
for_each_entry(block, &super->s_object_alias, alias_list) { 
if (err) 
if (PagePrivate(page)) { 
if (li->li_block) 
if (page->index < first_indirect_block()) { 
if (page->index == first_indirect_block()) { 
if (!page_is_empty) { 
for (i = start; i < LOGFS_BLOCK_FACTOR; i++) { 
if (ptr) 
if (ptr & LOGFS_FULLY_POPULATED) 
if (PagePrivate(page)) 
if (PagePrivate(page)) 
if (!block) 
if (!bofs) 
if (bix >= maxbix(li->li_height)) 
for (level = LEVEL(li->li_height); 
if (!ipage) 
if (ret) { 
if (!bofs) 
if (index < I0_BLOCKS) 
if (!bofs) 
if (bix >= maxbix(li->li_height)) 
for (level = LEVEL(li->li_height); level != 0; level = SUBLEVEL(level)) { 
if (!ipage) 
if (ret) { 
if (!bofs) 
if (bix < I0_BLOCKS) 
for (; bix < I0_BLOCKS; bix++) 
for (level = LEVEL(li->li_height); level != 0; level = SUBLEVEL(level)) { 
if (!page) 
if (ret) { 
while (slot < LOGFS_BLOCK_FACTOR) { 
if (!data && !(be64_to_cpu(rblock[slot]) & LOGFS_FULLY_POPULATED)) 
if (slot >= LOGFS_BLOCK_FACTOR) { 
if (!bofs) { 
if (bix < I0_BLOCKS) { 
if (bix < I0_BLOCKS) 
if (!li->li_data[INDIRECT_INDEX]) 
if (li->li_data[INDIRECT_INDEX] & LOGFS_FULLY_POPULATED) 
if (bix >= maxbix(li->li_height)) 
if (bix < maxbix(li->li_height)) 
if some port writes semi- 
if (bix < I0_BLOCKS) { 
if (bix < I0_BLOCKS) 
if (bix < maxbix(li->li_height)) { 
if (ret >= end) 
for (level = LEVEL(li->li_height); level != 0; level = SUBLEVEL(level)){ 
if (ret) { 
if (!bofs) 
if (pure_ofs(bofs) == ofs) 
if (!bofs) 
if (bix >= maxbix(li->li_height)) 
if (pure_ofs(bofs) == ofs) 
if ((inode->i_nlink == 0) && atomic_read(&inode->i_count) == 1) 
if (bix < I0_BLOCKS) 
if the block is invalid, 1 if it is valid and 2 if it will 
if (ino == -1) 
if (IS_ERR(inode)) 
if (ret) 
for a journal commit. 
if (btree_lookup64(&super->s_shadow_tree.old, ofs)) 
if (ret) { 
if (!bytes) 
if (available < bytes) 
if (available < bytes + super->s_root_reserve && 
if (block && block->reserved_bytes) 
while ((ret = logfs_reserve_bytes(inode, 6 * LOGFS_MAX_OBJECTSIZE)) && 
if (!ret) { 
if (!ta) 
if (inode->i_ino != LOGFS_INO_MASTER) { 
if (!btree_lookup32(&tree->segment_map, segno)) { 
for the current write 
for the current write to the tree, along with any shadows in 
if an inode is written, 
if (PagePrivate(page)) { 
if (shadow) { 
ift); 
if (block->inode && block->inode->i_ino == LOGFS_INO_MASTER) { 
if (!test_bit(child_no, block->alias_map)) { 
for the inode itself. 
if (shadow->new_len == shadow->old_len) 
if (wc->ofs == 0) 
if (wc->flags & WF_WRITE) 
if (wc->flags & WF_DELETE) 
if (err) { 
if (level != 0) { 
if (wc->ofs && full) 
if (err) 
if (empty0 != empty1) 
if (full0 != full1) 
if (!ipage) 
if (this_wc->ofs) { 
if (ret) 
if (!PageUptodate(ipage)) { 
if ((__force u8)level-1 > (__force u8)target_level) 
if (ret) 
if (child_wc.ofs || logfs_block(ipage)->partial) 
for indirect blocks in the future, which we cannot reserve */ 
if (li->li_height > (__force u8)target_level) 
if (ret) 
if (li->li_data[INDIRECT_INDEX] != wc.ofs) { 
if (block && block->ta) 
force u8)level; 
while (height > li->li_height || bix >= maxbix(li->li_height)) { 
if (!page) 
if (err) 
if (logfs_block(page) && logfs_block(page)->reserved_bytes) 
if (index < I0_BLOCKS) 
if (err) 
if (page->index < I0_BLOCKS) 
if (err) 
if (!page) 
if (!page) 
if (!err) { 
if (!err && shrink_level(gc_level) == 0) { 
for the inode 
if (inode->i_ino == LOGFS_INO_MASTER) 
if (size <= pageofs || size - pageofs >= PAGE_SIZE) 
if (err) 
if (err) { 
for (e = I0_BLOCKS - 1; e >= 0; e--) { 
if (!wc.ofs) 
if (!page) 
if (err) { 
if (err) 
ifferent blocksizes */ 
force u8)level]; 
force u8)level]; 
if (*bix <= logfs_start_index(SUBLEVEL(*level))) 
if (err) 
for (e = LOGFS_BLOCK_FACTOR - 1; e >= 0; e--) { 
if (size > next_bix * LOGFS_BLOCKSIZE) 
if (!child_wc.ofs) 
if (!page) 
if ((__force u8)level > 1) 
if (err) 
if (!truncate_happened) { 
if (logfs_block(ipage)->partial) 
if (!wc.ofs) 
if (!page) 
if (err) 
if (li->li_data[INDIRECT_INDEX] != wc.ofs) 
if (size >= logfs_factor(logfs_inode(inode)->li_height)) 
if (ret) 
if 
while (size > target) { 
if (size < target) 
if (!err) 
if (!err) { 
if (err) 
if (!block) 
if (PagePrivate(page)) { 
if (!block) 
if (!PagePrivate(page)) { 
if (ino << sb->s_blocksize_bits > i_size_read(master_inode)) 
if (!logfs_exist_block(master_inode, ino)) 
if (IS_ERR(page)) 
if (!page) 
if (i_size_read(master_inode) < size) 
if (!page) 
if (err) 
for this case */ 
if (write) 
if (write) { 
ift; 
force u8)gc_level; 
if (!page) 
fore and should remain dead, 
if (!inode->i_nlink) { 
if (i_size_read(inode) > 0) 
if (!block) 
if ((logfs_super(sb)->s_flags & LOGFS_SB_FLAG_SHUTDOWN)) { 
ifile or directory) 
if write lock is already taken, 1 otherwise 
formance and stack space. 
if (!page) 
if (i_size_read(inode) < pos + LOGFS_BLOCKSIZE) 
if (IS_ERR(inode)) 
file : ./test/kernel/fs/mpage.c 
[ OK ] open : 4 ok... 
for multipage BIOs. 
ifferent BIOs 
for the details. 
for_each_segment_all(bv, bio, i) { 
if (bio == NULL && (current->flags & PF_MEMALLOC)) { 
while (!bio && (nr_vecs /= 2)) 
if (bio) { 
for mpage_readpages.  The fs supplied get_block might 
if (!page_has_buffers(page)) { 
if there is only one buffer on 
if (inode->i_blkbits == PAGE_CACHE_SHIFT &&  
if (block == page_block) { 
while (page_bh != head); 
if the 
forth and use its buffer_mapped() flag to 
if (page_has_buffers(page)) 
if (last_block > last_block_in_file) 
if (buffer_mapped(map_bh) && block_in_file > *first_logical_block && 
for (relative_block = 0; ; relative_block++) { 
if (page_block == blocks_per_page) 
while (page_block < blocks_per_page) { 
if (block_in_file < last_block) { 
if (get_block(inode, block_in_file, map_bh, 0)) 
if (!buffer_mapped(map_bh)) { 
if (first_hole == blocks_per_page) 
if (buffer_uptodate(map_bh)) { 
if (first_hole != blocks_per_page) 
if (page_block && blocks[page_block-1] != map_bh->b_blocknr-1) 
for (relative_block = 0; ; relative_block++) { 
if (page_block == blocks_per_page) 
if (first_hole != blocks_per_page) { 
if (first_hole == 0) { 
if (fully_mapped) { 
if (fully_mapped && blocks_per_page == 1 && !PageUptodate(page) && 
if (bio && (*last_block_in_bio != blocks[0] - 1)) 
if (bio == NULL) { 
if (!bdev_read_page(bdev, blocks[0] << (blkbits - 9), 
if (bio == NULL) 
if (bio_add_page(bio, page, length, 0) < length) { 
if ((buffer_boundary(map_bh) && relative_block == nblocks) || 
if (bio) 
if (!PageUptodate(page)) 
for example. 
formance. 
for (page_idx = 0; page_idx < nr_pages; page_idx++) { 
if (!add_to_page_cache_lru(page, mapping, 
if (bio) 
if (bio) 
for obtaining the disk 
for pages which are unmapped at the end: end-of-file. 
if (!page_has_buffers(page)) 
if (buffer_counter++ == first_unmapped) 
while (bh != head); 
if the page is not uptodate or a concurrent 
fore we reach the platter. 
if (buffer_heads_over_limit && PageUptodate(page)) 
if (page_has_buffers(page)) { 
if (!buffer_mapped(bh)) { 
if (buffer_dirty(bh)) 
if (first_unmapped == blocks_per_page) 
if (first_unmapped != blocks_per_page) 
if (!buffer_dirty(bh) || !buffer_uptodate(bh)) 
if (page_block) { 
if (boundary) { 
while ((bh = bh->b_this_page) != head); 
for (page_block = 0; page_block < blocks_per_page; ) { 
if (mpd->get_block(inode, block_in_file, &map_bh, 1)) 
if (buffer_new(&map_bh)) 
if (buffer_boundary(&map_bh)) { 
if (page_block) { 
if (block_in_file == last_block) 
if (page->index >= end_index) { 
if (page->index > end_index || !offset) 
if (bio && mpd->last_block_in_bio != blocks[0] - 1) 
if (bio == NULL) { 
if (!bdev_write_page(bdev, blocks[0] << (blkbits - 9), 
if (bio == NULL) 
fore marking the buffer clean or 
if (bio_add_page(bio, page, length, 0) < length) { 
if (boundary || (first_unmapped != blocks_per_page)) { 
if (boundary_block) { 
if (bio) 
if (mpd->use_writepage) { 
if it's dirty.  This is desirable behaviour for memory-cleaning writeback, 
for data-integrity system calls such as fsync().  fsync() 
for data integrity and we must wait for 
if (!get_block) 
if (mpd.bio) 
if (mpd.bio) 
file : ./test/kernel/fs/debugfs/file.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License version 
for people to use instead of /proc or /sys. 
for this file.  This should be a 
if it succeeds.  This 
if your module is unloaded, 
for this value, but rather, check for 
if there are no write bits set, make read only */ 
if there are no read bits set, make write only */ 
for this file.  This should be a 
if it succeeds.  This 
if your module is unloaded, 
for this value, but rather, check for 
if there are no write bits set, make read only */ 
if there are no read bits set, make write only */ 
for this file.  This should be a 
if it succeeds.  This 
if your module is unloaded, 
for this value, but rather, check for 
if there are no write bits set, make read only */ 
if there are no read bits set, make write only */ 
for this file.  This should be a 
if it succeeds.  This 
if your module is unloaded, 
for this value, but rather, check for 
if there are no write bits set, make read only */ 
if there are no read bits set, make write only */ 
for the decimal challenged). For details look at the above unsigned 
for this file.  This should be a 
if there are no write bits set, make read only */ 
if there are no read bits set, make write only */ 
for this file.  This should be a 
if there are no write bits set, make read only */ 
if there are no read bits set, make write only */ 
for this file.  This should be a 
if there are no write bits set, make read only */ 
if there are no read bits set, make write only */ 
for this file.  This should be a 
for this file.  This should be a 
for this file.  This should be a 
if there are no write bits set, make read only */ 
if there are no read bits set, make write only */ 
if (*val) 
if (copy_from_user(buf, user_buf, buf_size)) 
if (strtobool(buf, &bv) == 0) 
for this file.  This should be a 
if it succeeds.  This 
if your module is unloaded, 
for this value, but rather, check for 
for this file.  This should be a 
if it succeeds.  This 
if your module is unloaded, 
for this value, but rather, check for 
format_array(char *buf, size_t bufsize, 
while (--array_size >= 0) { 
if (!buf) 
format_array(buf, size, data->array, data->elements); 
for this file.  This should be a 
if (data == NULL) 
ifdef CONFIG_HAS_IOMEM 
if struct debugfs_reg32 structures 
ify a leading string, 
for example configuration of dma channels 
for (i = 0; i < nregs; i++, regs++) { 
for this file.  This should be a 
if it succeeds.  This 
if your module is unloaded, 
for this value, but rather, check for 
if /* CONFIG_HAS_IOMEM */ 
file : ./test/kernel/fs/debugfs/inode.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License version 
for people to use instead of /proc or /sys. 
ify.h> 
if (inode) { 
for "." entry) */ 
if (dentry->d_inode) 
if (inode) { 
if (!res) { 
ify_mkdir(dir, dentry); 
if (!res) 
while ((p = strsep(&data, ",")) != NULL) { 
if (match_int(&args[0], &option)) 
if (!uid_valid(uid)) 
if (match_int(&args[0], &option)) 
if (!gid_valid(gid)) 
if (match_octal(&args[0], &option)) 
if (err) 
if (!uid_eq(opts->uid, GLOBAL_ROOT_UID)) 
if (!gid_eq(opts->gid, GLOBAL_ROOT_GID)) 
if (opts->mode != DEBUGFS_DEFAULT_MODE) 
if (!fsi) { 
if (err) 
if (err) 
if (error) 
ified, we create it in the root. 
if (!parent) 
if (!IS_ERR(dentry)) { 
if (error) { 
for this file.  This should be a 
for 
for debugfs.  It allows for a 
if it succeeds.  This 
if your module is unloaded, 
for this file.  This should be a 
if it succeeds.  This 
if your module is unloaded, 
for this symbolic link.  This 
if it succeeds.  This 
if your module is 
if (!link) 
if (!result) 
if (debugfs_positive(dentry)) { 
if (!ret) 
for the file to be 
if (IS_ERR_OR_NULL(dentry)) 
if (!parent || !parent->d_inode) 
if (!ret) 
for the file to be 
if (IS_ERR_OR_NULL(dentry)) 
if (!parent || !parent->d_inode) 
for_each_entry_safe(child, next, &parent->d_subdirs, d_u.d_child) { 
if (!list_empty(&child->d_subdirs)) { 
if (!__debugfs_remove(child, parent)) 
if (child != dentry) { 
if (!__debugfs_remove(child, parent)) 
for the renamed object. This 
for rename to succeed. 
if it succeeds. If an error occurs, %NULL will be 
if (!old_dir->d_inode || !new_dir->d_inode) 
if (!old_dentry->d_inode || old_dentry == trap || 
if (IS_ERR(dentry) || dentry == trap || dentry->d_inode) 
ify_oldname_init(old_dentry->d_name.name); 
if (error) { 
ify_move(old_dir->d_inode, new_dir->d_inode, old_name, 
ify_oldname_free(old_name); 
if (dentry && !IS_ERR(dentry)) 
if (!debug_kobj) 
if (retval) 
file : ./test/kernel/fs/file.c 
[ OK ] open : 4 ok... 
if the allocation size will be considered "large" by the VM. 
if (size <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER)) { 
if (data != NULL) 
for write. 
fortable page-tuned chunks: starting at 1024B 
if sysctl_nr_open 
if (unlikely(nr > sysctl_nr_open)) 
if (!fdt) 
if (!data) 
if (!data) 
if (!new_fdt) 
if (unlikely(new_fdt->max_fds <= nr)) { 
while 
if (nr >= cur_fdt->max_fds) { 
if (cur_fdt != &files->fdtab) 
if the requested size exceeds 
for expansion. 
if (nr < fdt->max_fds) 
if (nr >= sysctl_nr_open) 
for (i = size / BITS_PER_LONG; i > 0; ) { 
if (!newf) 
while (unlikely(open_files > new_fdt->max_fds)) { 
if (new_fdt != &newf->fdtab) 
if (!new_fdt) { 
if (unlikely(new_fdt->max_fds < open_files)) { 
for (i = open_files; i != 0; i--) { 
if (f) { 
if a sibling thread 
if (new_fdt->max_fds > open_files) { 
for (;;) { 
if (i >= fdt->max_fds) 
while (set) { 
if (file) { 
if (files) 
if (atomic_dec_and_test(&files->count)) { 
if they are not embedded */ 
if (files) { 
if (fd < files->next_fd) 
if (fd < fdt->max_fds) 
if (fd >= end) 
if (error < 0) 
if (error) 
if (start <= files->next_fd) 
if (flags & O_CLOEXEC) 
if 1 
if (rcu_access_pointer(fdt->fd[fd]) != NULL) { 
if 
if (fd < files->next_fd) 
fore us.  We need to detect this and 
if we allow dup2() do it, _really_ bad things 
forced to by truly lousy API shoved down 
for __alloc_fd()/__fd_install() apply here... 
if (fd >= fdt->max_fds) 
if (!file) 
for (i = 0; ; i++) { 
if (fd >= fdt->max_fds) 
if (!set) 
for ( ; set ; fd++, set >>= 1) { 
if (!(set & 1)) 
if (!file) 
if (file) { 
if ((file->f_mode & mask) || 
if fd table isn't shared. 
fore exiting the syscall and returning control 
if (atomic_read(&files->count) == 1) { 
if (!file || unlikely(file->f_mode & mask)) 
if (!file) 
if (file && (file->f_mode & FMODE_ATOMIC_POS)) { 
if we have threads or if the file might be 
fork()). 
if (flag) 
if 
while extra work in fget() is trivial, locking implications 
if (!tofree && fd_is_open(fd, fdt)) 
if (flags & O_CLOEXEC) 
if (tofree) 
if (!file) 
if (fd >= rlimit(RLIMIT_NOFILE)) 
if (unlikely(err < 0)) 
if ((flags & ~O_CLOEXEC) != 0) 
if (unlikely(oldfd == newfd)) 
if (newfd >= rlimit(RLIMIT_NOFILE)) 
if (unlikely(!file)) 
if (unlikely(err < 0)) { 
if (unlikely(newfd == oldfd)) { /* corner case */ 
if (!fcheck_files(files, oldfd)) 
if (file) { 
if (ret >= 0) 
if (from >= rlimit(RLIMIT_NOFILE)) 
if (err >= 0) { 
if (!files) 
for (fdt = files_fdtable(files); n < fdt->max_fds; n++) { 
if (!file) 
if (res) 
file : ./test/kernel/fs/file_table.c 
[ OK ] open : 4 ok... 
ify.h> 
for file structures */ 
if defined(CONFIG_SYSCTL) && defined(CONFIG_PROC_FS) 
if 
if some error happend e.g. we over file 
for 
if it is opened for write.  If this is not 
if (get_nr_files() >= files_stat.max_files && !capable(CAP_SYS_ADMIN)) { 
fore 
if (percpu_counter_sum_positive(&nr_files) >= files_stat.max_files) 
if (unlikely(!f)) 
if (unlikely(error)) { 
if (get_nr_files() > old_max) { 
for the new file 
for init_file().  This is a 
if (IS_ERR(file)) 
if ((mode & FMODE_READ) && 
if ((mode & FMODE_WRITE) && 
if ((mode & (FMODE_READ | FMODE_WRITE)) == FMODE_READ) 
ify_close(file); 
if (unlikely(file->f_flags & FASYNC)) { 
if (file->f_op->release) 
if (unlikely(S_ISCHR(inode->i_mode) && inode->i_cdev != NULL && 
if ((file->f_mode & (FMODE_READ | FMODE_WRITE)) == FMODE_READ) 
if (file->f_mode & FMODE_WRITER) { 
for (; node; node = next) { 
for __fput() - execve() 
if (atomic_long_dec_and_test(&file->f_count)) { 
if (likely(!in_interrupt() && !(task->flags & PF_KTHREAD))) { 
if (!task_work_add(task, &file->f_u.fu_rcuhead, true)) 
if (llist_add(&file->f_u.fu_llist, &delayed_fput_list)) 
for kernel threads that might be needed 
for completion of __fput() and know 
if you really need it - at the very least, 
if (atomic_long_dec_and_test(&file->f_count)) { 
if (atomic_long_dec_and_test(&file->f_count)) { 
for files.  
file : ./test/kernel/fs/kernfs/file.c 
[ OK ] open : 4 ok... 
ify.h> 
for each open file and one kernfs_open_node 
ify() may be called from any context and bounces notifications 
for NULL. 
ify_list) 
ify_list = KERNFS_NOTIFY_EOL; 
for the given kernfs_node.  This function must 
while holding an active reference. 
if (kn->flags & KERNFS_LOCKDEP) 
for each seq_file iteration and 
fortunately, this is complicated due to the optional custom seq_file 
formed or not only on ERR_PTR(-ENODEV). 
if ERR_PTR(-ENODEV) while invoking it directly after 
if (ops->seq_stop) 
for the same open file. 
if (!kernfs_get_active(of->kn)) 
if (ops->seq_start) { 
if (next == ERR_PTR(-ENODEV)) 
if pos is at the beginning; otherwise, NULL. 
if (ops->seq_next) { 
if (next == ERR_PTR(-ENODEV)) 
if (v != ERR_PTR(-ENODEV)) 
ified in read(2) call should be passed to the read callback making 
for 
if (!buf) 
for the same open file. 
if (!kernfs_get_active(of->kn)) { 
if (ops->read) 
if (len < 0) 
if (copy_to_user(user_buf, buf, len)) { 
if (of->kn->flags & KERNFS_HAS_SEQ_SHOW) 
if userspace is only doing a partial 
if you're writing a value, first read the file, 
if (of->atomic_write_len) { 
if (len > of->atomic_write_len) 
if (!buf) 
if (copy_from_user(buf, user_buf, len)) { 
for the same open file. 
if (!kernfs_get_active(of->kn)) { 
if (ops->write) 
if (len > 0) 
if (!of->vm_ops) 
if (!kernfs_get_active(of->kn)) 
if (of->vm_ops->open) 
if (!of->vm_ops) 
if (!kernfs_get_active(of->kn)) 
if (of->vm_ops->fault) 
if (!of->vm_ops) 
if (!kernfs_get_active(of->kn)) 
if (of->vm_ops->page_mkwrite) 
if (!of->vm_ops) 
if (!kernfs_get_active(of->kn)) 
if (of->vm_ops->access) 
ifdef CONFIG_NUMA 
if (!of->vm_ops) 
if (!kernfs_get_active(of->kn)) 
if (of->vm_ops->set_policy) 
if (!of->vm_ops) 
if (!kernfs_get_active(of->kn)) 
if (of->vm_ops->get_policy) 
if (!of->vm_ops) 
if (!kernfs_get_active(of->kn)) 
if (of->vm_ops->migrate) 
if 
ifdef CONFIG_NUMA 
if 
for more details. 
if (!(of->kn->flags & KERNFS_HAS_MMAP)) 
if (!kernfs_get_active(of->kn)) 
if (rc) 
if the mmap fails: that 
if (vma->vm_file != file) 
if (of->mmapped && of->vm_ops != vma->vm_ops) 
if someone is trying to use close. 
if (vma->vm_ops && vma->vm_ops->close) 
for this instance of open 
if (!kn->attr.open && new_on) { 
if (on) { 
if (on) { 
if (!new_on) 
if (of) 
if (atomic_dec_and_test(&on->refcnt)) 
if (!kernfs_get_active(kn)) 
for details */ 
if ((file->f_mode & FMODE_WRITE) && 
if ((file->f_mode & FMODE_READ) && 
for the file */ 
if (!of) 
ifferent lockdep key to 
for files which implement mmap.  This is a rather 
while holding @of->mutex.  As each 
ifferentiate on 
for now. 
ifferent static lockdep keys. 
if (has_mmap) 
for details. 
if read access doesn't use 
if (ops->seq_show) 
if (error) 
if WRITE */ 
if (error) 
if (!(kn->flags & KERNFS_HAS_MMAP)) 
if (on) 
if (!on) 
for_each_entry(of, &on->files, list) { 
for 
ification), poll will 
for read, write, or exceptions. 
for attributes which actively support 
if it supports poll (Neither 'poll' nor 'select' return 
for the kobj, grab both */ 
if (of->event != atomic_read(&on->event)) 
ify_workfn(struct work_struct *work) 
ify_list */ 
ify_list; 
ify_lock); 
ify_list = kn->attr.notify_next; 
ify_lock); 
if (on) { 
ify */ 
for_each_entry(info, &kernfs_root(kn)->supers, node) { 
if (!inode) 
if (dentry) { 
ify(inode, FS_MODIFY, inode, FSNOTIFY_EVENT_INODE, 
ify - notify a kernfs file 
ify @kn such that poll(2) on @kn wakes up.  Maybe be called from any 
ify(struct kernfs_node *kn) 
if (WARN_ON(kernfs_type(kn) != KERNFS_FILE)) 
ify_lock, flags); 
ify_next = kernfs_notify_list; 
ify_work); 
ify); 
for the file 
for the file's active_ref, %NULL to disable lockdep 
if (name_is_static) 
if (!kn) 
ifdef CONFIG_DEBUG_LOCK_ALLOC 
if 
while holding active ref.  We 
if (ops->seq_show) 
if (ops->mmap) 
if (rc) { 
file : ./test/kernel/fs/kernfs/dir.c 
[ OK ] open : 4 ok... 
ifdef CONFIG_DEBUG_LOCK_ALLOC 
if 
if (p - buf < len + 1) { 
while (kn && kn->parent); 
if @buf 
if (p) 
while (len--) 
for magic directory entries */ 
if (hash >= INT_MAX) 
if (hash != kn->hash) 
if (ns != kn->ns) 
while (*node) { 
if (result < 0) 
if (result > 0) 
if (kernfs_type(kn) == KERNFS_DIR) 
if @kn was actually 
if (RB_EMPTY_NODE(&kn->rb)) 
if (kernfs_type(kn) == KERNFS_DIR) 
if @kn 
if (unlikely(!kn)) 
if (!atomic_inc_unless_negative(&kn->active)) 
if (kernfs_lockdep(kn)) 
if @kn 
if (unlikely(!kn)) 
if (kernfs_lockdep(kn)) 
if (likely(v != KN_DEACTIVATED_BIAS)) 
if (kernfs_lockdep(kn)) { 
if (atomic_read(&kn->active) != KN_DEACTIVATED_BIAS) 
for draining */ 
if (kernfs_lockdep(kn)) { 
if (kn) { 
if it reached zero. 
if (!kn || !atomic_dec_and_test(&kn->count)) 
while holding reference. 
if (kernfs_type(kn) == KERNFS_LINK) 
if (!(kn->flags & KERNFS_STATIC_NAME)) 
if (kn->iattr) { 
if (kn) { 
if (flags & LOOKUP_RCU) 
form fresh lookup for negatives */ 
if (!kernfs_active(kn)) 
if (dentry->d_parent->d_fsdata != kn->parent) 
if (strcmp(dentry->d_name.name, kn->name) != 0) 
ifferent namespace */ 
force lookup.  If we have submounts we must allow the 
if (check_submounts_and_drop(dentry) != 0) 
for determining what's accessible. 
if (dentry->d_sb->s_op == &kernfs_sops) 
if (!(flags & KERNFS_STATIC_NAME)) { 
if (!name) 
if (!kn) 
if (ret < 0) 
if (kn) { 
if @kn is a 
if entry with the given name already 
if (WARN(has_ns != (bool)kn->ns, KERN_WARNING "kernfs: ns %s in '%s' for '%s'\n", 
if (kernfs_type(parent) != KERNFS_DIR) 
if ((parent->flags & KERNFS_ACTIVATED) && !kernfs_active(parent)) 
if (ret) 
if (ps_iattr) { 
for 
if (!(kernfs_root(kn)->flags & KERNFS_ROOT_CREATE_DEACTIVATED)) 
for 
for kernfs_node with name @name under @parent.  Returns pointer to 
if (has_ns != (bool)ns) { 
for '%s'\n", 
while (node) { 
if (result < 0) 
if (result > 0) 
for 
for kernfs_node with name @name under @parent and get a reference 
for the hierarchy 
if (!root) 
if (!kn) { 
if (!(root->flags & KERNFS_ROOT_CREATE_DEACTIVATED)) 
if (!kn) 
if (!rc) 
if (kernfs_ns_enabled(parent)) 
if (!kn || !kernfs_active(kn)) { 
if (!inode) { 
if (!scops || !scops->mkdir) 
if (!kernfs_get_active(parent)) 
if (!scops || !scops->rmdir) 
if (!kernfs_get_active(kn)) 
if (!scops || !scops->rename) 
if (!kernfs_get_active(kn)) 
if (!kernfs_get_active(new_parent)) { 
while (true) { 
if (kernfs_type(pos) != KERNFS_DIR) 
if (!rbn) 
for post-order walk 
for post-order traversal of @root's 
if first iteration, visit leftmost descendant which may be root */ 
if we visited @root, we're done */ 
if there's an unvisited sibling, visit its leftmost descendant */ 
if (rbn) 
for ensuring that this function is not called 
while ((pos = kernfs_next_descendant_post(pos, kn))) { 
if non-root @kn has already finished removal. 
for kernfs_remove_self() which plays with active ref 
if (!kn || (kn->parent && RB_EMPTY_NODE(&kn->rb))) 
while ((pos = kernfs_next_descendant_post(pos, kn))) 
iff @kn was activated.  This avoids draining and 
for nodes which have never been 
if (kn->flags & KERNFS_ACTIVATED) 
for cleanups. 
if (!pos->parent || kernfs_unlink_sibling(pos)) { 
if (ps_iattr) { 
while (pos != kn); 
for ensuring that the objects it 
fore finishing the kernfs operation.  Note that while this 
for the kernfs operation instance. 
fore the enclosing kernfs operation returns. 
fore @kn is released. 
if (kernfs_lockdep(kn)) 
for a sysfs device directory can be 
while holding an active ref itself.  It isn't necessary 
fore proceeding with the usual removal path.  kernfs will ignore later 
forms removal and 
for the completion of not only the winning kernfs_remove_self() but also 
for example, all concurrent writes to a "delete" file to 
form removal.  When the removal 
while holding kernfs_mutex.  The ones which lost arbitration 
if (!(kn->flags & KERNFS_SUICIDAL)) { 
while (true) { 
if ((kn->flags & KERNFS_SUICIDED) && 
while holding kernfs_mutex; otherwise, waiting 
for the kernfs_node with @name and @ns under @parent and remove it. 
if (!parent) { 
if (kn) 
if (kn) 
if (!kn->parent) 
if (!kernfs_active(kn) || !kernfs_active(new_parent)) 
if ((kn->parent == new_parent) && (kn->ns == new_ns) && 
if (kernfs_find_ns(new_parent, new_name, new_ns)) 
if (strcmp(kn->name, new_name) != 0) { 
if (!new_name) 
if (new_name) { 
if (pos) { 
if (!valid) 
if (!pos && (hash > 1) && (hash < INT_MAX)) { 
while (node) { 
if (hash < pos->hash) 
if (hash > pos->hash) 
while (pos && (!kernfs_active(pos) || pos->ns != ns)) { 
if (!node) 
if (pos) { 
if (!node) 
while (pos && (!kernfs_active(pos) || pos->ns != ns)); 
if (!dir_emit_dots(file, ctx)) 
if (kernfs_ns_enabled(parent)) 
for (pos = kernfs_dir_pos(ns, parent, ctx->pos, pos); 
if (!dir_emit(ctx, name, len, ino, type)) 
file : ./test/kernel/fs/kernfs/inode.c 
[ OK ] open : 4 ok... 
if (bdi_init(&kernfs_bdi)) 
if (kn->iattr) 
if (!kn->iattr) 
if (!attrs) 
if (ia_valid & ATTR_UID) 
if (ia_valid & ATTR_GID) 
if (ia_valid & ATTR_ATIME) 
if (ia_valid & ATTR_MTIME) 
if (ia_valid & ATTR_CTIME) 
if (ia_valid & ATTR_MODE) { 
if (!kn) 
if (error) 
if (error) 
if (!attrs) 
if (!attrs) 
if (!strncmp(name, XATTR_SECURITY_PREFIX, XATTR_SECURITY_PREFIX_LEN)) { 
if (error) 
if (error) 
if (secdata) 
if (!strncmp(name, XATTR_TRUSTED_PREFIX, XATTR_TRUSTED_PREFIX_LEN)) { 
if (!attrs) 
if (!attrs) 
if (!attrs) 
if (attrs) { 
ifysecctx(inode, attrs->ia_secdata, 
if (kernfs_type(kn) == KERNFS_DIR) 
for kernfs_node 
for 
if (inode && (inode->i_state & I_NEW)) 
for 
if (mask & MAY_NOT_BLOCK) 
file : ./test/kernel/fs/kernfs/symlink.c 
[ OK ] open : 4 ok... 
for the symlink to point to 
if (!kn) 
if (kernfs_ns_enabled(parent)) 
if (!error) 
while (base->parent) { 
while (kn->parent && base != kn) 
if (base == kn) 
for reverse fillup */ 
while (kn->parent && kn != base) { 
if (len < 2) 
if ((s - path) + len > PATH_MAX) 
while (kn->parent && kn != base) { 
if (len) 
if (page) { 
if (error < 0) 
if (!IS_ERR(page)) 
file : ./test/kernel/fs/kernfs/mount.c 
[ OK ] open : 4 ok... 
if (scops && scops->remount_fs) 
if (scops && scops->show_options) 
if (sb->s_op == &kernfs_sops) 
if (!inode) { 
if (!root) { 
if (!error) 
ified for the mount 
ific magic number 
ified @fs_type and 
if (!info) 
if (IS_ERR(sb) || sb->s_fs_info != info) 
if (IS_ERR(sb)) 
if (new_sb_created) 
if (!sb->s_root) { 
if (error) { 
for kernfs 
for file_system_type->kill_sb().  If a kernfs 
fore freeing kernfs_super_info. 
file : ./test/kernel/fs/cramfs/inode.c 
[ OK ] open : 4 ok... 
for Linux. 
if (!cino->offset) 
if (!cino->size) 
for entries 
if (!inode) 
if (!(inode->i_state & I_NEW)) 
if the lower 2 bits are zero, the inode contains data */ 
for directories, 
for "compressed" 
if (!len) 
if an existing buffer already has the data.. */ 
for (i = 0; i < READ_BUFFERS; i++) { 
if (buffer_dev[i] != sb) 
if (blocknr < buffer_blocknr[i]) 
if (blk_offset + len > BUFFER_SIZE) 
for (i = 0; i < BLKS_PER_BUF; i++) { 
if (blocknr + i < devsize) { 
if (IS_ERR(page)) 
for (i = 0; i < BLKS_PER_BUF; i++) { 
if (page) { 
if (!PageUptodate(page)) { 
for (i = 0; i < BLKS_PER_BUF; i++) { 
if (page) { 
if (!sbi) 
for (i = 0; i < READ_BUFFERS; i++) 
if (super.magic != CRAMFS_MAGIC) { 
for wrong endianness */ 
if (!silent) 
if (super.magic != CRAMFS_MAGIC) { 
if (!silent) 
if (super.flags & ~CRAMFS_SUPPORTED_FLAGS) { 
if (!S_ISDIR(super.root.mode)) { 
if (super.flags & CRAMFS_FLAG_FSID_VERSION_2) { 
if (root_offset == 0) 
if (!(super.flags & CRAMFS_FLAG_SHIFTED_ROOT_OFFSET) && 
if (IS_ERR(root)) 
if (!sb->s_root) 
if (ctx->pos >= inode->i_size) 
if (offset & 3) 
if (!buf) 
while (offset < inode->i_size) { 
ifted by two 
for (;;) { 
if (buf[namelen-1]) 
if (!dir_emit(ctx, buf, namelen, ino, mode >> 12)) 
while (offset < dir->i_size) { 
if (sorted && (dentry->d_name.name[0] < name[0])) 
if (((dentry->d_name.len + 3) & ~3) != namelen) 
for (;;) { 
if (name[namelen-1]) 
if (namelen != dentry->d_name.len) 
if (retval > 0) 
if (!retval) { 
if (sorted) 
if (IS_ERR(inode)) 
if (page->index < maxblock) { 
if (page->index) 
if (compr_len == 0) 
if (unlikely(compr_len > (PAGE_CACHE_SIZE << 1))) { 
if (unlikely(bytes_filled < 0)) 
if (rv < 0) 
if (rv < 0) 
file : ./test/kernel/fs/cramfs/uncompress.c 
[ OK ] open : 4 ok... 
if it 
if (err != Z_OK) { 
if (err != Z_STREAM_END) 
while decompressing!\n", err); 
if (!initialized++) { 
if ( !stream.workspace ) { 
if (!--initialized) { 
file : ./test/kernel/fs/libfs.c 
[ OK ] open : 4 ok... 
for filesystems writers. 
for an in-memory filesystem just wastes 
if the dentry didn't already 
if (dentry->d_name.len > NAME_MAX) 
if (!dentry->d_sb->s_d_op) 
if (offset >= 0) 
if (offset != file->f_pos) { 
if (file->f_pos >= 2) { 
for cursor */ 
while (n && p != &dentry->d_subdirs) { 
if (simple_positive(next)) 
for ramfs-type trees they can't go away without unlink() or rmdir(), 
if (!dir_emit_dots(file, ctx)) 
if (ctx->pos == 2) 
for (p = q->next; p != &dentry->d_subdirs; p = p->next) { 
if (!simple_positive(next)) { 
if (!dir_emit(ctx, next->d_name.name, next->d_name.len, 
for pseudo-filesystems (sockfs, pipefs, bdev - stuff that 
if (IS_ERR(s)) 
if (!root) 
if (!dentry) { 
if (inode->i_private) 
for_each_entry(child, &dentry->d_subdirs, d_u.d_child) { 
if (simple_positive(child)) { 
if (!simple_empty(dentry)) 
if (!simple_empty(new_dentry)) 
if (new_dentry->d_inode) { 
if (they_are_dirs) { 
if (they_are_dirs) { 
for simple filesystem 
for in-memory filesystems or special files 
if (error) 
if (iattr->ia_valid & ATTR_SIZE) 
if (!page) 
if (!PageUptodate(page) && (len != PAGE_CACHE_SIZE)) { 
for non-block-device FSes 
for updating a page after writing is 
for 
if we did a short copy */ 
if (!PageUptodate(page)) 
if (last_pos > inode->i_size) 
for this filesystem, then you must take care 
if (!inode) 
if (!root) 
for (i = 0; !files->name || files->name[0]; i++, files++) { 
if it tries to conflict with the root inode */ 
if (!dentry) 
if (!inode) { 
if (unlikely(!*mount)) { 
if (IS_ERR(mnt)) 
if (!*mount) 
if (!--*count) 
if (pos < 0) 
if (pos >= available || !count) 
if (count > available - pos) 
if (ret == count) 
if (pos < 0) 
if (pos >= available || !count) 
if (count > available - pos) 
if (res == count) 
if (pos < 0) 
if (pos >= available) 
if (count > available - pos) 
for reading. 
if (size > SIMPLE_TRANSACTION_LIMIT - 1) 
if (!ar) 
if (file->private_data) { 
if (copy_from_user(ar->data, buf, size)) 
if (!ar) 
format for read operation */ 
ific access operations. */ 
if (!attr) 
if (!attr->get) 
if (ret) 
if (*ppos) {		/* continued read */ 
if (ret) 
if (!attr->set) 
if (ret) 
if (copy_from_user(attr->set_buf, buf, size)) 
if (ret == 0) 
for the fh_to_dentry export operation 
ified in the file handle. 
if (fh_len < 2) 
for the fh_to_parent export operation 
ified in the file handle if it 
if (fh_len <= 2) 
for simple filesystems 
if true 
for simple 
if (err) 
if (!(inode->i_state & I_DIRTY)) 
if (datasync && !(inode->i_state & I_DIRTY_DATASYNC)) 
if (ret == 0) 
for simple filesystems 
if true 
if (err) 
if so and -EFBIG otherwise. 
if (unlikely(num_blocks == 0)) 
if ((blocksize_bits < 9) || (blocksize_bits > PAGE_CACHE_SHIFT)) 
if ((last_fs_block > (sector_t)(~0ULL) >> (blocksize_bits - 9)) || 
for in-memory filesystems. 
if (!IS_ERR(s)) 
for all anon_inode files. Contrary to pipes, 
if (!inode) 
file : ./test/kernel/fs/qnx4/dir.c 
[ OK ] open : 4 ok... 
while (ctx->pos < inode->i_size) { 
if (bh == NULL) { 
for (; ix < QNX4_INODES_PER_BLOCK; ix++, ctx->pos += QNX4_DIR_ENTRY_SIZE) { 
if (!de->di_fname[0]) 
if (!(de->di_status & (QNX4_FILE_USED|QNX4_FILE_LINK))) 
if (!(de->di_status & QNX4_FILE_LINK)) 
if (!(de->di_status & QNX4_FILE_LINK)) 
if (!dir_emit(ctx, de->di_fname, size, ino, DT_UNKNOWN)) { 
file : ./test/kernel/fs/qnx4/inode.c 
[ OK ] open : 4 ok... 
if ( phys ) { 
fore EOF 
if (*offset < size) 
if (block) { 
while ( --nxtnt > 0 ) { 
if ( !bh ) { 
if ( memcmp( xblk->xblk_signature, "IamXblk", 7 ) ) { 
if (block) { 
if ( ++ix >= xblk->xblk_num_xtnts ) { 
if ( bh ) 
if (s->RootDir.di_fname[0] != '/' || s->RootDir.di_fname[1] != '\0') 
for (j = 0; j < rl; j++) { 
if (bh == NULL) 
for (i = 0; i < QNX4_INODES_PER_BLOCK; i++, rootdir++) { 
if (strcmp(rootdir->di_fname, QNX4_BMNAME) != 0) 
if (!qnx4_sb(sb)->BitMap) 
for bitmap inode"; 
if (!qs) 
if we don't belong here... */ 
if (!bh) { 
fore allocating dentries, inodes, .. */ 
if (errmsg != NULL) { 
if (IS_ERR(root)) { 
if (s->s_root == NULL) 
if (qs) { 
if (!inode) 
if (!(inode->i_state & I_NEW)) 
if (!ino) { 
if (!(bh = sb_bread(sb, block))) { 
if (S_ISREG(inode->i_mode)) { 
if (S_ISDIR(inode->i_mode)) { 
if (S_ISLNK(inode->i_mode)) { 
if (!ei) 
if (qnx4_inode_cachep == NULL) 
fore we 
if (err) 
if (err) { 
file : ./test/kernel/fs/qnx4/namei.c 
[ OK ] open : 4 ok... 
for rmdir/unlink. 
if the filename is correct. For some obscure reason, qnx writes a 
for a second time the way it is, they want us not to access the qnx 
if (bh == NULL) { 
if ((de->di_status & QNX4_FILE_LINK) != 0) { 
if ( thislen > namelen ) 
if (len != thislen) { 
if (strncmp(name, de->di_fname, len) == 0) { 
while (blkofs * QNX4_BLOCK_SIZE + offset < dir->i_size) { 
if (block) 
if (!bh) { 
if (qnx4_match(len, name, bh, &offset)) { 
if (offset < bh->b_size) { 
if (!(bh = qnx4_find_entry(len, dir, name, &de, &ino))) 
if ((de->di_status & QNX4_FILE_LINK) == QNX4_FILE_LINK) { 
if (IS_ERR(foundinode)) { 
file : ./test/kernel/fs/qnx4/bitmap.c 
[ OK ] open : 4 ok... 
while (total < size) { 
if ((bh = sb_bread(sb, start + offset)) == NULL) { 
file : ./test/kernel/fs/ioctl.c 
[ OK ] open : 4 ok... 
ific ioctl methods 
ific argument for ioctl 
if (!filp->f_op->unlocked_ioctl) 
if (error == -ENOIOCTLCMD) 
if (!mapping->a_ops->bmap) 
if (!capable(CAP_SYS_RAWIO)) 
if (res) 
if this was the last 
if (fieinfo->fi_extents_max == 0) { 
if (fieinfo->fi_extents_mapped >= fieinfo->fi_extents_max) 
if (flags & SET_UNKNOWN_FLAGS) 
if (flags & SET_NO_UNMOUNTED_IO_FLAGS) 
if (flags & SET_NOT_ALIGNED_FLAGS) 
if (copy_to_user(dest, &extent, sizeof(extent))) 
if (fieinfo->fi_extents_mapped == fieinfo->fi_extents_max) 
for fiemap 
if (incompat_flags) { 
if (len == 0) 
if (start > maxbytes) 
if (len > maxbytes || (maxbytes - len) < start) 
if (!inode->i_op->fiemap) 
if (copy_from_user(&fiemap, ufiemap, sizeof(fiemap))) 
if (fiemap.fm_extent_count > FIEMAP_MAX_EXTENTS) 
if (error) 
if (fiemap.fm_extent_count != 0 && 
if (fieinfo.fi_flags & FIEMAP_FLAG_SYNC) 
if (copy_to_user(ufiemap, &fiemap, sizeof(fiemap))) 
ifdef CONFIG_BLOCK 
for block based inodes (no locking) 
for block based inodes.  Basically it will just loop 
if you want the locking done for you. 
if (ret) 
if (len >= isize) { 
if (logical_to_blk(inode, len) == 0) 
if (ret) 
if (!buffer_mapped(&map_bh)) { 
if (!past_eof && 
if (past_eof && size) { 
if (size) { 
if we have holes up to/past EOF then we're done */ 
for the case where say we want to map all the 
if there is a hole after the second to last block 
if (start_blk > last_blk && !whole_file) { 
if size != 0 then we know we already have an extent 
if (size) { 
if (ret) 
if (!past_eof && logical + size >= isize) 
while (1); 
if (ret == 1) 
for block based inodes 
formation 
for the fs 
if  /*  CONFIG_BLOCK  */ 
if (copy_from_user(&sr, argp, sizeof(sr))) 
if (error) 
ifdef __sparc__ 
if (O_NONBLOCK != O_NDELAY) 
if 
if (on) 
if (error) 
if ((flag ^ filp->f_flags) & FASYNC) { 
if (!capable(CAP_SYS_ADMIN)) 
if (sb->s_op->freeze_fs == NULL) 
if (!capable(CAP_SYS_ADMIN)) 
for drivers and not intended to be EXPORT_SYMBOL()'d. 
if (S_ISDIR(inode->i_mode) || S_ISREG(inode->i_mode) || 
if (S_ISREG(inode->i_mode)) 
if (!f.file) 
if (!error) 
file : ./test/kernel/fs/xfs/xfs_dir2_readdir.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
if (!xfs_sb_version_hasftype(&mp->m_sb)) 
if (filetype >= XFS_DIR3_FT_MAX) 
if set, indicates that the type field needs to be set up. 
formation from file mode to DT_* as defined in linux/fs.h 
if appropriate for the given operation and filesystem config. 
form entry number */ 
form directory entry */ 
if_flags & XFS_IFINLINE); 
if the directory is way too short. 
if (dp->i_d.di_size < offsetof(xfs_dir2_sf_hdr_t, parent)) { 
if_bytes == dp->i_d.di_size); 
if_u1.if_data; 
if (xfs_dir2_dataptr_to_db(geo, ctx->pos) > geo->datablk) 
for . and .. as we will always need them. 
if (ctx->pos <= dot_offset) { 
if (!dir_emit(ctx, ".", 1, dp->i_ino, DT_DIR)) 
if (ctx->pos <= dotdot_offset) { 
if (!dir_emit(ctx, "..", 2, ino, DT_DIR)) 
while there are more entries and put'ing works. 
for (i = 0; i < sfp->count; i++) { 
if (ctx->pos > off) { 
if (!dir_emit(ctx, (char *)sfep->name, sfep->namelen, ino, 
for block directories. 
for block */ 
if (xfs_dir2_dataptr_to_db(geo, ctx->pos) > geo->datablk) 
if (error) 
fore this. 
for the loop. 
while (ptr < endptr) { 
if (be16_to_cpu(dup->freetag) == XFS_DIR2_DATA_FREE_TAG) { 
for the next iteration. 
fore the desired starting point, skip it. 
if ((char *)dep - (char *)hdr < wantoff) 
if (!dir_emit(ctx, (char *)dep->name, dep->namelen, 
for current block */ 
for read-ahead */ 
for blocks */ 
if (bp) { 
for the 
for (i = geo->fsbcount; i > 0; ) { 
if (!map->br_blockcount && --mip->map_valid) 
if (1 + mip->ra_want > mip->map_blocks && 
if we should ignore this or try to return an 
if (error) 
for, set the final map 
if (mip->nmap == mip->map_size - mip->map_valid) { 
for holes in the mapping, and eliminate them.  Count up 
for (i = mip->map_valid; i < mip->map_valid + mip->nmap; ) { 
if (length) 
if (!mip->map_valid) { 
if (error) 
if (mip->ra_current) 
for (mip->ra_index = mip->ra_offset = i = 0; 
if (i > mip->ra_current && 
if (i > mip->ra_current) { 
for (j = 0; j < geo->fsbcount; j += length ) { 
if this one is used up. 
if (mip->ra_offset == map[mip->ra_index].br_blockcount) { 
for leaf and node directories. 
if (ctx->pos >= XFS_DIR2_MAX_DATAPTR) 
while (curoff < XFS_DIR2_LEAF_OFFSET) { 
if (!bp || ptr >= (char *)bp->b_addr + geo->blksize) { 
if (error || !map_info->map_valid) 
if (curoff < newoff) 
if (curoff > newoff) 
if (byteoff == 0) 
while ((char *)ptr - (char *)hdr < byteoff) { 
if (be16_to_cpu(dup->freetag) 
if (ptr >= (char *)hdr + geo->blksize) { 
if (be16_to_cpu(dup->freetag) == XFS_DIR2_DATA_FREE_TAG) { 
if (!dir_emit(ctx, (char *)dep->name, dep->namelen, 
if (curoff > xfs_dir2_dataptr_to_byte(XFS_DIR2_MAX_DATAPTR)) 
if (bp) 
if (XFS_FORCED_SHUTDOWN(dp->i_mount)) 
if (dp->i_d.di_format == XFS_DINODE_FMT_LOCAL) 
if ((rval = xfs_dir2_isblock(&args, &v))) 
if (v) 
file : ./test/kernel/fs/xfs/xfs_trans_ail.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
ifdef DEBUG 
if (list_empty(&ailp->xa_ail)) 
if (&prev_lip->li_ail != &ailp->xa_ail) 
if (&prev_lip->li_ail != &ailp->xa_ail) 
if /* DEBUG */ 
if (list_empty(&ailp->xa_ail)) 
if (lip->li_ail.next == &ailp->xa_ail) 
if (lip) 
if the AIL is empty. 
if (lip) 
for us. However, for this to be safe, removing an 
for invalidation. 
if ((__psint_t)lip & 1) 
if (lip) 
for_each_entry(cur, &ailp->xa_cursors, list) { 
for a 
if the list is empty. 
if (lsn == 0) { 
for_each_entry(lip, &ailp->xa_ail, li_ail) { 
if (lip) 
for_each_entry_reverse(lip, &ailp->xa_ail, li_ail) { 
if the list is empty. 
for push traversals. The 
if one is 
if (!lip || (__psint_t) lip & 1) 
while we have a 
if (cur) 
form the splice.  Unless the AIL was empty, 
if (lip) 
force the log first and wait for it 
if (ailp->xa_log_flush && ailp->xa_last_pushed_lsn == 0 && 
force(mp, XFS_LOG_SYNC); 
if (!lip) { 
while ((XFS_LSN_CMP(lip->li_lsn, target) <= 0)) { 
for that is that an 
if most of the 
while we can't make 
if (stuck > 100) 
if (lip == NULL) 
if (xfs_buf_delwri_submit_nowait(&ailp->xa_buf_list)) 
if (!count || XFS_LSN_CMP(lsn, target) >= 0) { 
for I/O to complete and remove pushed items from the 
if (((stuck + flushing) * 100) / count > 90) { 
fore 
if they are pinned will all 
force to unpin the stuck items. 
while. 
while (!kthread_should_stop()) { 
if the AIL is empty and we are not racing with a target 
if (!xfs_ail_min(ailp) && 
if (tout) 
forward.  It does this by 
for space to become available. 
if we set the pushing bit approriately. 
if (!lip || XFS_FORCED_SHUTDOWN(ailp->xa_mount) || 
fore it clears 
if (threshold_lsn) 
while ((lip = xfs_ail_max(ailp)) != NULL) { 
forward in the AIL. 
fore returning. 
for (i = 0; i < nr_items; i++) { 
if (lip->li_flags & XFS_LI_IN_AIL) { 
if (XFS_LSN_CMP(lsn, lip->li_lsn) <= 0) 
if (mlip == lip) 
if (!list_empty(&tmp)) 
if (mlip_changed) { 
for deletion. This includes checking that the items are in the AIL. 
ificantly reduce the amount 
fore returning. 
for (i = 0; i < nr_items; i++) { 
if (!(lip->li_flags & XFS_LI_IN_AIL)) { 
if (!XFS_FORCED_SHUTDOWN(mp)) { 
force_shutdown(mp, shutdown_type); 
if (mlip == lip) 
if (mlip_changed) { 
if (list_empty(&ailp->xa_ail)) 
if (!ailp) 
if (IS_ERR(ailp->xa_task)) 
file : ./test/kernel/fs/xfs/xfs_sysctl.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
ifdef CONFIG_PROC_FS 
if (!ret && write && *valp) { 
for_each_possible_cpu(c) { 
if (!ret && write) { 
ifdef DEBUG 
if 
if /* CONFIG_PROC_FS */ 
ifetime", 
ifdef CONFIG_PROC_FS 
if /* CONFIG_PROC_FS */ 
if (!xfs_table_header) 
file : ./test/kernel/fs/xfs/xfs_log_recover.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
if defined(DEBUG) 
if 
for buffer create/read/write/access 
ify the given count of basic blocks is valid number of blocks 
for an operation involving the given XFS log buffer. 
if (!xlog_buf_bbcount_valid(log, nbblks)) { 
for buffer", 
for complete log sectors. 
for a non-sector- 
if the log I/O is 
if (nbblks > 1 && log->l_sectBBsize > 1) 
if (bp) 
if (!xlog_buf_bbcount_valid(log, nbblks)) { 
for buffer", 
if (XFS_FORCED_SHUTDOWN(log->l_mp)) 
if (error) 
if (error) 
if (error) 
if (error) 
for the given number of blocks. 
for synchronous log writes. 
if (!xlog_buf_bbcount_valid(log, nbblks)) { 
for buffer", 
if (error) 
ifdef DEBUG 
formation 
if 
for recovery 
if (unlikely(head->h_fmt != cpu_to_be32(XLOG_FMT))) { 
format - can't recover"); 
if (unlikely(!uuid_equal(&mp->m_sb.sb_uuid, &head->h_fs_uuid))) { 
if (uuid_is_nil(&head->h_fs_uuid)) { 
if (unlikely(!uuid_equal(&mp->m_sb.sb_uuid, &head->h_fs_uuid))) { 
if (bp->b_error) { 
force_shutdown(bp->b_target->bt_mount, 
while (mid_blk != first_blk && mid_blk != end_blk) { 
if (error) 
if (mid_cycle == cycle) 
if there is no such 
form another test. 
ify_cycle( 
while (bufblks > log->l_logBBsize) 
while (!(bp = xlog_get_bp(log, bufblks))) { 
if (bufblks < log->l_sectBBsize) 
for (i = start_blk; i < start_blk + nbblks; i += bufblks) { 
if (error) 
for (j = 0; j < bcount; j++) { 
if (cycle == stop_on_cycle_no) { 
fore, we subtract one to get the block number 
ified on a previous 
ify_log_record( 
if (!(bp = xlog_get_bp(log, num_blks))) { 
if (error) 
for (i = (*last_blk) - 1; i >= 0; i--) { 
if (smallmem) { 
if (error) 
if (head->h_magicno == cpu_to_be32(XLOG_HEADER_MAGIC_NUM)) 
if (!smallmem) 
for the end of the physical log. 
if (i == -1) { 
fore_ the head. So we check the uuid. 
if ((error = xlog_header_check_mount(log->l_mp, head))) 
fore we expected one. 
if (xfs_sb_version_haslogv2(&log->l_mp->m_sb)) { 
if (h_size % XLOG_HEADER_CYCLE_SIZE) 
if (*last_blk - i + extra_bblks != 
if we start writing 
if normal, non-zero if error. 
if ((error = xlog_find_zeroed(log, &first_blk)) == -1) { 
if (!first_blk) { 
if (error) { 
if (!bp) 
if (error) 
if (error) 
if (first_half_cycle == last_half_cycle) { 
ifying that there are no holes still 
for cycle numbers equal to x-1. 
for the first occurrence of last_half_cycle.  The binary 
for occurrences of last_half_cycle before 
for occurrences of last_half_cycle - 1 
if ((error = xlog_find_cycle_start(log, bp, first_blk, 
if 
if (head_blk >= num_scan_bblks) { 
formed 
if ((error = xlog_find_verify_cycle(log, 
if (new_blk != -1) 
for blocks with cycle number 
for this is 
for occurrences of last_half_cycle.  If we 
for 
if ((error = xlog_find_verify_cycle(log, start_blk, 
if (new_blk != -1) { 
ify that it doesn't find 
if ((error = xlog_find_verify_cycle(log, 
if (new_blk != -1) 
if (head_blk >= num_scan_bblks) { 
fore head_blk */ 
if (error) 
if ((error = xlog_find_verify_log_record(log, start_blk, 
if ((error = xlog_find_verify_log_record(log, 
if (error) 
if (new_blk != log_bbnum) 
if (error) 
if (head_blk == log_bbnum) 
if (error) 
if ((error = xlog_find_head(log, head_blk))) 
if (!bp) 
if (*head_blk == 0) {				/* special case */ 
if (error) 
if (xlog_get_cycle(offset) == 0) { 
for log record header block 
for (i = (int)(*head_blk) - 1; i >= 0; i--) { 
if (error) 
if (*(__be32 *)offset == cpu_to_be32(XLOG_HEADER_MAGIC_NUM)) { 
if (!found) { 
for (i = log->l_logBBsize - 1; i >= (int)(*head_blk); i--) { 
if (error) 
if (*(__be32 *)offset == 
if (!found) { 
for.  Therefore, we know that the last good log record 
if (found == 2) 
for unmount record.  If we find it, then we know there 
fore comparing 
if there is one, so we pass the lsn of the 
if (xfs_sb_version_haslogv2(&log->l_mp->m_sb)) { 
if ((h_version & XLOG_VERSION_2) && 
if (h_size % XLOG_HEADER_CYCLE_SIZE) 
if (*head_blk == after_umount_blk && 
if (error) 
if (op_head->oh_flags & XLOG_UNMOUNT_TRANS) { 
if we 
fore earlier ones. 
ifying it so that we'll never 
if we are going to recover the filesystem 
if (!readonly)" 
if NORECOVERY is specified on mount, 
if the -device- itself is readonly, just skip this. 
if (!xfs_readonly_buftarg(log->l_mp->m_logdev_targp)) 
if (error) 
form an X block read 
if (!bp) 
if (error) 
if (first_cycle == 0) {		/* completely zeroed log */ 
if (error) 
if (last_cycle != 0) {		/* log completely written to */ 
if (first_cycle != 1) { 
if ((error = xlog_find_cycle_start(log, bp, 0, &last_blk, 0))) 
if (last_blk < num_scan_bblks) 
for any instances of cycle number 0 that occur before 
if ((error = xlog_find_verify_cycle(log, start_blk, 
if (new_blk != -1) 
if ((error = xlog_find_verify_log_record(log, start_blk, 
if (error) 
if (error) 
while (bufblks > log->l_logBBsize) 
while (!(bp = xlog_get_bp(log, bufblks))) { 
if (bufblks < sectbb) 
if (balign != start_block) { 
if (error) 
for (i = start_block; i < end_block; i += bufblks) { 
if (j == 0 && (start_block + endcount > ealign)) { 
if (error) 
for (; j < endcount; j++) { 
if (error) 
if we come up, write only a little bit more, and then crash again. 
fore the crash, but 
if (head_cycle == tail_cycle) { 
if (unlikely(head_block < tail_block || head_block >= log->l_logBBsize)) { 
if (unlikely(head_block >= tail_block || head_cycle != (tail_cycle + 1))){ 
if (tail_distance <= 0) { 
for no reason. 
if ((head_block + max_distance) <= log->l_logBBsize) { 
if (error) 
if (error) 
if (error) 
for_each_entry(trans, head, r_list) { 
if (list_empty(&trans->r_itemq)) { 
format structures fit into the first 32 bits of the structure. 
format_t	*in_f;			/* any will do */ 
if (!len) 
if (list_empty(&trans->r_itemq)) { 
if (*(uint *)dp != XFS_TRANS_HEADER_MAGIC) { 
if (len == sizeof(xfs_trans_header_t)) 
format_t *)ptr; 
if (item->ri_total != 0 && 
if (item->ri_total == 0) {		/* first region to be added */ 
format", 
formed so 
form the cancelled buffer table. Hence they have tobe done last. 
fore we remove the unlinked inode list pointer. 
for all buffers except cancelled/inode unlink buffers 
for inode unlink buffers 
ific ordering that we need to 
for_each_entry_safe(item, n, &sort_list, ri_list) { 
if (buf_f->blf_flags & XFS_BLF_CANCEL) { 
if (buf_f->blf_flags & XFS_BLF_INODE_BUF) { 
if (!list_empty(&sort_list)) 
if (!list_empty(&buffer_list)) 
if (!list_empty(&inode_list)) 
if (!list_empty(&inode_buffer_list)) 
if (!list_empty(&cancel_list)) 
format_t	*buf_f = item->ri_buf[0].i_addr; 
if (!(buf_f->blf_flags & XFS_BLF_CANCEL)) { 
for_each_entry(bcp, bucket, bc_list) { 
if (!log->l_buf_cancel_table) { 
for_each_entry(bcp, bucket, bc_list) { 
if this is the last reference. 
if the same buffer is re-used again after its 
if (!bcp) 
if this is the last reference. 
if (flags & XFS_BLF_CANCEL) { 
form recovery for a buffer full of inodes.  In these buffers, the only 
for the inodes is always logged through the inodes themselves rather 
format_t	*buf_f) 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
for (i = 0; i < inodes_per_buf; i++) { 
while (next_unlinked_offset >= 
if (bit == -1) 
if (next_unlinked_offset < reg_buf_offset) 
if (unlikely(*logged_nextp == 0)) { 
for whoever 
for these cases we 
format flags.  Hence 
ify that it belongs to this 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (lsn != (xfs_lsn_t)-1) { 
if (lsn != (xfs_lsn_t)-1) { 
ified, and there is no global buffer LSN. Hence we need to 
ifications. 
for writeback. Magic numbers are in a 
format_t	*buf_f) 
if (magic32 != XFS_AGF_MAGIC) { 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (magic32 != XFS_AGFL_MAGIC) { 
if (magic32 != XFS_AGI_MAGIC) { 
ifdef CONFIG_XFS_QUOTA 
if 
if (magic16 != XFS_DINODE_MAGIC) { 
if (magic32 != XFS_SYMLINK_MAGIC) { 
if (magic32 != XFS_DIR2_BLOCK_MAGIC && 
if (magic32 != XFS_DIR2_DATA_MAGIC && 
if (magic32 != XFS_DIR2_FREE_MAGIC && 
if (magicda != XFS_DIR2_LEAF1_MAGIC && 
if (magicda != XFS_DIR2_LEAFN_MAGIC && 
if (magicda != XFS_DA_NODE_MAGIC && 
if (magicda != XFS_ATTR_LEAF_MAGIC && 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (magic32 != XFS_ATTR3_RMT_MAGIC) { 
if (magic32 != XFS_SB_MAGIC) { 
form a 'normal' buffer recovery.  Each logged region of the 
format structure indicates 
format_t	*buf_f) 
format structure */ 
while (1) { 
if (bit == -1) 
for writing into 
if (item->ri_buf[i].i_len < (nbits << XFS_BLF_SHIFT)) 
if this is a dquot buffer. Just checking 
for other buf types also. 
if (buf_f->blf_flags & 
if (item->ri_buf[i].i_addr == NULL) { 
if (item->ri_buf[i].i_len < sizeof(xfs_disk_dquot_t)) { 
if (error) 
if we should have replayed the item. If we replay old 
ification failures. Hence for now 
for non-crc filesystems 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
form a dquot buffer recovery. 
format	*buf_f) 
if (mp->m_qflags == 0) { 
if (buf_f->blf_flags & XFS_BLF_UDQUOT_BUF) 
if (buf_f->blf_flags & XFS_BLF_PDQUOT_BUF) 
if (buf_f->blf_flags & XFS_BLF_GDQUOT_BUF) 
if (log->l_quotaoffs_flag & type) 
ification made to a buffer at runtime. 
ifferently.  Inode buffers are handled differently 
if the blocks covered by the buffer are reused for 
fore we crash we don't end up replaying old, freed 
for more details on the implementation of the table of cancel records. 
format_t	*buf_f = item->ri_buf[0].i_addr; 
if (xlog_check_buffer_cancelled(log, buf_f->blf_blkno, 
if (buf_f->blf_flags & XFS_BLF_INODE_BUF) 
if (!bp) 
if (error) { 
if we get an LSN from it and it's less than 
if (lsn && lsn != -1 && XFS_LSN_CMP(lsn, current_lsn) >= 0) 
if (buf_f->blf_flags & XFS_BLF_INODE_BUF) { 
if (buf_f->blf_flags & 
if (error) 
form delayed write on the buffer.  Asynchronous writes will be 
ifferent size if the log was generated 
ifferent inode cluster size.  Regardless, if the 
for *our* value of mp->m_inode_cluster_size, then we need to keep 
if (XFS_DINODE_MAGIC == 
fork owner changes 
fork and change the 
ified. We can do this 
ify for writeback so we pass the buffer_list 
for the operation to use. 
format *in_f, 
if (!ip) 
iformat_fork(ip, dip); 
if (in_f->ilf_fields & XFS_ILOG_DOWNER) { 
if (error) 
if (in_f->ilf_fields & XFS_ILOG_AOWNER) { 
if (error) 
format_t	*in_f; 
if (item->ri_buf[0].i_len == sizeof(xfs_inode_log_format_t)) { 
format_t), KM_SLEEP); 
format_convert(&item->ri_buf[0], in_f); 
for it, 
if (xlog_check_buffer_cancelled(log, in_f->ilf_blkno, 
if (!bp) { 
if (error) { 
if (unlikely(dip->di_magic != cpu_to_be16(XFS_DINODE_MAGIC))) { 
if (unlikely(dicp->di_magic != XFS_DINODE_MAGIC)) { 
if it's less 
if (dip->di_version >= 3) { 
if (lsn && lsn != -1 && XFS_LSN_CMP(lsn, current_lsn) >= 0) { 
for v1/2 inodes. All changes for v3 inodes 
if (!xfs_sb_version_hascrc(&mp->m_sb) && 
if (be16_to_cpu(dip->di_flushiter) == DI_MAX_FLUSH && 
if (unlikely(S_ISREG(dicp->di_mode))) { 
format != XFS_DINODE_FMT_EXTENTS) && 
if (unlikely(S_ISDIR(dicp->di_mode))) { 
format != XFS_DINODE_FMT_EXTENTS) && 
format != XFS_DINODE_FMT_LOCAL)) { 
if (unlikely(dicp->di_nextents + dicp->di_anextents > dicp->di_nblocks)){ 
if (unlikely(dicp->di_forkoff > mp->m_sb.sb_inodesize)) { 
forkoff 0x%x", __func__, 
if (unlikely(item->ri_buf[1].i_len > isize)) { 
format */ 
format */ 
if (in_f->ilf_size == 2) 
fork flags set. 
if (in_f->ilf_fields & XFS_ILOG_AFORK) { 
if (in_f->ilf_fields & (XFS_ILOG_DOWNER|XFS_ILOG_AOWNER)) 
if (need_free) 
format_t	*qoff_f = item->ri_buf[0].i_addr; 
if this was user quotaoff, 
if (qoff_f->qf_flags & XFS_UQUOTA_ACCT) 
if (qoff_f->qf_flags & XFS_PQUOTA_ACCT) 
if (qoff_f->qf_flags & XFS_GQUOTA_ACCT) 
format_t	*dq_f; 
if (mp->m_qflags == 0) 
if (recddq == NULL) { 
if (item->ri_buf[1].i_len < sizeof(xfs_disk_dquot_t)) { 
if (log->l_quotaoffs_flag & type) 
if (error) 
if (error) 
if (error) { 
if it's less 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
if (lsn && lsn != -1 && XFS_LSN_CMP(lsn, current_lsn) >= 0) { 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
format structure which was logged on disk. 
format_t	*efi_formatp; 
formatp->efi_nextents); 
format)))) { 
formatp->efi_nextents); 
format structure is found in 
if it was still in the log.  To do this 
for the efi with an id equal to that in the 
format_t	*efd_formatp; 
formatp = item->ri_buf[0].i_addr; 
formatp->efd_nextents - 1) * sizeof(xfs_extent_32_t)))) || 
formatp->efd_nextents - 1) * sizeof(xfs_extent_64_t))))); 
for the efi with the id in the efd format structure 
while (lip != NULL) { 
if (efip->efi_format.efi_id == efi_id) { 
format structure is found in a 
ifications will hit the cached buffer 
if (icl->icl_type != XFS_LI_ICREATE) { 
if (icl->icl_size != 1) { 
if (agno >= mp->m_sb.sb_agcount) { 
if (!agbno || agbno == NULLAGBLOCK || agbno >= mp->m_sb.sb_agblocks) { 
if (isize != mp->m_sb.sb_inodesize) { 
if (!count) { 
if (!length || length >= mp->m_sb.sb_agblocks) { 
if (count != mp->m_ialloc_inos || 
if (xlog_check_buffer_cancelled(log, 
for_each_entry_safe(item, n, &trans->r_itemq, ri_list) { 
for (i = 0; i < item->ri_cnt; i++) 
format	*buf_f = item->ri_buf[0].i_addr; 
if (xlog_peek_buffer_cancelled(log, buf_f->blf_blkno, 
format	ilf_buf; 
if (item->ri_buf[0].i_len == sizeof(struct xfs_inode_log_format)) { 
format_convert(&item->ri_buf[0], ilfp); 
if (xlog_peek_buffer_cancelled(log, ilfp->ilf_blkno, ilfp->ilf_len, 0)) 
format	*dq_f; 
if (mp->m_qflags == 0) 
if (recddq == NULL) 
if (item->ri_buf[1].i_len < sizeof(struct xfs_disk_dquot)) 
if (log->l_quotaoffs_flag & type) 
for_each_entry(item, item_list, ri_list) { 
if (error) 
form the transaction. 
for them. 
if (error) 
for_each_entry_safe(item, next, &trans->r_itemq, ri_list) { 
if (items_queued >= XLOG_RECOVER_COMMIT_QUEUE_MAX) { 
if (error) 
if (!list_empty(&ra_list)) { 
if (!list_empty(&done_list)) 
format matches our own - else we can't recover */ 
while ((dp < lp) && num_logops) { 
if (ohead->oh_clientid != XFS_TRANSACTION && 
if (trans == NULL) {		   /* not found; add new tid */ 
if (dp + be32_to_cpu(ohead->oh_len) > lp) { 
if (flags & XLOG_WAS_CONT_TRANS) 
if (error) { 
for (i = 0; i < efip->efi_format.efi_nextents; i++) { 
if ((startblock_fsb == 0) || 
format.efi_nextents); 
if (error) 
format.efi_nextents); 
format.efi_extents[i]); 
if (error) 
while (lip != NULL) { 
if (lip->li_type != XFS_LI_EFI) { 
for (; lip; lip = xfs_trans_ail_cursor_next(ailp, &cur)) 
if 
if (test_bit(XFS_EFI_RECOVERED, &efip->efi_flags)) { 
if (error) 
forms a transaction to null out a bad inode pointer 
if (error) 
if (error) 
if (error) 
if (error) 
if (error) 
for the next pass */ 
form a transaction to 
while in this function. 
for (agno = 0; agno < mp->m_sb.sb_agcount; agno++) { 
for this ag. 
if (error) { 
for the AGI 
while we need the buffer. 
for (bucket = 0; bucket < XFS_AGI_UNLINKED_BUCKETS; bucket++) { 
while (agino != NULLAGINO) { 
if and only if the CRC in the header is non-zero. This makes the 
if (crc != rhead->h_crc) { 
if we are enforcing 
if (xfs_sb_version_hascrc(&log->l_mp->m_sb)) 
if (error) 
for (i = 0; i < BTOBB(be32_to_cpu(rhead->h_len)) && 
if (xfs_sb_version_haslogv2(&log->l_mp->m_sb)) { 
for ( ; i < BTOBB(be32_to_cpu(rhead->h_len)); i++) { 
if (unlikely(rhead->h_magicno != cpu_to_be32(XLOG_HEADER_MAGIC_NUM))) { 
if (unlikely( 
if (unlikely( hlen <= 0 || hlen > INT_MAX )) { 
if (unlikely( blkno > log->l_logBBsize || blkno > INT_MAX )) { 
if (xfs_sb_version_haslogv2(&log->l_mp->m_sb)) { 
if (!hbp) 
if (error) 
if (error) 
if ((be32_to_cpu(rhead->h_version) & XLOG_VERSION_2) && 
if (h_size % XLOG_HEADER_CYCLE_SIZE) 
if (!hbp) 
if (!dbp) { 
if (tail_blk <= head_blk) { 
for (blk_no = tail_blk; blk_no < head_blk; ) { 
if (error) 
if (error) 
if (error) 
if (error) 
if (error) 
form recovery around the end of the physical log. 
while (blk_no < log->l_logBBsize) { 
for header wrapping around physical end-of-log 
if (blk_no + hblks <= log->l_logBBsize) { 
if (error) 
if (blk_no != log->l_logBBsize) { 
fore physical log end */ 
if (error) 
for the second read; 
if (error) 
if (error) 
for log record */ 
if (error) 
if (blk_no != log->l_logBBsize) { 
fore the physical 
if (error) 
for the second read; 
if (error) 
if (error) 
if (error) 
while (blk_no < head_blk) { 
if (error) 
if (error) 
if (error) 
if (error) 
if (error) 
for those which 
ific routines. 
for use in the second pass. 
for (i = 0; i < XLOG_BC_TABLE_SIZE; i++) 
if (error != 0) { 
ifdef DEBUG 
for (i = 0; i < XLOG_BC_TABLE_SIZE; i++) 
if	/* DEBUG */ 
if (error) 
if (XFS_FORCED_SHUTDOWN(log->l_mp)) { 
ify it. 
if (XFS_FORCED_SHUTDOWN(log->l_mp)) { 
if (error) { 
format */ 
form recovery and re-initialize some log variables in xlog_find_tail. 
if ((error = xlog_find_tail(log, &head_blk, &tail_blk))) 
if (tail_blk != head_blk) { 
for ENOSPC and turns it into an intelligent 
ify 
if ((error = xfs_dev_is_read_only(log->l_mp, "recovery"))) { 
if there are any unknown log features 
fore touching anything. 
if (XFS_SB_VERSION_NUM(&log->l_mp->m_sb) == XFS_SB_VERSION_5 && 
for the 
forming recovery actions 
if (log->l_flags & XLOG_RECOVERY_NEEDED) { 
if (error) { 
force(log->l_mp, XFS_LOG_SYNC); 
if defined(DEBUG) 
ifree; 
ifree = 0LL; 
for (agno = 0; agno < mp->m_sb.sb_agcount; agno++) { 
if (error) { 
if (error) { 
ifree += be32_to_cpu(agi->agi_freecount); 
if /* DEBUG */ 
file : ./test/kernel/fs/xfs/xfs_dquot_item.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format) + 
for the given dquot log item. 
format( 
format	*qlf; 
format)); 
if the count goes to 0.	 The 
if (atomic_dec_and_test(&dqp->q_pincount)) 
for the given dquot to be unpinned. 
if (atomic_read(&dqp->q_pincount) == 0) 
force(dqp->q_mount, 0); 
if (atomic_read(&dqp->q_pincount) > 0) 
if (!xfs_dqlock_nowait(dqp)) 
if (atomic_read(&dqp->q_pincount) > 0) { 
for the flush to finish and remove the item from 
if (!xfs_dqflock_nowait(dqp)) { 
if (error) { 
if (!xfs_buf_delwri_queue(bp, buffer_list)) 
ific to the current transaction.  If the 
for the logitem. 
for dquots 
format	= xfs_qm_dquot_logitem_format, 
for a newly allocated dquot. 
for an quotaoff item.  It just logs the 
format( 
format *qlf; 
for an quotaoff item, so just return. 
for an quotaoff item, unpinning does 
for the log to be flushed to disk. 
if we allow 
ifferent routines for 
forward or back or do something else). 
format	= xfs_qm_qoff_logitem_format, 
format	= xfs_qm_qoff_logitem_format, 
file : ./test/kernel/fs/xfs/xfs_file.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
for read and write IO paths to ensure we consistently use 
if (type & XFS_IOLOCK_EXCL) 
if (type & XFS_IOLOCK_EXCL) 
if (type & XFS_IOLOCK_EXCL) 
ified range of buffer supplied, 
fore the operation, it will be read from disk before 
if (bytes > count) 
if (status) 
while (count); 
for explicit 
if (xfs_ipincount(ip)) 
if (!lsn) 
force_lsn(mp, lsn, XFS_LOG_SYNC, NULL); 
if (error) 
if (XFS_FORCED_SHUTDOWN(mp)) 
iflags_clear(ip, XFS_ITRUNCATED); 
for file data 
fore logging the new inode size in case of 
if (XFS_IS_REALTIME_INODE(ip)) 
if (mp->m_logdev_targp != mp->m_ddev_targp) 
if (xfs_ipincount(ip)) { 
if (lsn) 
force_lsn(mp, lsn, XFS_LOG_SYNC, &log_flushed); 
force about was 
if we were overwriting 
if ((mp->m_flags & XFS_MOUNT_BARRIER) && 
if (unlikely(file->f_flags & O_DIRECT)) 
if (file->f_mode & FMODE_NOCMTIME) 
if (unlikely(ioflags & IO_ISDIRECT)) { 
if ((pos | size) & target->bt_logical_sectormask) { 
if (n <= 0 || size == 0) 
if (n < size) 
if (XFS_FORCED_SHUTDOWN(mp)) 
for direct IO, we effectively serialise all new concurrent 
if the page cache needs invalidation. 
if ((ioflags & IO_ISDIRECT) && inode->i_mapping->nrpages) { 
if (inode->i_mapping->nrpages) { 
if (ret) { 
if (ret > 0) 
if (infilp->f_mode & FMODE_NOCMTIME) 
if (XFS_FORCED_SHUTDOWN(ip->i_mount)) 
if (ret > 0) 
if (error) 
if (imap.br_startblock == HOLESTARTBLOCK) 
if (isize + zero_len > offset) 
fore the inode size was updated but after blocks were 
if (XFS_B_FSB_OFFSET(mp, isize) != 0) { 
if (error) 
if (last_fsb == end_zero_fsb) { 
while (start_zero_fsb <= end_zero_fsb) { 
if (error) 
if (imap.br_state == XFS_EXT_UNWRITTEN || 
if ((zero_off + zero_len) > offset) 
if (error) 
if called for a direct write beyond i_size. 
if (error) 
fore. 
if (*pos > i_size_read(inode)) { 
if (error) 
if (likely(!(file->f_mode & FMODE_NOCMTIME))) { 
if (error) 
if the process is not being run by root.  This keeps 
for and issue a direct IO write. 
if the IO is not aligned to filesystem blocks, the direct IO layer 
for 
fore we try to map the overlapping block. This is currently implemented by 
if ((pos | count) & target->bt_logical_sectormask) 
if ((pos & mp->m_blockmask) || ((pos + count) & mp->m_blockmask)) 
for 
if (unaligned_io || mapping->nrpages) 
if there are cached pages that need invalidate after we got 
while 
if (mapping->nrpages && iolock == XFS_IOLOCK_SHARED) { 
if (ret) 
if (mapping->nrpages) { 
if (ret) 
for all other IO to drain, 
if (unaligned_io) 
if (iolock == XFS_IOLOCK_EXCL) { 
for XFS. */ 
if (ret) 
form_write(file, from, pos); 
if (ret == -ENOSPC && !enospc) { 
if (ocount == 0) 
if (XFS_FORCED_SHUTDOWN(ip->i_mount)) 
if (unlikely(file->f_flags & O_DIRECT)) 
if (ret > 0) { 
if (err < 0) 
if (!S_ISREG(inode->i_mode)) 
if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE | 
if (mode & FALLOC_FL_PUNCH_HOLE) { 
if (error) 
if (mode & FALLOC_FL_COLLAPSE_RANGE) { 
if (offset & blksize_mask || len & blksize_mask) { 
if (offset + len >= i_size_read(inode)) { 
if (error) 
if (!(mode & FALLOC_FL_KEEP_SIZE) && 
if (error) 
if (mode & FALLOC_FL_ZERO_RANGE) 
if (error) 
if (error) { 
if (ip->i_d.di_mode & S_IXGRP) 
if (!(mode & (FALLOC_FL_PUNCH_HOLE | FALLOC_FL_COLLAPSE_RANGE))) 
if (file->f_flags & O_DSYNC) 
if (error) 
if needed */ 
if (!(file->f_flags & O_LARGEFILE) && i_size_read(inode) > MAX_NON_LFS) 
if (XFS_FORCED_SHUTDOWN(XFS_M(inode->i_sb))) 
if (error) 
if (ip->i_d.di_nextents > 0) 
for correct information, but the XFS dir2 leaf 
for mapping to 
if (error) 
for a writable 
for either xfs_seek_data() or xfs_seek_hole(). 
ified by the 
if (buffer_unwritten(bh) || 
if (type == DATA_OFF) 
if (type == HOLE_OFF) 
if (found) { 
while ((bh = bh->b_this_page) != head); 
for unwritten extents according to the desired 
if the desired type of offset was found, and the argument 
if this is the first time we got into the loop, it means 
if it does not reach the endpoint to search, 
if (nr_pages == 0) { 
if (type == DATA_OFF) 
if (lastoff == startoff || lastoff < endoff) { 
if the first page index offset is 
if (type == HOLE_OFF && lastoff == startoff && 
for (i = 0; i < nr_pages; i++) { 
if the page index is out of range. 
ified search range, there should be a hole 
if (page->index > end) { 
if (unlikely(page->mapping != inode->i_mapping)) { 
if (!page_has_buffers(page)) { 
if (found) { 
if this is the first time to 
for searching data, 
if (nr_pages < want) { 
while (index <= end); 
if (start >= isize) { 
for (;;) { 
if (error) 
if (nmap == 0) { 
for (i = 0; i < nmap; i++) { 
if (map[i].br_startblock == DELAYSTARTBLOCK || 
if (map[i].br_state == XFS_EXT_UNWRITTEN) { 
if nothing in map[1]. 
if (nmap == 1) { 
if reading offset not beyond or hit EOF. 
if (start >= isize) { 
if (error) 
if (XFS_FORCED_SHUTDOWN(mp)) 
if (start >= isize) { 
for (;;) { 
if (error) 
if (nmap == 0) { 
for (i = 0; i < nmap; i++) { 
if (map[i].br_startblock == HOLESTARTBLOCK) 
if (map[i].br_state == XFS_EXT_UNWRITTEN) { 
if (nmap == 1) { 
if the current reading offset not beyond or hit EOF. 
if (start >= isize) { 
for unwritten extents, we need to deal with this 
if (error) 
ifdef CONFIG_COMPAT 
if 
ifdef CONFIG_COMPAT 
if 
file : ./test/kernel/fs/xfs/xfs_log_rlimit.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
for a local 
for (; resp < end_resp; resp++) { 
if (log_space < tmp) { 
if (attr_space > log_space) { 
for the given superblock configuration. 
if (tres.tr_logcount > 1) 
if (xfs_sb_version_haslogv2(&mp->m_sb) && mp->m_sb.sb_logsunit > 1) 
for calculating the minimum 
ified, a transaction requires 2 LSU 
for the reservation because there are two log writes that can 
if we 
for at one new transaction, which 
if lsunit is specified. 
if (lsunit) { 
file : ./test/kernel/fs/xfs/xfs_filestream.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
for this directory */ 
for all 
for deletion, then gets pre-empted. 
for the element 
for an AG that now has no references. 
for an AG that isn't in use and has 
for it to be chosen. */ 
for (nscan = 0; 1; nscan++) { 
if (!pag->pagf_init) { 
if (err && !trylock) { 
if (!pag->pagf_init) 
if (pag->pagf_freeblks > maxfree) { 
forces mutual 
if (xfs_filestream_get_ag(mp, ag) > 1) { 
if (((minlen && longest >= minlen) || 
if necessary. */ 
if (ag != startag) 
if (trylock != 0) { 
if lowspace wasn't set, set it for the 3rd pass. */ 
if (max_ag != NULLAGNUMBER) { 
if none matched */ 
if (*agp == NULLAGNUMBER) 
if (!item) 
if (err) { 
if (!dentry) 
if (!parent) 
for a file, either by finding an 
if (!pip) 
if (mru) { 
for inode32, otherwise 
if (mp->m_flags & XFS_MOUNT_32BITINODES) { 
if (xfs_filestream_pick_ag(pip, startag, &ag, 0, 0)) 
for the current file and its file stream. 
if (!pip) 
if (mru) { 
if (mru) 
if (*agp == NULLAGNUMBER) 
file : ./test/kernel/fs/xfs/xfs_bmap_btree.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
if (extent_flag) { 
for DMIG */ 
form of btree root to in-memory form. 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
form. 
if XFS_BIG_BLKNOS 
ifdef DEBUG 
if	/* DEBUG */ 
if (ext_flag) { 
for DMIG */ 
if XFS_BIG_BLKNOS 
ifdef DEBUG 
if	/* DEBUG */ 
format bmap extent record. 
if XFS_BIG_BLKNOS 
if (isnullstartblock(startblock)) { 
if	/* XFS_BIG_BLKNOS */ 
form. 
format bmap extent record from the arguments. 
if XFS_BIG_BLKNOS 
if (isnullstartblock(startblock)) { 
if	/* XFS_BIG_BLKNOS */ 
form. 
if XFS_BIG_BLKNOS 
if (isnullstartblock(v)) { 
if	/* XFS_BIG_BLKNOS */ 
if (v == XFS_EXT_NORM) 
form of btree root to on-disk form. 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
for 
if any flags found, 
ifork_t		*ifp, 
for (; num > 0; num--, idx++) { 
if ((ep->l0 >> 
fork); 
if (args.fsbno == NULLFSBLOCK) { 
for an extent insert.  If 
for two tree splits. 
for this operation to succeed.  If the 
if (cur->bc_private.b.flist->xbf_low) { 
if (!args.wasdel && xfs_trans_get_block_res(args.tp) == 0) { 
if (error) 
if (args.fsbno == NULLFSBLOCK && args.minleft) { 
if 
if (error) 
if (args.fsbno == NULLFSBLOCK) { 
if (level == cur->bc_nlevels - 1) { 
fork	*ifp; 
fork); 
ifp->if_broot_bytes, level == 0) / 2; 
if (level == cur->bc_nlevels - 1) { 
fork	*ifp; 
fork); 
ifp->if_broot_bytes, level == 0); 
format. 
for the root node this checks the available space in the dinode fork 
for the root node, too. 
if (level != cur->bc_nlevels - 1) 
forksize, level == 0); 
iff( 
ify( 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (!uuid_equal(&block->bb_u.l.bb_uuid, &mp->m_sb.sb_uuid)) 
if (be64_to_cpu(block->bb_u.l.bb_blkno) != bp->b_bn) 
ifying the owner here. Right now 
if (be64_to_cpu(block->bb_u.l.bb_owner) == 0) 
ification. 
ify that the level 
if (level > max(mp->m_bm_maxlevels[0], mp->m_bm_maxlevels[1])) 
if (be16_to_cpu(block->bb_numrecs) > mp->m_bmap_dmxr[level != 0]) 
ification */ 
if (!block->bb_u.l.bb_rightsib || 
ify( 
if (!xfs_btree_lblock_verify_crc(bp)) 
if (!xfs_bmbt_verify(bp)) 
if (bp->b_error) { 
ifier_error(bp); 
ify( 
if (!xfs_bmbt_verify(bp)) { 
ifier_error(bp); 
ify_read = xfs_bmbt_read_verify, 
if defined(DEBUG) || defined(XFS_WARN) 
if	/* DEBUG */ 
iff		= xfs_bmbt_key_diff, 
if defined(DEBUG) || defined(XFS_WARN) 
if 
fork)	/* data or attr fork */ 
ifp->if_broot->bb_level) + 1; 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
forksize = XFS_IFORK_SIZE(ip, whichfork); 
fork = whichfork; 
if (leaf) 
if (leaf) 
format fork fo the inode passed in. Change it to 
forks between inodes. The operation that the caller is doing will 
ification, the fork switch should be done 
ification when the buffers are already pinned in memory, 
fork switch can be done before changing the owner as we won't need to 
ified for the caller to issue IO on. 
fork, 
if (whichfork == XFS_DATA_FORK) 
format == XFS_DINODE_FMT_BTREE); 
format == XFS_DINODE_FMT_BTREE); 
if (!cur) 
file : ./test/kernel/fs/xfs/xfs_ialloc_btree.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
if (error) { 
if (args.fsbno == NULLFSBLOCK) { 
if (error) 
for lookup 
iff( 
ify( 
ify the exact owner as the 
formation will not yet have been initialised 
if we 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (!uuid_equal(&block->bb_u.s.bb_uuid, &mp->m_sb.sb_uuid)) 
if (block->bb_u.s.bb_blkno != cpu_to_be64(bp->b_bn)) 
if (pag && 
ification */ 
if (level >= mp->m_in_maxlevels) 
if (be16_to_cpu(block->bb_numrecs) > mp->m_inobt_mxr[level != 0]) 
ification */ 
if (!block->bb_u.s.bb_rightsib || 
ify( 
if (!xfs_btree_sblock_verify_crc(bp)) 
if (!xfs_inobt_verify(bp)) 
if (bp->b_error) { 
ifier_error(bp); 
ify( 
if (!xfs_inobt_verify(bp)) { 
ifier_error(bp); 
ify_read = xfs_inobt_read_verify, 
if defined(DEBUG) || defined(XFS_WARN) 
if	/* DEBUG */ 
iff		= xfs_inobt_key_diff, 
if defined(DEBUG) || defined(XFS_WARN) 
if 
iff		= xfs_inobt_key_diff, 
if defined(DEBUG) || defined(XFS_WARN) 
if 
for agi structure */ 
if (btnum == XFS_BTNUM_INO) { 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
if (leaf) 
file : ./test/kernel/fs/xfs/xfs_inode_buf.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
if defined(DEBUG) 
for (i = 0; i < j; i++) { 
if (!dip->di_next_unlinked)  { 
if 
for readahead, the buffer 
ify( 
for (i = 0; i < ni; i++) { 
if (unlikely(XFS_TEST_ERROR(!di_ok, mp, 
if (readahead) { 
ifier_error(bp); 
if 
ify( 
ify(bp, false); 
ify( 
ify(bp, true); 
ify( 
ify(bp, false); 
ify_read = xfs_inode_buf_read_verify, 
ify_read = xfs_inode_buf_readahead_verify, 
if (error) { 
if (error == EFSCORRUPTED && 
format = from->di_format; 
forkoff = from->di_forkoff; 
if (to->di_version == 3) { 
format = from->di_format; 
forkoff = from->di_forkoff; 
if (from->di_version == 3) { 
ify( 
if (dip->di_magic != cpu_to_be16(XFS_DINODE_MAGIC)) 
ified here */ 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (!xfs_verify_cksum((char *)dip, mp->m_sb.sb_inodesize, 
if (be64_to_cpu(dip->di_ino) != ip->i_ino) 
if (!uuid_equal(&dip->di_uuid, &mp->m_sb.sb_uuid)) 
if (dip->di_version < 3) 
if we are initialising a new inode and we are not 
if we are using version 4 superblocks (i.e. v1/v2 inode 
format) then log recovery is dependent on the di_flushiter field being 
formation in the in-core inode. 
if (error) 
if possible */ 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
if (error) 
ified */ 
for inode %lld failed", 
iformat_fork() handles copying in the inode format 
formation. 
if (dip->di_mode) { 
iformat_fork(ip, dip); 
ifdef DEBUG 
format() returned error %d", 
if /* DEBUG */ 
if (dip->di_version == 3) { 
formats in memory to version 2 
for v2 inodes unconditionally during mount 
if (ip->i_d.di_version == 1) { 
for a while.  This helps to keep recently accessed 
if it is not dirty within the 
fore putting it in the cache where other 
file : ./test/kernel/fs/xfs/xfs_da_format.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
form directory ops 
form directories, the file type field is stored at 
if (ftype >= XFS_DIR3_FT_MAX) 
form directories can come in two versions, 
forms transparently by looking at the headers i8count field. 
if (hdr->i8count) 
if (hdr->i8count) 
form directory entries the inode numbers are stored at variable 
for the "." and "..", and 
for dirents 
if (ftype >= XFS_DIR3_FT_MAX) 
forw = be32_to_cpu(from->hdr.info.forw); 
forw = cpu_to_be32(from->forw); 
forw = be32_to_cpu(hdr3->info.hdr.forw); 
forw = cpu_to_be32(from->forw); 
forw = be32_to_cpu(from->hdr.info.forw); 
forw = cpu_to_be32(from->forw); 
forw = be32_to_cpu(hdr3->info.hdr.forw); 
forw = cpu_to_be32(from->forw); 
if (dp) 
if (mp->m_dir_inode_ops) 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
if (xfs_sb_version_hasftype(&mp->m_sb)) 
if (dp) 
if (mp->m_nondir_inode_ops) 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
file : ./test/kernel/fs/xfs/xfs_iops.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
ifferent lock order w.r.t. mmap_sem compared to regular 
ifferent class for the directory ilock so 
for (xattr = xattr_array; xattr->name != NULL; xattr++) { 
if (error < 0) 
for default ACLs) is a mechanism by which creation of 
if (S_ISCHR(mode) || S_ISBLK(mode)) { 
if (error) 
if (!tmpfile) { 
if (unlikely(error)) 
if (unlikely(error)) 
ifdef CONFIG_XFS_POSIX_ACL 
if (error) 
if (acl) { 
if (error) 
if 
if (default_acl) 
if (acl) 
if (!tmpfile) 
if (dentry->d_name.len >= MAXNAMELEN) 
if (unlikely(error)) { 
if (dentry->d_name.len >= MAXNAMELEN) 
if (unlikely(error)) { 
if exact match, just splice and exit */ 
if (unlikely(error)) 
if (error) 
if (xfs_sb_version_hasasciici(&XFS_M(dir->i_sb)->m_sb)) 
if (unlikely(error)) 
if (unlikely(error)) 
for this reason... 
if (!link) 
if (unlikely(error)) 
if (XFS_FORCED_SHUTDOWN(mp)) 
if_u2.if_rdev) & 0x1ff, 
if (XFS_IS_REALTIME_INODE(ip)) { 
if (iattr->ia_valid & ATTR_ATIME) { 
if (iattr->ia_valid & ATTR_CTIME) { 
if (iattr->ia_valid & ATTR_MTIME) { 
if (!(flags & XFS_ATTR_NOACL)) { 
if (XFS_FORCED_SHUTDOWN(mp)) 
if (error) 
fore we start any other transactions. Trying to do this later 
fore we take the ilock, we're covered 
if (XFS_IS_QUOTA_ON(mp) && (mask & (ATTR_UID|ATTR_GID))) { 
if ((mask & ATTR_UID) && XFS_IS_UQUOTA_ON(mp)) { 
if ((mask & ATTR_GID) && XFS_IS_GQUOTA_ON(mp)) { 
for an example. 
if (error) 
if (error) 
if (mask & (ATTR_UID|ATTR_GID)) { 
if the ownership did change 
while we didn't have the inode locked, inode's dquot(s) 
if uid/gid is actually 
if (XFS_IS_QUOTA_RUNNING(mp) && 
if (error)	/* out of quota */ 
if (mask & (ATTR_UID|ATTR_GID)) { 
if ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) && 
ifications 
if (!uid_eq(iuid, uid)) { 
if (!gid_eq(igid, gid)) { 
if (mask & ATTR_MODE) 
if (mask & (ATTR_ATIME|ATTR_CTIME|ATTR_MTIME)) 
if (mp->m_flags & XFS_MOUNT_WSYNC) 
fore chown. 
if (error) 
if ((mask & ATTR_MODE) && !(flags & XFS_ATTR_NOACL)) { 
if (error) 
if (mp->m_flags & XFS_MOUNT_RDONLY) 
if (XFS_FORCED_SHUTDOWN(mp)) 
if (error) 
for zero length files. 
if (newsize == 0 && oldsize == 0 && ip->i_d.di_nextents == 0) { 
if (error) 
fore we join the inode to the 
fore joining 
if (newsize > oldsize) { 
ify 
if (error) 
for other data not within the range we 
if (oldsize != ip->i_d.di_size && newsize > ip->i_d.di_size) { 
if (error) 
for all direct I/O to complete. 
fore we truncate 
if (error) 
if (error) 
if we are changing the size or we are 
if it wants a timestamp update. 
if (newsize != oldsize && 
if 
if (newsize <= oldsize) { 
if (error) 
if we delay flushing for a long time, we expose 
for writeout. 
iflags_set(ip, XFS_ITRUNCATED); 
if (iattr->ia_valid & ATTR_MODE) 
if (iattr->ia_valid & (ATTR_ATIME|ATTR_CTIME|ATTR_MTIME)) 
if (mp->m_flags & XFS_MOUNT_WSYNC) 
if (lock_flags) 
if (iattr->ia_valid & ATTR_SIZE) { 
if (error) { 
if (flags & S_CTIME) { 
if (flags & S_MTIME) { 
if (flags & S_ATIME) { 
format( 
for a hole */ 
if (bmv->bmv_oflags & BMV_OF_PREALLOC) 
if (bmv->bmv_oflags & BMV_OF_DELALLOC) { 
if (bmv->bmv_oflags & BMV_OF_LAST) 
if (error > 0) { 
if (error) 
for xfs internal routine */ 
for whole file */ 
iflags = BMV_IF_PREALLOC | BMV_IF_NO_HOLES; 
iflags |= BMV_IF_ATTRFORK; 
iflags |= BMV_IF_DELALLOC; 
format, fieinfo); 
for rmdir and unlink. 
ifferences deeper in the code, 
for those. 
for rmdir and unlink. 
ifferences deeper in the code, 
for those. 
iflags_to_iflags( 
if (ip->i_d.di_flags & XFS_DIFLAG_IMMUTABLE) 
if (ip->i_d.di_flags & XFS_DIFLAG_APPEND) 
if (ip->i_d.di_flags & XFS_DIFLAG_SYNC) 
if (ip->i_d.di_flags & XFS_DIFLAG_NOATIME) 
for the writeback code */ 
if_u2.if_rdev) & 0x1ff, 
iflags_to_iflags(inode, ip); 
if (xfs_sb_version_hasasciici(&XFS_M(inode->i_sb)->m_sb)) 
if (!(ip->i_df.if_flags & XFS_IFINLINE)) 
fork no ACL can exist on this inode, 
if (!XFS_IFORK_Q(ip)) { 
iflags_clear(ip, XFS_INEW); 
file : ./test/kernel/fs/xfs/xfs_dir2_block.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
ify( 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
if (!uuid_equal(&hdr3->uuid, &mp->m_sb.sb_uuid)) 
if (be64_to_cpu(hdr3->blkno) != bp->b_bn) 
if (hdr3->magic != cpu_to_be32(XFS_DIR2_BLOCK_MAGIC)) 
if (__xfs_dir3_data_check(NULL, bp)) 
ify( 
if (xfs_sb_version_hascrc(&mp->m_sb) && 
if (!xfs_dir3_block_verify(bp)) 
if (bp->b_error) 
ify( 
if (!xfs_dir3_block_verify(bp)) { 
ifier_error(bp); 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (bip) 
ify_read = xfs_dir3_block_read_verify, 
if (!err && tp) 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
for the leaf. 
if (btp->stale) { 
fore the first leaf entry. 
fore the first leaf entry.  */ 
if it works at all. 
if (be16_to_cpu(dup->freetag) == XFS_DIR2_DATA_FREE_TAG) { 
if ((be32_to_cpu(btp->stale) - 1) * (uint)sizeof(*blp) < len) 
fore the first leaf entry. 
fore the first leaf entry.  */ 
fore the first leaf entry needs to be free so it 
if (be16_to_cpu(enddup->freetag) == XFS_DIR2_DATA_FREE_TAG) { 
if it's the same one. 
if (dup != enddup) { 
if (be16_to_cpu(dup->length) < len) 
if (be16_to_cpu(dup->length) < len + (uint)sizeof(*blp)) { 
if it works. 
if (be16_to_cpu(bf[1].length) >= len) 
for (; fromidx >= 0; fromidx--) { 
if (highstale == -1) 
if (*lfloghigh == -1) 
if (fromidx < toidx) 
fore the next call to use_free. 
if (needscan) 
for block */ 
for binary srch */ 
for binary srch */ 
for binary srch */ 
if (error) 
if we can reuse stale entries or whether we need extra 
for entry and new leaf. 
for a space check now. 
if (args->op_flags & XFS_DA_OP_JUSTCHECK) { 
if (!dup) 
for the new entry & leaf ... 
if (!dup) { 
if (args->total == 0) 
format. 
if (error) 
if (compact) { 
if (btp->stale) { 
if none. 
for (low = 0, high = be32_to_cpu(btp->count) - 1; low <= high; ) { 
if ((hash = be32_to_cpu(blp[mid].hashval)) == args->hashval) 
if (hash < args->hashval) 
while (mid >= 0 && be32_to_cpu(blp[mid].hashval) >= args->hashval) { 
if (!btp->stale) { 
for the new leaf entry, now in use. 
fore the next call to use_free. 
if (needscan) { 
for the new leaf entry. 
if (mid) 
for our new entry. 
for (lowstale = mid; 
for (highstale = mid + 1; 
if (lowstale >= 0 && 
if (mid - lowstale) 
if (highstale - mid) 
for the data entry used. 
if (needscan) 
if (needlog) 
if ((error = xfs_dir2_block_lookup_int(args, &bp, &ent))) 
if appropriate, release the block. 
if (error) 
for our hash value. 
for (low = 0, high = be32_to_cpu(btp->count) - 1; ; ) { 
if ((hash = be32_to_cpu(blp[mid].hashval)) == args->hashval) 
if (hash < args->hashval) 
if (low > high) { 
while (mid > 0 && be32_to_cpu(blp[mid - 1].hashval) == args->hashval) { 
forward through all the entries with the 
if ((addr = be32_to_cpu(blp[mid].address)) == XFS_DIR2_NULL_DATAPTR) 
if it's an exact match, return the index 
for an exact match. 
if (cmp != XFS_CMP_DIFFERENT && cmp != args->cmpresult) { 
if (cmp == XFS_CMP_EXACT) 
while (++mid < be32_to_cpu(btp->count) && 
if (args->cmpresult == XFS_CMP_CASE) 
format directory. 
form header */ 
if ((error = xfs_dir2_block_lookup_int(args, &bp, &ent))) { 
if necessary. 
if (needscan) 
if (needlog) 
if the size as a shortform is good enough. 
if (size > XFS_IFORK_DSIZE(dp)) 
if ((error = xfs_dir2_block_lookup_int(args, &bp, &ent))) { 
for the block leaf entries. 
if possible. 
for bestfree */ 
while (dp->i_d.di_size > args->geo->blksize) { 
if (be16_to_cpu(bestsp[be32_to_cpu(ltp->bestcount) - 1]) == 
if ((error = 
if we don't already have it, give up if it fails. 
if (!dbp) { 
if (error) 
if (be16_to_cpu(dup->freetag) != XFS_DIR2_DATA_FREE_TAG || 
form. 
for (from = to = 0; from < leafhdr.count; from++) { 
if we need it and log the data block header. 
if (needscan) 
if (needlog) 
if (error) 
if the resulting block can be shrunken to shortform. 
if (size > XFS_IFORK_DSIZE(dp)) 
form directory to block form. 
form header  */ 
ifork	*ifp; 
ifp = XFS_IFORK_PTR(dp, XFS_DATA_FORK); 
if the shortform directory is way too short. 
if (dp->i_d.di_size < offsetof(xfs_dir2_sf_hdr_t, parent)) { 
ifp->if_u1.if_data; 
ifp->if_u1.if_data != NULL); 
ifp->if_bytes, KM_SLEEP); 
ifp->if_bytes, XFS_DATA_FORK); 
if (error) { 
format. 
if (error) { 
for . 
for .. 
if (!sfp->count) 
while (offset < endoffset) { 
if (sfep == NULL) 
if (offset < newoffset) { 
if (++i == sfp->count) 
file : ./test/kernel/fs/xfs/xfs_sb.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
ifree),		0 }, 
if (pag) { 
if (found <= 0) { 
if (sbp->sb_magicnum != XFS_SB_MAGIC) { 
if (!xfs_sb_good_version(sbp)) { 
fore checking anything else. For 
if (check_version && XFS_SB_VERSION_NUM(sbp) == XFS_SB_VERSION_5) { 
if (xfs_sb_has_ro_compat_feature(sbp, 
if (!(mp->m_flags & XFS_MOUNT_RDONLY)) { 
if (xfs_sb_has_incompat_feature(sbp, 
if (xfs_sb_version_has_pquotino(sbp)) { 
if (sbp->sb_qflags & (XFS_PQUOTA_ENFD | XFS_GQUOTA_ENFD | 
if (unlikely( 
ify logdev on the mount command line."); 
if (unlikely( 
ify logdev on the mount command line."); 
if (unlikely( 
if (unlikely(sbp->sb_blocksize > PAGE_SIZE)) { 
if (xfs_sb_validate_fsb_count(sbp, sbp->sb_dblocks) || 
if (check_inprogress && sbp->sb_inprogress) { 
ifferent values for a quota 
formation 
if (sbp->sb_uquotino == 0) 
if (sbp->sb_gquotino == 0) 
if (sbp->sb_pquotino == 0) 
if we are working 
if (xfs_sb_version_has_pquotino(sbp)) 
if (sbp->sb_qflags & XFS_OQUOTA_ENFD) 
if (sbp->sb_qflags & XFS_OQUOTA_CHKD) 
if (sbp->sb_qflags & XFS_PQUOTA_ACCT)  { 
if PQUOTA is set in disk superblock, 
ifree = be64_to_cpu(from->sb_ifree); 
if we are working 
if (xfs_sb_version_has_pquotino(from)) 
if (*fields & XFS_SB_QFLAGS) { 
if (from->sb_qflags & 
if (from->sb_qflags & 
if ((*fields & XFS_SB_GQUOTINO) && 
if ((*fields & XFS_SB_PQUOTINO) && 
if (!fields) 
while (fields) { 
if (size == 1 || xfs_sb_info[f].type == 1) { 
ify( 
for the primary superblock as 
for secondary superblocks, 
ify( 
if (dsb->sb_magicnum == cpu_to_be32(XFS_SB_MAGIC) && 
if (!xfs_buf_verify_cksum(bp, XFS_SB_CRC_OFF)) { 
if (bp->b_bn == XFS_SB_DADDR || 
ify(bp, true); 
if (error) { 
if (error == EFSCORRUPTED || error == EFSBADCRC) 
for a filesystem match, so we may not want to emit 
ify( 
if (dsb->sb_magicnum == cpu_to_be32(XFS_SB_MAGIC)) { 
ify(bp); 
ify( 
ify(bp, false); 
ifier_error(bp); 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (bip) 
ify_read = xfs_sb_read_verify, 
ify_read = xfs_sb_quiet_read_verify, 
formation is no longer persistent in the superblock. Once we have 
ifree = 0; 
for (index = 0; index < agcount; index++) { 
formation we need and populates the 
if (error) 
if (error) 
ifree += pag->pagi_freecount; 
ifree = ifree; 
if (!fields) 
ified range */ 
file : ./test/kernel/fs/xfs/xfs_bmap_util.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
ifferently based on whether the file is a real time file or not, because the 
if the given transaction was committed and a new one 
if (flist->xbf_count == 0) { 
for (free = flist->xbf_first; free; free = free->xbfi_next) 
if (error) 
if (error) 
for (free = flist->xbf_first; free != NULL; free = next) { 
if ((error = xfs_free_extent(ntp, free->xbfi_startblock, 
force shutdown here to make sure it 
if (!XFS_FORCED_SHUTDOWN(mp)) 
force_shutdown(mp, 
for allocation routines */ 
for allocators */ 
if (error) 
if (do_mod(ap->offset, align) || ap->length % align) 
if it's a really large request (bigger than 
if (ralen * mp->m_sb.sb_rextsize >= MAXEXTLEN) 
ifications to the RT bitmap inode. 
if (ap->eof && ap->offset == 0) { 
if (error) 
if ((error = xfs_rtallocate_extent(ap->tp, ap->blkno, 1, ap->length, 
if (rtb == NULLFSBLOCK && prod > 1 && 
if (ap->blkno != NULLFSBLOCK) { 
if (ap->wasdel) 
for allocation 
for memory reclaim 
if (args->kswapd) 
if (!args->stack_switch) 
for_completion(&done); 
if the endoff is outside the last extent. If so the caller will grow 
for an empty fork, so 1 is returned in *eof in that case. 
fork, 
fork, &rec, eof); 
ifork_t		*ifp, 
for (b = 0; b < numrecs; b++) { 
format. 
for (b = 1; b <= numrecs; b++) { 
ifork_t	*ifp,		/* inode fork pointer */ 
if (error) 
if (--level) { 
while (nextbno != NULLFSBLOCK) { 
if (error) 
if (unlikely((error = 
for (;;) { 
if (nextbno == NULLFSBLOCK) 
if (error) 
fork. 
fork,	/* data or attr fork */ 
ifork_t		*ifp;	/* fork structure */ 
for checking */ 
ifp = XFS_IFORK_PTR(ip, whichfork); 
fork) == XFS_DINODE_FMT_EXTENTS ) { 
ifp->if_bytes / (uint)sizeof(xfs_bmbt_rec_t), 
ifp->if_broot; 
ifp->if_broot_bytes); 
if (unlikely(xfs_bmap_count_tree(mp, tp, ifp, bno, level, count) < 0)) { 
if we failed to map the extent. 
ifork_t		*ifp;		/* inode fork pointer */ 
if (startblock == HOLESTARTBLOCK) { 
if (prealloced && out->bmv_offset + out->bmv_length == end) { 
if (fixlen <= 0) 
if (startblock == DELAYSTARTBLOCK) 
ifp = XFS_IFORK_PTR(ip, XFS_DATA_FORK); 
ifp->if_bytes / (uint)sizeof(xfs_bmbt_rec_t))-1)) 
format for output. 
formatter short-circuits the loop, 
format_t	formatter,	/* format to user */ 
for -1 case */ 
for user's data */ 
fork;	/* data or attr fork */ 
iflags;		/* interface flags */ 
for xfs_bmapi */ 
iflags = bmv->bmv_iflags; 
fork = iflags & BMV_IF_ATTRFORK ? XFS_ATTR_FORK : XFS_DATA_FORK; 
if (XFS_IFORK_Q(ip)) { 
format != XFS_DINODE_FMT_EXTENTS && 
format != XFS_DINODE_FMT_LOCAL) 
if (unlikely( 
format != 0 && 
if (ip->i_d.di_format != XFS_DINODE_FMT_EXTENTS && 
format != XFS_DINODE_FMT_BTREE && 
if (xfs_get_extsz_hint(ip) || 
if (bmv->bmv_length == -1) { 
if (bmv->bmv_length == 0) { 
if (bmv->bmv_length < 0) { 
if (nex <= 0) 
if (bmv->bmv_count > ULONG_MAX / sizeof(struct getbmapx)) 
if (!out) 
if (whichfork == XFS_DATA_FORK) { 
if (error) 
if (nex > XFS_IFORK_NEXTENTS(ip, whichfork) * 2 + 1) 
fork) * 2 + 1; 
if (!(iflags & BMV_IF_PREALLOC)) 
if (!map) 
if (XFS_IFORK_NEXTENTS(ip, whichfork) == 0 && 
fork == XFS_ATTR_FORK || !(iflags & BMV_IF_DELALLOC))) { 
if (error) 
for (i = 0; i < nmap && nexleft && bmv->bmv_length; i++) { 
if (map[i].br_state == XFS_EXT_UNWRITTEN) 
if (map[i].br_startblock == DELAYSTARTBLOCK) 
if we are not supposed to be finding delalloc 
if (map[i].br_startblock == DELAYSTARTBLOCK && 
iflags & BMV_IF_DELALLOC) != 0); 
fork == XFS_ATTR_FORK) { 
if (!xfs_getbmapx_fix_eof_hole(ip, &out[cur_ext], 
if ((iflags & BMV_IF_NO_HOLES) && 
while (nmap && nexleft && bmv->bmv_length); 
for (i = 0; i < cur_ext; i++) { 
format results & advance arg */ 
if (error || full) 
if the ranges only partially overlap 
fore trying to unmap the range. Otherwise we will be 
if (error) { 
if (!XFS_FORCED_SHUTDOWN(ip->i_mount)) { 
if (!nimaps) { 
if (imap.br_startblock != DELAYSTARTBLOCK) { 
while we initialise the firstblock/flist pair, they 
for a delalloc extent and hence we need 
if (error) 
while(remaining > 0); 
for and free post EOF 
force) 
if (!S_ISREG(ip->i_d.di_mode)) 
if (VFS_I(ip)->i_size == 0 && 
if (!(ip->i_df.if_flags & XFS_IFEXTENTS)) 
forced to remove them. 
if (ip->i_d.di_flags & (XFS_DIFLAG_PREALLOC | XFS_DIFLAG_APPEND)) 
force || ip->i_delayed_blks == 0) 
if there are any blocks beyond the end 
if (last_fsb <= end_fsb) 
if (!error && (nimaps != 0) && 
if (error) 
if (need_iolock) { 
if (error) { 
if (need_iolock) 
fore the 
if (error) { 
if (!error) 
if (need_iolock) 
if (XFS_FORCED_SHUTDOWN(mp)) 
if (error) 
if (len <= 0) 
while (allocatesize_fsb && !error) { 
for data/realtime. 
if (unlikely(extsz)) { 
if ((temp = do_mod(startoffset_fsb, extsz))) 
if ((temp = do_mod(e, extsz))) 
force the limit. 
if (unlikely(rt)) { 
for running out of space 
if (error) { 
if (error) 
if (error) { 
if (error) { 
if (error) { 
if (nimaps == 0) { 
if (startoff >= XFS_ISIZE(ip)) 
if (endoff > XFS_ISIZE(ip)) 
if (!bp) 
for (offset = startoff; offset <= endoff; offset = lastoffset + 1) { 
if (error || nimap < 1) 
if (lastoffset > endoff) 
if (imap.br_startblock == HOLESTARTBLOCK) 
if (imap.br_state == XFS_EXT_UNWRITTEN) 
if (XFS_FORCED_SHUTDOWN(mp)) { 
if (error) { 
if (XFS_FORCED_SHUTDOWN(mp)) { 
if (error) { 
if (error) 
if (len <= 0)	/* if nothing being freed */ 
for the completion of any pending DIOs */ 
if (error) 
for us. 
if (rt && !xfs_sb_version_hasextflgbit(&mp->m_sb)) { 
if (error) 
if (nimap && imap.br_startblock != HOLESTARTBLOCK) { 
if (mod) 
if (error) 
if (nimap && imap.br_startblock != HOLESTARTBLOCK) { 
if (mod && (mod != mp->m_sb.sb_rextsize)) 
if ((done = (endoffset_fsb <= startoffset_fsb))) 
if (offset < XFS_FSB_TO_B(mp, startoffset_fsb)) 
if (!error && 
while (!error && !done) { 
for running out of space 
if (error) { 
if (error) 
if (error) { 
if (error) { 
if (start_boundary < end_boundary - 1) { 
if (error) 
for the edges */ 
if (error) 
if (end_boundary != offset + len) 
ift extent for the given file. 
ift extent records to the left to cover a hole. 
ift_fsb; 
ift_fsb = XFS_B_TO_FSB(mp, len); 
if (error) 
while (!error && !done) { 
for transaction. 
if (error) { 
if (error) 
ift_extents(tp, ip, &done, start_fsb, 
if (error) 
if (error) 
format of the data fork in the temporary inode is 
fork offset, but attr2 has a dynamically sized 
formats on the target inode. 
for 6.  If we defragment down to 7 extents, then the tmp format is a 
forks on attr2 filesystems. 
fork in the source 
format( 
format */ 
format == XFS_DINODE_FMT_LOCAL) 
if the target inode has less extents that then temporary inode then 
if (ip->i_d.di_nextents < tip->i_d.di_nextents) 
if the target inode is in extent form and the temp inode is in btree 
form then we will end up with the target inode in the wrong format 
if (ip->i_d.di_format == XFS_DINODE_FMT_EXTENTS && 
format == XFS_DINODE_FMT_BTREE) 
form to max in target */ 
form to max in temp */ 
format, check that the temp root block will fit 
format... 
if (tip->i_d.di_format == XFS_DINODE_FMT_BTREE) { 
if_broot) > XFS_IFORK_BOFF(ip)) 
if (XFS_IFORK_NEXTENTS(tip, XFS_DATA_FORK) <= 
format checks */ 
if (XFS_IFORK_BOFF(tip) && 
if (XFS_IFORK_NEXTENTS(ip, XFS_DATA_FORK) <= 
ifork_t	*tempifp, *ifp, *tifp; 
forkblks = 0; 
ifp = kmem_alloc(sizeof(xfs_ifork_t), KM_MAYFAIL); 
ify that both files have the same format */ 
ify both files are either real-time or non-realtime */ 
if (error) 
ify O_DIRECT for ftmp */ 
ify all data are being swapped */ 
fore(ip, 0); 
formats now that data is flushed */ 
if (error) { 
format is incompatible for exchanging.", 
ify times with that 
if ((sbp->bs_ctime.tv_sec != VFS_I(ip)->i_ctime.tv_sec) || 
if the file is memory mapped.  Once we have tossed 
for pages. By making the page fault call 
if (VN_MAPPED(VFS_I(ip))) { 
fork will not change since 
if non-io related 
if (error) { 
if ( ((XFS_IFORK_Q(ip) != 0) && (ip->i_d.di_anextents > 0)) && 
format != XFS_DINODE_FMT_LOCAL)) { 
if (error) 
if ( ((XFS_IFORK_Q(tip) != 0) && (tip->i_d.di_anextents > 0)) && 
format != XFS_DINODE_FMT_LOCAL)) { 
forkblks); 
fore we've swapped the forks, lets set the owners of the forks 
forks. 
forks and log that, log 
if (ip->i_d.di_version == 3 && 
format == XFS_DINODE_FMT_BTREE) { 
if (error) 
if (tip->i_d.di_version == 3 && 
format == XFS_DINODE_FMT_BTREE) { 
if (error) 
forks of the inodes 
ifp = &ip->i_df; 
ifp = *ifp;	/* struct copy */ 
ifp = *tempifp;	/* struct copy */ 
forkblks + aforkblks; 
format; 
format = tmp; 
ified 
while defrag is in progress). In that case, we need to copy over the 
fork is truncated away when the 
format) { 
if (ip->i_d.di_nextents <= XFS_INLINE_EXTS) { 
ifp->if_u2.if_inline_ext; 
format) { 
if (tip->i_d.di_nextents <= XFS_INLINE_EXTS) { 
ifp->if_u2.if_inline_ext; 
fore returning to the user. 
if (mp->m_flags & XFS_MOUNT_WSYNC) 
ifp); 
file : ./test/kernel/fs/xfs/xfs_inode_fork.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
ifork_zone; 
format_local(xfs_inode_t *, xfs_dinode_t *, int, int); 
iformat_btree(xfs_inode_t *, xfs_dinode_t *, int); 
ifork_t		*ifp, 
for (i = 0; i < nrecs; i++) { 
if (fmt == XFS_EXTFMT_NOSTATE) 
ifp, nrecs, fmt) 
ific information from the 
if_rdev to the proper value.  For files, directories, 
format, only the root is immediately 
iformat_fork( 
form_t	*atp; 
if (unlikely(be32_to_cpu(dip->di_nextents) + 
iformat(1)", XFS_ERRLEVEL_LOW, 
if (unlikely(dip->di_forkoff > ip->i_mount->m_sb.sb_inodesize)) { 
forkoff = 0x%x.", 
forkoff); 
if (unlikely((ip->i_d.di_flags & XFS_DIFLAG_REALTIME) && 
iformat(realtime)", 
if (unlikely(dip->di_format != XFS_DINODE_FMT_DEV)) { 
format(3)", XFS_ERRLEVEL_LOW, 
if_u2.if_rdev = xfs_dinode_get_rdev(dip); 
format) { 
if (unlikely(S_ISREG(be16_to_cpu(dip->di_mode)))) { 
format for regular file).", 
iformat(4)", 
if (unlikely(di_size < 0 || 
for local inode).", 
iformat(5)", 
iformat_local(ip, dip, XFS_DATA_FORK, size); 
iformat_extents(ip, dip, XFS_DATA_FORK); 
iformat_btree(ip, dip, XFS_DATA_FORK); 
iformat(6)", XFS_ERRLEVEL_LOW, 
iformat(7)", XFS_ERRLEVEL_LOW, ip->i_mount); 
if (error) { 
if (!XFS_DFORK_Q(dip)) 
ifork_zone, KM_SLEEP | KM_NOFS); 
format) { 
form_t *)XFS_DFORK_APTR(dip); 
if (unlikely(size < sizeof(struct xfs_attr_sf_hdr))) { 
fork size %Ld).", 
iformat(8)", 
iformat_local(ip, dip, XFS_ATTR_FORK, size); 
iformat_extents(ip, dip, XFS_ATTR_FORK); 
iformat_btree(ip, dip, XFS_ATTR_FORK); 
if (error) { 
fork_zone, ip->i_afp); 
fork(ip, XFS_DATA_FORK); 
if_inline_data, then copy 
for it 
if_data to point at the data. 
for the data, make 
iformat_local( 
fork, 
ifork_t	*ifp; 
if (unlikely(size > XFS_DFORK_SIZE(dip, ip->i_mount, whichfork))) { 
for local fork, size = %d).", 
fork)); 
ifp = XFS_IFORK_PTR(ip, whichfork); 
if (size == 0) 
if (size <= sizeof(ifp->if_u2.if_inline_data)) 
ifp->if_u1.if_data = kmem_alloc(real_size, KM_SLEEP | KM_NOFS); 
ifp->if_real_bytes = real_size; 
ifp->if_u1.if_data, XFS_DFORK_PTR(dip, whichfork), size); 
ifp->if_flags |= XFS_IFINLINE; 
if_inline_ext, then copy them there. 
for them and copy 
iformat_extents( 
fork) 
ifork_t	*ifp; 
ifp = XFS_IFORK_PTR(ip, whichfork); 
fork); 
if (unlikely(size < 0 || size > XFS_DFORK_SIZE(dip, ip->i_mount, whichfork))) { 
iformat_extents(1)", XFS_ERRLEVEL_LOW, 
ifp->if_real_bytes = 0; 
ifp->if_u1.if_extents = NULL; 
ifp->if_u1.if_extents = ifp->if_u2.if_inline_ext; 
ifp, 0, nex); 
if (size) { 
fork); 
for (i = 0; i < nex; i++, dp++) { 
fork); 
if (unlikely(xfs_check_nostate_extents( 
iformat_extents(2)", 
ifp->if_flags |= XFS_IFEXTENTS; 
format. 
iformat_btree( 
fork) 
ifork_t		*ifp; 
ifp = XFS_IFORK_PTR(ip, whichfork); 
fork); 
if -- fork has less extents than can fit in 
fork (fork shouldn't be a btree format), root btree 
if (unlikely(XFS_IFORK_NEXTENTS(ip, whichfork) <= 
fork) || 
fork) || 
iformat_btree", XFS_ERRLEVEL_LOW, 
ifp->if_broot_bytes = size; 
ifp->if_broot != NULL); 
fork), 
ifp->if_flags &= ~XFS_IFEXTENTS; 
format inode. 
fork) 
ifork_t	*ifp; 
if (unlikely(XFS_IFORK_FORMAT(ip, whichfork) != XFS_DINODE_FMT_BTREE)) { 
fork); 
iformat_btree) 
ifp->if_bytes = ifp->if_real_bytes = 0; 
ifp, 0, nextents); 
fork); 
ifp); 
ifp, nextents, XFS_EXTFMT_INODE(ip)); 
if_broot based on the number of records 
if_broot to fit the new size.  When shrinking this 
if_broot is currently NULL, then 
if_broot area is changing 
if_broot array. 
iff, 
fork) 
ifork_t		*ifp; 
if (rec_diff == 0) { 
ifp = XFS_IFORK_PTR(ip, whichfork); 
fore, just 
if (ifp->if_broot_bytes == 0) { 
ifp->if_broot = kmem_alloc(new_size, KM_SLEEP | KM_NOFS); 
if_broot, then we need 
ifp->if_broot_bytes, 0); 
ifp->if_broot = kmem_realloc(ifp->if_broot, new_size, 
ifp->if_broot, 1, 
ifp->if_broot, 1, 
ifp->if_broot_bytes = (int)new_size; 
fork)); 
iff is less than 0.  In this case, we are shrinking the 
ifp->if_broot != NULL) && (ifp->if_broot_bytes > 0)); 
iff; 
if (new_max > 0) 
if (new_size > 0) { 
ifp->if_broot, 
ifp->if_flags &= ~XFS_IFBROOT; 
if there are any. 
if (new_max > 0) { 
ifp->if_broot, 1); 
ifp->if_broot, 1, 
ifp->if_broot); 
ifp->if_broot_bytes = (int)new_size; 
ifp->if_broot) <= 
fork)); 
if_data 
iff parameter. 
if_data area is changing 
if_data array. 
iff, 
fork) 
if (byte_diff == 0) { 
ifp = XFS_IFORK_PTR(ip, whichfork); 
if (new_size == 0) { 
ifp->if_u1.if_data); 
ifp->if_u1.if_data = NULL; 
if (new_size <= sizeof(ifp->if_u2.if_inline_data)) { 
if_inline_ext/data, 
if (ifp->if_u1.if_data == NULL) { 
if (ifp->if_u1.if_data != ifp->if_u2.if_inline_data) { 
ifp->if_u2.if_inline_data, ifp->if_u1.if_data, 
ifp->if_u1.if_data); 
force 
if (ifp->if_u1.if_data == NULL) { 
ifp->if_u1.if_data = kmem_alloc(real_size, 
if (ifp->if_u1.if_data != ifp->if_u2.if_inline_data) { 
if the underlying size 
if (ifp->if_real_bytes != real_size) { 
ifp->if_u1.if_data, 
ifp->if_real_bytes, 
ifp->if_real_bytes == 0); 
ifp->if_u1.if_data, ifp->if_u2.if_inline_data, 
ifp->if_real_bytes = real_size; 
ifp->if_bytes <= XFS_IFORK_SIZE(ip, whichfork)); 
fork( 
fork) 
ifp = XFS_IFORK_PTR(ip, whichfork); 
ifp->if_broot); 
format is local, then we can't have an extents 
if we do. 
if (XFS_IFORK_FORMAT(ip, whichfork) == XFS_DINODE_FMT_LOCAL) { 
ifp->if_u1.if_data != NULL)) { 
ifp->if_u1.if_data); 
ifp->if_real_bytes = 0; 
if ((ifp->if_flags & XFS_IFEXTENTS) && 
ifp->if_u1.if_extents != NULL) && 
ifp->if_real_bytes != 0); 
ifp->if_u1.if_extents == NULL || 
ifp->if_real_bytes == 0); 
fork == XFS_ATTR_FORK) { 
form 
fork, the in-core and on-disk fork sizes can be 
fork size to determine the 
fork) 
ifork_t		*ifp; 
ifp = XFS_IFORK_PTR(ip, whichfork); 
ifp->if_bytes > 0); 
fork); 
for (i = 0; i < nrecs; i++) { 
if (isnullstartblock(start_block)) { 
format */ 
ifp, copied, XFS_EXTFMT_INODE(ip)); 
formats 
format, this can only happen when the fork has 
format always takes precedence, because the 
iflush_fork( 
fork) 
ifork_t		*ifp; 
if (!iip) 
ifp = XFS_IFORK_PTR(ip, whichfork); 
if we gave up in iformat in an error path, 
for the attribute fork. 
if (!ifp) { 
fork == XFS_ATTR_FORK); 
fork); 
fork)) { 
if ((iip->ili_fields & dataflag[whichfork]) && 
ifp->if_u1.if_data != NULL); 
fork)); 
ifp->if_flags & XFS_IFEXTENTS) || 
fork])); 
ifp->if_bytes > 0)) { 
fork) > 0); 
fork); 
if ((iip->ili_fields & brootflag[whichfork]) && 
ifp->if_broot != NULL); 
fork)); 
fork)); 
if (iip->ili_fields & XFS_ILOG_DEV) { 
fork == XFS_DATA_FORK); 
if (iip->ili_fields & XFS_ILOG_UUID) { 
fork == XFS_DATA_FORK); 
if_u2.if_uuid, 
ifork_t	*ifp,		/* inode fork pointer */ 
ifp->if_bytes / sizeof(xfs_bmbt_rec_t)); 
ifp->if_u1.if_ext_irec->er_extbuf; 
ifp, &page_idx, &erp_idx, 0); 
if (ifp->if_bytes) { 
for incore inode 
ifork_t	*ifp = (state & BMAP_ATTRFORK) ? ip->i_afp : &ip->i_df; 
ifp->if_flags & XFS_IFEXTENTS); 
for (i = idx; i < idx + count; i++, new++) 
for incore file 
if the new extents are being 
for the new extents to be inserted. The 
ifork_t	*ifp,		/* inode fork pointer */ 
iff)	/* number of extents to add */ 
ifp->if_bytes / (uint)sizeof(xfs_bmbt_rec_t); 
iff = ext_diff * sizeof(xfs_bmbt_rec_t); 
iff) 
if (nextents + ext_diff <= XFS_INLINE_EXTS) { 
ifp->if_u2.if_inline_ext[idx + ext_diff], 
ifp->if_u2.if_inline_ext[idx], 0, byte_diff); 
ifp->if_u1.if_extents = ifp->if_u2.if_inline_ext; 
if (nextents + ext_diff <= XFS_LINEAR_EXTS) { 
if (idx < nextents) { 
ifp->if_u1.if_extents[idx], 
ifp->if_u1.if_extents[idx], 0, byte_diff); 
iff > XFS_LINEAR_EXTS); 
ifp, &page_idx, &erp_idx, 1); 
ifp); 
ifp->if_u1.if_ext_irec; 
if (erp && erp->er_extcount + ext_diff <= XFS_LINEAR_EXTS) { 
iff], 
iff); 
iff; 
if (erp) { 
iff); 
iff; 
while (count) { 
if (count) 
ifp->if_bytes = new_size; 
for the target extent list 
fore idx 
ifork_t	*ifp,			/* inode fork pointer */ 
iff;		/* new bytes being added */ 
iff;		/* number of extents to add */ 
for nex2 extents */ 
ifp->if_flags & XFS_IFEXTIREC); 
ifp->if_real_bytes / XFS_IEXT_BUFSZ; 
if (nex2) { 
iff, KM_NOFS); 
ifp, erp_idx + 1, -nex2); 
iff = MIN(ext_cnt, (int)XFS_LINEAR_EXTS - erp->er_extcount); 
iff; 
iff; 
while (ext_cnt) { 
ifp, erp_idx); 
iff; 
iff; 
if (nex2) { 
iff = nex2 * sizeof(xfs_bmbt_rec_t); 
if (nex2 <= ext_avail) { 
if space is available in the 
if ((erp_idx < nlists - 1) && 
ifp->if_u1.if_ext_irec[erp_idx+1].er_extcount))) { 
for nex2 extents */ 
for 
ifp, erp_idx); 
iff); 
ifp, erp_idx + 1, nex2); 
for incore file 
iff,	/* number of extents to remove */ 
ifork_t	*ifp = (state & BMAP_ATTRFORK) ? ip->i_afp : &ip->i_df; 
iff > 0); 
iff) * sizeof(xfs_bmbt_rec_t); 
ifp); 
ifp, idx, ext_diff); 
ifp, idx, ext_diff); 
ifp, idx, ext_diff); 
iff extents from the inline buffer, beginning 
ifork_t	*ifp,		/* inode fork pointer */ 
iff)	/* number of extents to remove */ 
ifp->if_flags & XFS_IFEXTIREC)); 
ifp->if_bytes / (uint)sizeof(xfs_bmbt_rec_t); 
iff) < XFS_INLINE_EXTS); 
ifp->if_u2.if_inline_ext[idx], 
iff)) * 
ifp->if_u2.if_inline_ext[nextents - ext_diff], 
ifp->if_u2.if_inline_ext[idx], 0, 
iff extents from a linear (direct) extent list, 
if the 
iff up in the list to overwrite the records being 
ifork_t	*ifp,		/* inode fork pointer */ 
iff)	/* number of extents to remove */ 
ifp->if_flags & XFS_IFEXTIREC)); 
iff * sizeof(xfs_bmbt_rec_t)); 
if (new_size == 0) { 
if needed) */ 
ifp->if_u1.if_extents[idx], 
iff)) * 
ifp->if_u1.if_extents[nextents - ext_diff], 
for us. 
ifp, new_size); 
fore idx 
ifork_t	*ifp,		/* inode fork pointer */ 
iff;	/* extents to remove in current list */ 
fore idx */ 
ifp->if_flags & XFS_IFEXTIREC); 
while (ext_cnt) { 
iff = MIN(ext_cnt, (erp->er_extcount - nex1)); 
for deletion of entire list; 
if (ext_diff == erp->er_extcount) { 
iff; 
if (ext_cnt) { 
ifp->if_u1.if_ext_irec[erp_idx]; 
if needed) */ 
iff], 
iff; 
iff; 
ifp->if_bytes -= count * sizeof(xfs_bmbt_rec_t); 
ifork_t	*ifp,		/* inode fork pointer */ 
ifp->if_flags & XFS_IFEXTIREC) || 
ifp->if_real_bytes))); 
if (new_size == 0) { 
if (ifp->if_real_bytes) { 
if (new_size <= XFS_INLINE_EXTS * sizeof(xfs_bmbt_rec_t)) { 
ifp->if_bytes = new_size; 
if (!is_power_of_2(new_size)){ 
if (rnew_size != ifp->if_real_bytes) { 
ifp->if_u1.if_extents, 
ifp->if_real_bytes, KM_NOFS); 
if (rnew_size > ifp->if_real_bytes) { 
ifp->if_real_bytes); 
if (!is_power_of_2(new_size)) { 
ifp, rnew_size); 
ifp->if_bytes = new_size; 
ifork_t	*ifp,		/* inode fork pointer */ 
ifp->if_flags & XFS_IFEXTENTS); 
ifp->if_u2.if_inline_ext, ifp->if_u1.if_extents, 
ifp->if_u1.if_extents); 
ifp->if_real_bytes = 0; 
if_bytes here. It is the caller's responsibility to update 
ifork_t	*ifp,		/* inode fork pointer */ 
ifp->if_u1.if_extents = kmem_alloc(new_size, KM_NOFS); 
if (ifp->if_bytes) { 
ifp->if_bytes); 
ifp->if_real_bytes = new_size; 
ifork_t	*ifp,		/* inode fork pointer */ 
ifp->if_flags & XFS_IFEXTIREC); 
ifp->if_real_bytes); 
if (new_size == 0) { 
ifp->if_u1.if_ext_irec = (xfs_ext_irec_t *) 
ifork_t	*ifp)		/* inode fork pointer */ 
ifp->if_flags & XFS_IFEXTIREC); 
ifp); 
ifp->if_u1.if_ext_irec->er_extbuf; 
ifp->if_flags &= ~XFS_IFEXTIREC; 
ifp->if_bytes = size; 
ifp, size); 
ifork_t	*ifp)		/* inode fork pointer */ 
ifp->if_real_bytes / XFS_IEXT_BUFSZ; 
for (erp_idx = nlists - 1; erp_idx >= 0 ; erp_idx--) { 
ifp->if_flags &= ~XFS_IFEXTIREC; 
ifp->if_u1.if_extents); 
ifp->if_u2.if_inline_ext, 0, XFS_INLINE_EXTS * 
ifp->if_u1.if_extents = NULL; 
ifp->if_bytes = 0; 
for file system block bno. 
ifork_t	*ifp,		/* inode fork pointer */ 
for */ 
ifp->if_bytes / (uint)sizeof(xfs_bmbt_rec_t); 
if (ifp->if_flags & XFS_IFEXTIREC) { 
ifp, bno, &erp_idx); 
ifp->if_u1.if_extents; 
while (low <= high) { 
if (bno < startoff) { 
if (bno >= startoff + blockcount) { 
if (ifp->if_flags & XFS_IFEXTIREC) { 
if (ifp->if_flags & XFS_IFEXTIREC) { 
if (bno >= startoff + blockcount) { 
ifp, idx); 
for filesystem block bno. Store the index of the 
ifork_t	*ifp,		/* inode fork pointer */ 
for */ 
ifp->if_flags & XFS_IFEXTIREC); 
while (low <= high) { 
ifp->if_u1.if_ext_irec[erp_idx]; 
if (bno < xfs_bmbt_get_startoff(erp->er_extbuf)) { 
if (erp_next && bno >= 
ifork_t	*ifp,		/* inode fork pointer */ 
ifp->if_flags & XFS_IFEXTIREC); 
ifp->if_bytes / sizeof(xfs_bmbt_rec_t)); 
ifp->if_real_bytes / XFS_IEXT_BUFSZ; 
while (low <= high) { 
ifp->if_u1.if_ext_irec[erp_idx]; 
if (page_idx < erp->er_extoff || (page_idx == erp->er_extoff && 
if (page_idx > erp->er_extoff + erp->er_extcount || 
if (page_idx == erp->er_extoff + erp->er_extcount && 
for incore extents increases above XFS_IEXT_BUFSZ. 
ifork_t	*ifp)		/* inode fork pointer */ 
ifp->if_flags & XFS_IFEXTIREC)); 
if (nextents == 0) { 
if (!ifp->if_real_bytes) { 
if (ifp->if_real_bytes < XFS_IEXT_BUFSZ) { 
ifp->if_u1.if_extents; 
ifp->if_flags |= XFS_IFEXTIREC; 
ifp->if_bytes = nextents * sizeof(xfs_bmbt_rec_t); 
ifork_t	*ifp,		/* inode fork pointer */ 
for new irec */ 
ifp->if_flags & XFS_IFEXTIREC); 
ifp, ++nlists * 
ifp->if_u1.if_ext_irec; 
for (i = nlists - 1; i > erp_idx; i--) { 
ifp->if_u1.if_ext_irec; 
ifp->if_real_bytes = nlists * XFS_IEXT_BUFSZ; 
ifork_t	*ifp,		/* inode fork pointer */ 
ifp->if_flags & XFS_IFEXTIREC); 
ifp->if_u1.if_ext_irec[erp_idx]; 
ifp, erp_idx + 1, 
ifp->if_u1.if_ext_irec; 
for (i = erp_idx; i < nlists - 1; i++) { 
if (--nlists) { 
ifp->if_u1.if_ext_irec); 
ify 
if possible.  The 
ifork_t	*ifp)		/* inode fork pointer */ 
ifp->if_flags & XFS_IFEXTIREC); 
ifp->if_bytes / (uint)sizeof(xfs_bmbt_rec_t); 
ifp); 
ifp); 
if (nextents <= XFS_LINEAR_EXTS) { 
if (nextents < (nlists * XFS_LINEAR_EXTS) >> 1) { 
ifork_t	*ifp)		/* inode fork pointer */ 
ifp->if_flags & XFS_IFEXTIREC); 
while (erp_idx < nlists - 1) { 
if (erp_next->er_extcount <= 
fore removing extent record 
ifp, erp_idx + 1); 
iff contains the number of extents that were added 
ifork_t	*ifp,		/* inode fork pointer */ 
iff)	/* number of new extents */ 
ifp->if_flags & XFS_IFEXTIREC); 
for (i = erp_idx; i < nlists; i++) { 
file : ./test/kernel/fs/xfs/xfs_quotaops.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
if (!XFS_IS_QUOTA_RUNNING(mp)) 
if (!XFS_IS_QUOTA_RUNNING(mp)) 
if (sb->s_flags & MS_RDONLY) 
if (op != Q_XQUOTARM && !XFS_IS_QUOTA_RUNNING(mp)) 
if (uflags & FS_QUOTA_UDQ_ACCT) 
if (uflags & FS_QUOTA_PDQ_ACCT) 
if (uflags & FS_QUOTA_GDQ_ACCT) 
if (uflags & FS_QUOTA_UDQ_ENFD) 
if (uflags & FS_QUOTA_GDQ_ENFD) 
if (uflags & FS_QUOTA_PDQ_ENFD) 
if (!XFS_IS_QUOTA_ON(mp)) 
if (sb->s_flags & MS_RDONLY) 
if (XFS_IS_QUOTA_ON(mp)) 
if (uflags & FS_USER_QUOTA) 
if (uflags & FS_GROUP_QUOTA) 
if (uflags & FS_USER_QUOTA) 
if (!XFS_IS_QUOTA_RUNNING(mp)) 
if (!XFS_IS_QUOTA_ON(mp)) 
if (sb->s_flags & MS_RDONLY) 
if (!XFS_IS_QUOTA_RUNNING(mp)) 
if (!XFS_IS_QUOTA_ON(mp)) 
file : ./test/kernel/fs/xfs/xfs_alloc.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
if (!error && *stat == 1) { 
if (args->alignment > 1 && len >= args->minlen) { 
iff = aligned_bno - bno; 
iff >= len ? 0 : len - diff; 
iff for "near" allocations. 
ifference value (absolute) */ 
if it is past 
fore desired block. The second case is there to allow 
if (freebno >= wantbno || (userdata && freeend < wantend)) { 
if (freeend >= wantend && alignment > 1) { 
if (newbno1 >= freeend) 
if (newbno2 < freebno) 
if (newbno1 != NULLAGBLOCK && newbno2 != NULLAGBLOCK) { 
if (newbno2 != NULLAGBLOCK) 
if (freeend >= wantend) { 
if (alignment > 1) { 
if (newbno1 > freeend - wantlen && 
if (newbno1 >= freeend) 
for some k. 
if (args->prod <= 1 || rlen < args->mod || rlen == args->maxlen || 
if (k == args->mod) 
if (k > args->mod) 
if ((int)rlen < (int)args->minlen) 
if there is too little space left in the a.g. 
iff;		/* free space difference */ 
iff = be32_to_cpu(agf->agf_freeblks) 
if (diff >= 0) 
iff;		/* shrink the allocated space */ 
for flen blocks. 
for by-size btree */ 
if necessary. 
if (flags & XFSA_FIXUP_CNT_OK) { 
if ((error = xfs_alloc_get_rec(cnt_cur, &nfbno1, &nflen1, &i))) 
if 
if ((error = xfs_alloc_lookup_eq(cnt_cur, fbno, flen, &i))) 
if necessary. 
if (flags & XFSA_FIXUP_BNO_OK) { 
if ((error = xfs_alloc_get_rec(bno_cur, &nfbno1, &nflen1, &i))) 
if 
if ((error = xfs_alloc_lookup_eq(bno_cur, fbno, flen, &i))) 
ifdef DEBUG 
if 
if (rbno == fbno && rlen == flen) 
if (rbno == fbno) { 
if (rbno + rlen == fbno + flen) { 
if ((error = xfs_btree_delete(cnt_cur, &i))) 
if (nfbno1 != NULLAGBLOCK) { 
if ((error = xfs_btree_insert(cnt_cur, &i))) 
if (nfbno2 != NULLAGBLOCK) { 
if ((error = xfs_btree_insert(cnt_cur, &i))) 
if (nfbno1 == NULLAGBLOCK) { 
if ((error = xfs_btree_delete(bno_cur, &i))) 
if ((error = xfs_alloc_update(bno_cur, nfbno1, nflen1))) 
if (nfbno2 != NULLAGBLOCK) { 
if ((error = xfs_alloc_lookup_eq(bno_cur, nfbno2, nflen2, &i))) 
if ((error = xfs_btree_insert(bno_cur, &i))) 
ify( 
if (!uuid_equal(&agfl->agfl_uuid, &mp->m_sb.sb_uuid)) 
if (be32_to_cpu(agfl->agfl_magicnum) != XFS_AGFL_MAGIC) 
for any useful checking. growfs ensures we can't 
if (bp->b_pag && be32_to_cpu(agfl->agfl_seqno) != bp->b_pag->pag_agno) 
for (i = 0; i < XFS_AGFL_SIZE(mp); i++) { 
ify( 
ification of non-crc AGFLs because mkfs does not 
ify just those entries are valid. 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (!xfs_buf_verify_cksum(bp, XFS_AGFL_CRC_OFF)) 
if (!xfs_agfl_verify(bp)) 
if (bp->b_error) 
ify( 
ification of non-crc AGFLs */ 
if (!xfs_agfl_verify(bp)) { 
ifier_error(bp); 
if (bip) 
ify_read = xfs_agfl_read_verify, 
for the ag free block array */ 
if (error) 
if (unlikely(be32_to_cpu(agf->agf_freeblks) > 
form k * prod + mod unless there's nothing that large. 
for allocation */ 
if (error || args->agbno == NULLAGBLOCK) 
if (!args->wasfromfl) { 
if (error) 
if (!args->isfl) { 
form k * prod + mod unless there's nothing that large. 
for the by-number freespace btree. 
for the closest free block <= bno, it must contain bno 
if (error) 
if (!i) 
if (error) 
for overlapping busy extents. 
if the start of the extent is busy, or the freespace isn't 
for the minimum request. 
if (tbno > args->agbno) 
if (tlen < args->minlen) 
if (tend < args->agbno + args->minlen) 
if given. 
if (!xfs_alloc_fix_minleft(args)) 
for args->len 
if (error) { 
iff,	/* difference for search comparison */ 
iff; 
if (!gdiff) 
if (error) 
if (!dir) { 
if (*sbnoa <= args->agbno - gdiff) 
if (*slena >= args->minlen) { 
iff = xfs_alloc_compute_diff(args->agbno, args->len, 
if (sdiff < gdiff) 
if (!dir) 
if (error) 
while (i); 
form k * prod + mod unless there's nothing that large. 
for bno btree, right side */ 
for count btree */ 
iff;		/* difference to right side entry */ 
iff;		/* difference to left side entry */ 
forced = 0; 
if 
for the by-size btree. 
if there are any free extents as big as maxlen. 
if ((error = xfs_alloc_lookup_ge(cnt_cur, 0, args->maxlen, &i))) 
if (!i) { 
if (i == 0 || ltlen == 0) { 
while loop so we can break out of it, 
while (xfs_btree_islastblock(cnt_cur, 0)) { 
ifdef DEBUG 
if 
if (ltlen || args->alignment > 1) { 
if ((error = xfs_alloc_get_rec(cnt_cur, &ltbno, 
if (ltlen >= args->minlen) 
if ((error = xfs_btree_increment(cnt_cur, 0, &i))) 
while (i); 
if (!i) 
iff = 0; 
if it's better than 
if ((error = xfs_alloc_get_rec(cnt_cur, &ltbno, &ltlen, &i))) 
if (ltlena < args->minlen) 
if (args->len < blen) 
iff = xfs_alloc_compute_diff(args->agbno, args->len, 
if (ltnew != NULLAGBLOCK && 
iff = ltdiff; 
if (blen == 0) 
if ((error = xfs_alloc_get_rec(cnt_cur, &ltbno, &ltlen, &i))) 
if (!xfs_alloc_fix_minleft(args)) { 
for blen blocks. 
for the by-bno tree. 
if ((error = xfs_alloc_fixup_trees(cnt_cur, bno_cur_lt, ltbno, 
for both to fail; the upper 
for the leftward search. 
if ((error = xfs_alloc_lookup_le(bno_cur_lt, args->agbno, args->maxlen, &i))) 
if (!i) { 
for the rightward 
for the rightward search. 
if ((error = xfs_btree_dup_cursor(bno_cur_lt, &bno_cur_gt))) 
if any, or to the leftmost entry. 
if ((error = xfs_btree_increment(bno_cur_gt, 0, &i))) 
if (!i) { 
if (bno_cur_lt) { 
if (ltlena >= args->minlen) 
if ((error = xfs_btree_decrement(bno_cur_lt, 0, &i))) 
if (!i) { 
if (bno_cur_gt) { 
if (gtlena >= args->minlen) 
if ((error = xfs_btree_increment(bno_cur_gt, 0, &i))) 
if (!i) { 
while (bno_cur_lt || bno_cur_gt); 
if (bno_cur_lt && bno_cur_gt) { 
for a right side entry. 
iff = xfs_alloc_compute_diff(args->agbno, args->len, 
iff, &gtbno, &gtlen, 
for a left side entry. 
iff = xfs_alloc_compute_diff(args->agbno, args->len, 
iff, &ltbno, &ltlen, 
if (error) 
if (bno_cur_lt == NULL && bno_cur_gt == NULL) { 
if (!forced++) { 
force(args->mp, XFS_LOG_SYNC); 
if (bno_cur_gt) { 
if (!xfs_alloc_fix_minleft(args)) { 
iff(args->agbno, rlen, args->alignment, 
if ((error = xfs_alloc_fixup_trees(cnt_cur, bno_cur_lt, ltbno, ltlen, 
if (j) 
if (cnt_cur != NULL) 
if (bno_cur_lt != NULL) 
if (bno_cur_gt != NULL) 
form k * prod + mod unless there's nothing that large. 
for bno btree */ 
forced = 0; 
for the by-size btree. 
for an entry >= maxlen+alignment-1 blocks. 
if ((error = xfs_alloc_lookup_ge(cnt_cur, 0, 
for a smaller extent. In the case that there are 
if (!i || forced > 1) { 
if (error) 
if (i == 0 || flen == 0) { 
for a non-busy extent that is large enough. 
for (;;) { 
if (error) 
if (rlen >= args->maxlen) 
if (error) 
if (i == 0) { 
forcing the log out and 
if (!forced++) 
force(args->mp, XFS_LOG_SYNC); 
if the space hits maxlen 
for something better. 
if (rlen < args->maxlen) { 
for (;;) { 
if (i == 0) 
if ((error = xfs_alloc_get_rec(cnt_cur, &fbno, &flen, 
if (flen < bestrlen) 
if (rlen > bestrlen) { 
if (rlen == args->maxlen) 
if ((error = xfs_alloc_lookup_eq(cnt_cur, bestfbno, bestflen, 
if (rlen < args->minlen) { 
forced++) { 
force(args->mp, XFS_LOG_SYNC); 
if (!xfs_alloc_fix_minleft(args)) 
for the by-block tree. 
if ((error = xfs_alloc_fixup_trees(cnt_cur, bno_cur, fbno, flen, 
if (cnt_cur) 
if (bno_cur) 
if there is nothing in the tree. 
if ((error = xfs_btree_decrement(ccur, 0, &i))) 
if (i) { 
if (args->minlen == 1 && args->alignment == 1 && !args->isfl && 
if (error) 
if (fbno != NULLAGBLOCK) { 
if (args->userdata) { 
for some reason. 
if (flen < args->minlen) { 
for length. 
for a.g. freelist header */ 
if is freelist blocks - no sb acctg */ 
for by-block btree */ 
for filesystem */ 
for the by-block btree. 
for a neighboring block on the left (lower block numbers) 
if ((error = xfs_alloc_lookup_le(bno_cur, bno, len, &haveleft))) 
if (haveleft) { 
if ((error = xfs_alloc_get_rec(bno_cur, &ltbno, &ltlen, &i))) 
if (ltbno + ltlen < bno) 
for a neighboring block on the right (higher block numbers) 
if ((error = xfs_btree_increment(bno_cur, 0, &haveright))) 
if (haveright) { 
if ((error = xfs_alloc_get_rec(bno_cur, &gtbno, &gtlen, &i))) 
if (bno + len < gtbno) 
for the by-size tree. 
if (haveleft && haveright) { 
if ((error = xfs_alloc_lookup_eq(cnt_cur, ltbno, ltlen, &i))) 
if ((error = xfs_btree_delete(cnt_cur, &i))) 
if ((error = xfs_alloc_lookup_eq(cnt_cur, gtbno, gtlen, &i))) 
if ((error = xfs_btree_delete(cnt_cur, &i))) 
for the right block. 
if ((error = xfs_btree_delete(bno_cur, &i))) 
if ((error = xfs_btree_decrement(bno_cur, 0, &i))) 
ifdef DEBUG 
if ((error = xfs_alloc_get_rec(bno_cur, &xxbno, &xxlen, 
if 
if ((error = xfs_alloc_update(bno_cur, nbno, nlen))) 
if (haveleft) { 
if ((error = xfs_alloc_lookup_eq(cnt_cur, ltbno, ltlen, &i))) 
if ((error = xfs_btree_delete(cnt_cur, &i))) 
if ((error = xfs_btree_decrement(bno_cur, 0, &i))) 
if ((error = xfs_alloc_update(bno_cur, nbno, nlen))) 
if (haveright) { 
if ((error = xfs_alloc_lookup_eq(cnt_cur, gtbno, gtlen, &i))) 
if ((error = xfs_btree_delete(cnt_cur, &i))) 
if ((error = xfs_alloc_update(bno_cur, nbno, nlen))) 
if ((error = xfs_btree_insert(bno_cur, &i))) 
if ((error = xfs_alloc_lookup_eq(cnt_cur, nbno, nlen, &i))) 
if ((error = xfs_btree_insert(cnt_cur, &i))) 
if (error) 
if (!isfl) 
if (bno_cur) 
if (cnt_cur) 
for (level = 1; maxblocks > 1; level++) 
if (need > pag->pagf_flcount) 
if (pag->pagf_longest > delta) 
for this allocation. 
formation structure */ 
if (!pag->pagf_init) { 
if (!pag->pagf_init) { 
if we are not being asked to 
if (pag->pagf_metadata && args->userdata && 
if (!(flags & XFS_ALLOC_FLAG_FREEING)) { 
if ((args->minlen + args->alignment + args->minalignslop - 1) > 
if (agbp) 
if we're not blocking on locks, and it's held. 
if (agbp == NULL) { 
if (agbp == NULL) { 
if (!(flags & XFS_ALLOC_FLAG_FREEING)) { 
if ((args->minlen + args->alignment + args->minalignslop - 1) > 
if it's too long. 
while (be32_to_cpu(agf->agf_flcount) > need) { 
if (error) 
if ((error = xfs_free_ag_extent(tp, agbp, args->agno, bno, 1, 1))) 
if ((error = xfs_alloc_read_agfl(mp, tp, targs.agno, &agflbp))) 
if it's too short. 
while (be32_to_cpu(agf->agf_flcount) < need) { 
if ((error = xfs_alloc_ag_vextent(&targs))) { 
if we run out.  Won't happen if callers are obeying 
for free calls 
if (targs.agbno == NULLAGBLOCK) { 
for (bno = targs.agbno; bno < targs.agbno + targs.len; bno++) { 
if (error) 
for the block gotten. 
for a.g. freelist structure */ 
if (!agf->agf_flcount) { 
if (error) 
if (be32_to_cpu(agf->agf_flfirst) == XFS_AGFL_SIZE(mp)) 
if (btreeblk) { 
for a.g. freelist header */ 
for inode allocation to force the pag data to be initialized. 
if ((error = xfs_alloc_read_agf(mp, tp, agno, flags, &bp))) 
if (bp) 
for the allocation group. 
for a.g. freelist header */ 
if (!agflbp && (error = xfs_alloc_read_agfl(mp, tp, 
if (be32_to_cpu(agf->agf_fllast) == XFS_AGFL_SIZE(mp)) 
if (btreeblk) { 
ify( 
if (xfs_sb_version_hascrc(&mp->m_sb) && 
if (!(agf->agf_magicnum == cpu_to_be32(XFS_AGF_MAGIC) && 
for any useful checking. growfs ensures we can't 
if (bp->b_pag && be32_to_cpu(agf->agf_seqno) != bp->b_pag->pag_agno) 
if (xfs_sb_version_haslazysbcount(&mp->m_sb) && 
ify( 
if (xfs_sb_version_hascrc(&mp->m_sb) && 
if (XFS_TEST_ERROR(!xfs_agf_verify(mp, bp), mp, 
if (bp->b_error) 
ify( 
if (!xfs_agf_verify(mp, bp)) { 
ifier_error(bp); 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (bip) 
ify_read = xfs_agf_read_verify, 
for the ag freelist header */ 
if (error) 
if (!*bpp) 
for the ag freelist header */ 
if (error) 
if (!*bpp) 
if (!pag->pagf_init) { 
ifdef DEBUG 
if 
for the case where the last a.g. is shorter 
if (args->maxlen > agsize) 
if (args->alignment == 0) 
if (XFS_FSB_TO_AGNO(mp, args->fsbno) >= mp->m_sb.sb_agcount || 
force us into a single a.g. 
if (error) { 
if (!args->agbp) { 
if ((error = xfs_alloc_ag_vextent(args))) 
if ((args->userdata  == XFS_ALLOC_INITIAL_USER_DATA) && 
for a winner. 
if (type == XFS_ALLOCTYPE_ANY_AG) { 
if (type == XFS_ALLOCTYPE_FIRST_AG) { 
if (type == XFS_ALLOCTYPE_START_AG) 
for (;;) { 
if (no_min) args->minleft = 0; 
if (error) { 
if (args->agbp) { 
if (args->agno == sagno && 
if we already have allocated a 
if (++(args->agno) == mp->m_sb.sb_agcount) { 
if (args->agno == sagno) { 
if (flags == 0) { 
if (type == XFS_ALLOCTYPE_START_BNO) { 
if (bump_rotor || (type == XFS_ALLOCTYPE_ANY_AG)) { 
if (args->agbno == NULLAGBLOCK) 
ifdef DEBUG 
if 
if (args.agno >= args.mp->m_sb.sb_agcount) 
if (args.agbno >= args.mp->m_sb.sb_agblocks) 
if (error) 
if (args.agbno + len > 
if (!error) 
file : ./test/kernel/fs/xfs/xfs_buf.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
ifdef XFS_BUF_LOCK_TRACKING 
while (0) 
while (0) 
if the buffer is vmapped. 
if the buffer is not mapped, but the code is clever 
for b_addr and bp->b_page_count > 1. 
if (!(bp->b_state & XFS_BSTATE_DISPOSE) && 
if (map_count == 1) { 
if (!bp->b_maps) 
if it was allocated. 
if (bp->b_maps != &bp->__b_map) { 
if (unlikely(!bp)) 
ifically set by later operations on the buffer. 
if (error)  { 
for (i = 0; i < nmaps; i++) { 
ified number 
if (bp->b_pages == NULL) { 
if (page_count <= XB_PAGES) { 
if (bp->b_pages == NULL) 
if it was allocated. 
if (bp->b_pages != bp->b_page_array) { 
ified buffer. 
for 
if (bp->b_flags & _XBF_PAGES) { 
if (xfs_buf_is_vmapped(bp)) 
for (i = 0; i < bp->b_page_count; i++) { 
if (bp->b_flags & _XBF_KMEM) 
for buffer in question and builds it's page list. 
for buffers that are contained within a single page, just allocate 
if (size < PAGE_SIZE) { 
if (!bp->b_addr) { 
if (((unsigned long)(bp->b_addr + size - 1) & PAGE_MASK) != 
if (unlikely(error)) 
for (i = 0; i < bp->b_page_count; i++) { 
if (unlikely(page == NULL)) { 
if (!(++retries % 100)) 
for (i = 0; i < bp->b_page_count; i++) 
if necessary. 
if (bp->b_page_count == 1) { 
if (flags & XBF_UNMAPPED) { 
if (bp->b_addr) 
while (retried++ <= 1); 
if (!bp->b_addr) 
if absent, a lockable buffer for 
for (i = 0; i < nmaps; i++) 
for IOs smaller than the sector size / not sector aligned */ 
fortunately, so we 
if (blkno >= eofs) { 
ific error on buffer lookup failures. 
while (*rbp) { 
if (blkno < bp->b_bn) 
if (blkno > bp->b_bn) 
if the buffer 
for an exact match. 
if (bp->b_length != numblks) { 
if (new_bp) { 
if (!xfs_buf_trylock(bp)) { 
if the buffer is stale, clear all the external state associated with 
if (bp->b_flags & XBF_STALE) { 
ified range. The code is optimised for 
if (likely(bp)) 
if (unlikely(!new_bp)) 
if (error) { 
if (!bp) { 
if (bp != new_bp) 
if (!bp->b_addr) { 
if (unlikely(error)) { 
if (flags & XBF_ASYNC) 
if (bp) { 
if (!XFS_BUF_ISDONE(bp)) { 
if (flags & XBF_ASYNC) { 
if (bdi_read_congested(target->bt_bdi)) 
if (!bp) 
for a read IO */ 
if (XFS_FORCED_SHUTDOWN(target->bt_mount)) { 
if (bp->b_pages) 
if ((!is_vmalloc_addr(addr))) { 
if (bp->b_pages) 
if (rval) 
for (i = 0; i < bp->b_page_count; i++) { 
if (unlikely(bp == NULL)) 
if (error) 
for (i = 0; i < page_count; i++) { 
if (!bp->b_pages[i]) 
if (unlikely(error)) { 
while (--i >= 0) 
ified buffer.  If the 
if (!pag) { 
if (atomic_dec_and_test(&bp->b_hold)) 
if (atomic_dec_and_lock(&bp->b_hold, &pag->pag_buf_lock)) { 
if (!(bp->b_flags & XBF_STALE) && atomic_read(&bp->b_lru_ref)) { 
for the LRU and clear the 
if (list_lru_add(&bp->b_target->bt_lru, &bp->b_lru)) { 
for the 
if (!(bp->b_state & XFS_BSTATE_DISPOSE)) { 
if it is not already locked. 
fore 
if (locked) 
fore trying to get the lock. 
if (atomic_read(&bp->b_pin_count) && (bp->b_flags & XBF_STALE)) 
force(bp->b_target->bt_mount, 0); 
if (atomic_read(&bp->b_pin_count) == 0) 
for (;;) { 
if (atomic_read(&bp->b_pin_count) == 0) 
if (read && bp->b_ops && !bp->b_error && (bp->b_flags & XBF_DONE)) 
if (bp->b_iodone) 
if (bp->b_flags & XBF_ASYNC) 
if (bp->b_error == 0) 
if (bp->b_iodone || (read && bp->b_ops) || (bp->b_flags & XBF_ASYNC)) { 
ifdef XFSERRORDEBUG 
if 
for userdata errors; metadata bufs come with 
if (!(fl & XBF_ASYNC)) { 
for 
if (XFS_FORCED_SHUTDOWN(bp->b_target->bt_mount)) { 
if (!bp->b_iodone && !XFS_BUF_ISREAD(bp)) 
if (error) { 
force_shutdown(bp->b_target->bt_mount, 
if (atomic_dec_and_test(&bp->b_io_remaining) == 1) 
if (!bp->b_error) 
if (!bp->b_error && xfs_buf_is_vmapped(bp) && (bp->b_flags & XBF_READ)) 
fore the start offset */ 
while (offset >= PAGE_SIZE) { 
for the next time around. 
if (nr_pages > total_nr_pages) 
for (; size && nr_pages; nr_pages--, page_index++) { 
if (nbytes > size) 
if (rbytes < nbytes) 
if (likely(bio->bi_iter.bi_size)) { 
if (size) 
if (bp->b_flags & XBF_WRITE) { 
if (bp->b_flags & XBF_FUA) 
if (bp->b_flags & XBF_FLUSH) 
ifier callback function if it exists. If 
if (bp->b_ops) { 
if (bp->b_error) { 
force_shutdown(bp->b_target->bt_mount, 
if (bp->b_flags & XBF_READ_AHEAD) { 
for meta-data */ 
fore we start - 
for (i = 0; i < bp->b_map_count; i++) { 
if (bp->b_error) 
if (size <= 0) 
if (bp->b_flags & XBF_WRITE) 
fore we have started 
for later, 
if 
if any, or 
if (!bp->b_error) 
for_completion(&bp->b_iowait); 
if (bp->b_addr) 
while (boff < bend) { 
for any bufs with callbacks that have been submitted but have not yet 
while freeing all the buffers only held by the LRU. 
if (atomic_read(&bp->b_hold) > 1) { 
if (!spin_trylock(&bp->b_lock)) 
while (list_lru_count(&btp->bt_lru)) { 
while (!list_empty(&dispose)) { 
if (bp->b_flags & XBF_WRITE_FAIL) { 
if (loop++ != 0) 
if (!spin_trylock(&bp->b_lock)) 
if (!atomic_add_unless(&bp->b_lru_ref, -1, 0)) { 
while (!list_empty(&dispose)) { 
if (mp->m_flags & XFS_MOUNT_BARRIER) 
if (set_blocksize(btp->bt_bdev, sectorsize)) { 
if (!btp->bt_bdi) 
if (xfs_setsize_buftarg_early(btp, bdev)) 
if (list_lru_init(&btp->bt_lru)) 
if it hasn't already been.  Note that 
form 
if we queued up the buffer, or false if it already had 
for imediate writeout.  Just ignore it in that 
if (bp->b_flags & _XBF_DELWRI_Q) { 
while it 
if (list_empty(&bp->b_list)) { 
iff; 
if (diff < 0) 
if (diff > 0) 
for_each_entry_safe(bp, n, buffer_list, b_list) { 
if (xfs_buf_ispinned(bp)) { 
if (!xfs_buf_trylock(bp)) 
if (!(bp->b_flags & _XBF_DELWRI_Q)) { 
for_each_entry_safe(bp, n, io_list, b_list) { 
if (!wait) { 
for I/O completion on any of the buffers.  This interface 
for I/O 
if they require such 
for IO to complete. */ 
while (!list_empty(&io_list)) { 
if (!error) 
if (!xfs_buf_zone) 
if (!xfslogd_workqueue) 
file : ./test/kernel/fs/xfs/xfs_attr_remote.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
ifier 
if (bno != be64_to_cpu(rmt->rm_blkno)) 
if (offset != be32_to_cpu(rmt->rm_offset)) 
if (size != be32_to_cpu(rmt->rm_bytes)) 
if (ino != be64_to_cpu(rmt->rm_owner)) 
ify( 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (rmt->rm_magic != cpu_to_be32(XFS_ATTR3_RMT_MAGIC)) 
if (!uuid_equal(&rmt->rm_uuid, &mp->m_sb.sb_uuid)) 
if (be64_to_cpu(rmt->rm_blkno) != bno) 
if (be32_to_cpu(rmt->rm_bytes) > fsbsize - sizeof(*rmt)) 
if (be32_to_cpu(rmt->rm_offset) + 
if (rmt->rm_owner == 0) 
ify( 
ification of non-crc buffers */ 
while (len > 0) { 
if (!xfs_attr3_rmt_verify(mp, ptr, blksize, bno)) { 
if (bp->b_error) 
ify( 
ification of non-crc buffers */ 
while (len > 0) { 
ifier_error(bp); 
if (bip) { 
ify_read = xfs_attr3_rmt_read_verify, 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
while (len > 0 && *valuelen > 0) { 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
forwards */ 
forwards */ 
while (len > 0 && *valuelen > 0) { 
if (byte_cnt + hdr_size < blksize) { 
forwards */ 
forwards */ 
while (valuelen > 0) { 
if (error) 
for (i = 0; (i < nmap) && (valuelen > 0); i++) { 
if (error) 
if (error) 
forwards */ 
for it. 
for 
if (error) 
while (blkcnt > 0) { 
if (!error) { 
if (error) { 
if (committed) 
if (error) 
fore we turn off 
while (valuelen > 0) { 
if (error) 
if (!bp) 
if (error) 
forwards */ 
while (blkcnt > 0) { 
if (error) 
if (bp) { 
while (!done) { 
if (!error) { 
if (error) { 
if (committed) 
if (error) 
file : ./test/kernel/fs/xfs/xfs_dir2_data.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
format directory. 
for leaf lookup */ 
ifier, so we need to go the 
for zero bestfree entries. 
if (!bf[0].length) { 
if (!bf[1].length) { 
if (!bf[2].length) { 
while (p < endp) { 
for the space in the bestfree table. 
if (be16_to_cpu(dup->freetag) == XFS_DIR2_DATA_FREE_TAG) { 
if (dfp) { 
if (hdr->magic == cpu_to_be32(XFS_DIR2_BLOCK_MAGIC) || 
for (i = 0; i < be32_to_cpu(btp->count); i++) { 
if (hdr->magic == cpu_to_be32(XFS_DIR2_BLOCK_MAGIC) || 
for (i = stale = 0; i < be32_to_cpu(btp->count); i++) { 
if (i > 0) 
ify( 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
if (!uuid_equal(&hdr3->uuid, &mp->m_sb.sb_uuid)) 
if (be64_to_cpu(hdr3->blkno) != bp->b_bn) 
if (hdr3->magic != cpu_to_be32(XFS_DIR2_DATA_MAGIC)) 
if (__xfs_dir3_data_check(NULL, bp)) 
format of the directory. Hence we can either get a block 
ify( 
ify_read(bp); 
ify(bp); 
ifier_error(bp); 
ify( 
if (xfs_sb_version_hascrc(&mp->m_sb) && 
if (!xfs_dir3_data_verify(bp)) 
if (bp->b_error) 
ify( 
if (!xfs_dir3_data_verify(bp)) { 
ifier_error(bp); 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (bip) 
ify_read = xfs_dir3_data_read_verify, 
ify_read = xfs_dir3_data_reada_verify, 
if (!err && tp) 
if any that corresponds to it. 
ifdef DEBUG 
if 
ifdef DEBUG 
if we find the 
for it has to be exact. 
for (dfp = &bf[0], seenzero = matched = 0; 
if (!dfp->offset) { 
if (be16_to_cpu(dfp->offset) == off) { 
if (off < be16_to_cpu(dfp->offset)) 
if (dfp > &bf[0]) 
if 
if (be16_to_cpu(dup->length) < 
for our guy. 
for (dfp = &bf[0]; dfp < &bf[XFS_DIR2_DATA_FD_COUNT]; dfp++) { 
if (be16_to_cpu(dfp->offset) == off) 
if there are duplicate lengths. 
if (be16_to_cpu(new.length) > be16_to_cpu(dfp[0].length)) { 
if (be16_to_cpu(new.length) > be16_to_cpu(dfp[1].length)) { 
if (be16_to_cpu(new.length) > be16_to_cpu(dfp[2].length)) { 
if (dfp == &bf[0]) { 
if (dfp == &bf[1]) 
if (hdr->magic == cpu_to_be32(XFS_DIR2_BLOCK_MAGIC) || 
while (p < endp) { 
if (be16_to_cpu(dup->freetag) == XFS_DIR2_DATA_FREE_TAG) { 
for the created block. 
for the block. 
if (error) 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
for (i = 1; i < XFS_DIR2_DATA_FD_COUNT; i++) { 
for the block's body. 
ifdef DEBUG 
if 
fore us */ 
if (hdr->magic == cpu_to_be32(XFS_DIR2_DATA_MAGIC) || 
if it's free. 
if (offset > args->dp->d_ops->data_entry_offset) { 
fore us */ 
if (be16_to_cpu(prevdup->freetag) != XFS_DIR2_DATA_FREE_TAG) 
if the entry after 
if ((char *)hdr + offset + len < endptr) { 
if (be16_to_cpu(postdup->freetag) != XFS_DIR2_DATA_FREE_TAG) 
if (prevdup && postdup) { 
if prevdup and/or postdup are in bestfree table. 
if (!needscan) { 
if (dfp == &bf[1]) { 
fore us is free, merge with it. 
if (prevdup) { 
if (dfp) { 
if the new entry is big enough. 
if (postdup) { 
if (dfp) { 
if the new entry is big enough. 
for alignment with front and back of the entry. 
if (matchfront && matchback) { 
if (!needscan) 
if (matchfront) { 
if (dfp) { 
if there was a better 
for the last slot, or not.  Rescan. 
if (matchback) { 
if (dfp) { 
if there was a better 
for the last slot, or not.  Rescan. 
if the 3rd entry was valid, since these entries 
if (dfp) { 
if (!needscan) { 
file : ./test/kernel/fs/xfs/xfs_trans_resv.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
format structure overhead in the log in addition 
for a buffer.  Round the space required up 
for this overhead. 
format), 128); 
format, 
format forks in memory format use more space than the 
fork, we don't write all of this 
for sibling pointers which would waste 16 bytes of space. 
formation we actually log is greater than the size of the inode 
for object 
format object 
forks containing bmap btree root blocks. 
forks. 
format) + 
iffer slightly from that of the traditional inode allocation 
for inode chunks with at least one free 
ify' param indicates to include the record modification scenario. The 
for free space btree 
ifications. 
ify) 
if (!xfs_sb_version_hasfinobt(&mp->m_sb)) 
if (alloc) 
if (modify) 
for the overhead of the transaction mechanism. 
for the extents not freed is 
ify: 
for each of the ags: 4 * sector size 
ify: 
for the ags in which the blocks live: 3 * sector size 
for the free block count: sector size 
ify: 
fore ours in the agi hash list: inode cluster size 
for the ag in which the blocks live: sector size 
for the free block count: sector size 
ify: 
ify: 
for the ag in which the blocks live: 2 * sector size 
for the free block count: sector size 
ify case - allocation done by modification 
ify: 
for the nlink flag: sector size 
ification and allocation btrees) 
ify( 
for the nlink flag: sector size 
ify(mp)); 
for the nlink flag: sector size 
ify(mp)); 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
for remote symlink data which can be up to 1kB in 
ify: 
fore ours in the agi hash list: inode cluster size 
ification) 
ifree_reservation( 
for the transaction stuff. 
for bitmap/summary inode: max depth * blocksize 
for 1 block alloc: 2 * (2 * maxdepth - 1) * blocksize 
ification timestamp on a synchronous write. 
for block allocation) 
for the new directory block 
fork_reservation( 
fork of a file 
for each of the ags: 4 * sector size 
for allocations 
for allocations: sector size 
for the 1st bmap, here we calculate out the space unit for 
for the ag in which the blocks live: 2 * sector size 
for the free block count: sector size 
for changing quota flags: sector size 
if needed. 
for quota file extent allocation 
for the quota flags: sector size 
format and 
ifree.tr_logres = xfs_calc_ifree_reservation(mp); 
ifree.tr_logflags |= XFS_TRANS_PERM_LOG_RES; 
fork.tr_logres = xfs_calc_addafork_reservation(mp); 
fork.tr_logflags |= XFS_TRANS_PERM_LOG_RES; 
format with 
format */ 
file : ./test/kernel/fs/xfs/xfs_dquot_buf.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
for 2 reasons: 
while deleting the quotainode(s), and those blks got 
formation. Just don't complain about bad dquot blks. 
if (ddq->d_magic != cpu_to_be16(XFS_DQUOT_MAGIC)) { 
if (ddq->d_version != XFS_DQUOT_VERSION) { 
if (ddq->d_flags != XFS_DQ_USER && 
if (flags & XFS_QMOPT_DOWARN) 
if (id != -1 && id != be32_to_cpu(ddq->d_id)) { 
if (!errs && ddq->d_id) { 
if (!ddq->d_btimer) { 
if (ddq->d_ino_softlimit && 
if (!ddq->d_itimer) { 
if (ddq->d_rtb_softlimit && 
if (!ddq->d_rtbtimer) { 
if (!errs || !(flags & XFS_QMOPT_DQREPAIR)) 
if (flags & XFS_QMOPT_DOWARN) 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
ify_crc( 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if we are in log recovery, the quota subsystem has not been 
if (mp->m_quotainfo) 
for (i = 0; i < ndquots; i++, d++) { 
if (!uuid_equal(&d->dd_uuid, &mp->m_sb.sb_uuid)) 
ify( 
if we are in log recovery, the quota subsystem has not been 
if (mp->m_quotainfo) 
ify that each dquot is valid. 
for (i = 0; i < ndquots; i++) { 
if (i == 0) 
ify"); 
ify( 
if (!xfs_dquot_buf_verify_crc(mp, bp)) 
if (!xfs_dquot_buf_verify(mp, bp)) 
if (bp->b_error) 
ify( 
if (!xfs_dquot_buf_verify(mp, bp)) { 
ifier_error(bp); 
ify_read = xfs_dquot_buf_read_verify, 
file : ./test/kernel/fs/xfs/xfs_dir2.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
for directories that was 
for (i = 0, hash = 0; i < name->len; i++) 
if (args->namelen != len) 
for (i = 0; i < len; i++) { 
if (tolower(args->name[i]) != tolower(name[i])) 
if (!mp->m_dir_geo || !mp->m_attr_geo) { 
if (xfs_sb_version_hasasciici(&mp->m_sb)) 
if directory contains only "." and "..". 
if (dp->i_d.di_size == 0)	/* might happen during shutdown. */ 
if (dp->i_d.di_size > XFS_IFORK_DSIZE(dp)) 
if_u1.if_data; 
if (unlikely(XFS_TEST_ERROR(!ino_ok, mp, XFS_ERRTAG_DIR_INO_VALIDATE, 
if (error) 
if (!args) 
if (rval) 
if (!args) 
fork = XFS_DATA_FORK; 
if (dp->i_d.di_format == XFS_DINODE_FMT_LOCAL) { 
if (rval) 
if (v) { 
if (rval) 
if (v) 
for success (ie. name found) or an error. 
if (args->cmpresult == XFS_CMP_DIFFERENT) 
if (args->cmpresult != XFS_CMP_CASE || 
if (!args->value) 
if it differs 
for an exact match. 
if CI match */ 
for the ilock. 
fork = XFS_DATA_FORK; 
if (ci_name) 
if (dp->i_d.di_format == XFS_DINODE_FMT_LOCAL) { 
if (rval) 
if (v) { 
if (rval) 
if (v) 
if (rval == EEXIST) 
if (!rval) { 
if (ci_name) { 
if (!args) 
fork = XFS_DATA_FORK; 
if (dp->i_d.di_format == XFS_DINODE_FMT_LOCAL) { 
if (rval) 
if (v) { 
if (rval) 
if (v) 
if (rval) 
if (!args) 
fork = XFS_DATA_FORK; 
if (dp->i_d.di_format == XFS_DINODE_FMT_LOCAL) { 
if (rval) 
if (v) { 
if (rval) 
if (v) 
if this entry can be added to the directory without allocating space. 
if (resblks) 
if (!args) 
fork = XFS_DATA_FORK; 
if (dp->i_d.di_format == XFS_DINODE_FMT_LOCAL) { 
if (rval) 
if (v) { 
if (rval) 
if (v) 
for data and free blocks, not leaf/node blocks which are 
if (error) 
if this is the data space and it grew. 
if (space == XFS_DIR2_DATA_SPACE) { 
if (size > dp->i_d.di_size) { 
if the directory is a single-block form directory. 
if ((rval = xfs_bmap_last_offset(args->dp, &last, XFS_DATA_FORK))) 
if the directory is a single-leaf form directory. 
if ((rval = xfs_bmap_last_offset(args->dp, &last, XFS_DATA_FORK))) 
for data and free blocks, leaf/node are done 
if ((error = xfs_bunmapi(tp, dp, da, args->geo->fsbcount, 
if we're in a removename with 
for un-fragmented 
if (db >= xfs_dir2_byte_to_db(args->geo, XFS_DIR2_LEAF_OFFSET)) 
if (dp->i_d.di_size > xfs_dir2_db_off_to_byte(args->geo, db + 1, 0)) 
if ((error = xfs_bmap_last_before(tp, dp, &bno, XFS_DATA_FORK))) { 
if (db == args->geo->datablk) 
file : ./test/kernel/fs/xfs/xfs_dir2_node.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
ifdef DEBUG 
if (!xfs_dir3_leafn_check((dp), (bp))) \ 
while (0); 
if (leafhdr.magic == XFS_DIR3_LEAFN_MAGIC) { 
if (be64_to_cpu(leaf3->info.blkno) != bp->b_bn) 
if (leafhdr.magic != XFS_DIR2_LEAFN_MAGIC) 
if 
ify( 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
if (hdr3->magic != cpu_to_be32(XFS_DIR3_FREE_MAGIC)) 
if (!uuid_equal(&hdr3->uuid, &mp->m_sb.sb_uuid)) 
if (be64_to_cpu(hdr3->blkno) != bp->b_bn) 
if (hdr->magic != cpu_to_be32(XFS_DIR2_FREE_MAGIC)) 
ify( 
if (xfs_sb_version_hascrc(&mp->m_sb) && 
if (!xfs_dir3_free_verify(bp)) 
if (bp->b_error) 
ify( 
if (!xfs_dir3_free_verify(bp)) { 
ifier_error(bp); 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (bip) 
ify_read = xfs_dir3_free_read_verify, 
if it lands in a hole */ 
if (error) 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
ifdef DEBUG 
if 
format directory to a node-format directory. 
if ((error = xfs_dir2_grow_inode(args, XFS_DIR2_FREE_SPACE, &fdb))) { 
for the new freespace block. 
if (error) 
for (i = n = 0; i < be32_to_cpu(ltp->bestcount); i++, from++, to++) { 
format and back again. 
if (leaf->hdr.info.magic == cpu_to_be16(XFS_DIR2_LEAF1_MAGIC)) 
form directory. 
for new entry */ 
if (index < 0) 
if there are no stale entries it won't fit. 
if (leafhdr.count == dp->d_ops->leaf_max_ents(args->geo)) { 
if (args->op_flags & XFS_DA_OP_JUSTCHECK) 
if (compact) 
if (leafhdr.stale) { 
for this case. 
ifdef DEBUG 
if	/* DEBUG */ 
if (count) 
if (!leafhdr.count) 
for space to add a name in a node-format leaf block. 
for_addname( 
if (state->extravalid) { 
for (lep = &ents[index]; 
if (be32_to_cpu(lep->address) == XFS_DIR2_NULL_DATAPTR) 
for a place to put the new entry. 
if there is one with room. 
if (newdb != curdb) { 
formation. 
if (newfdb != curfdb) { 
fore, drop it. 
if (curbp) 
if (error) 
for our entry. 
if (unlikely(bests[fi] == cpu_to_be16(NULLDATAOFF))) { 
if (curfdb != newfdb) 
if (be16_to_cpu(bests[fi]) >= length) 
if (curbp) { 
for 
format leaf block. 
for_entry( 
if (state->extravalid) { 
for (lep = &ents[index]; 
if (be32_to_cpu(lep->address) == XFS_DIR2_NULL_DATAPTR) 
ifferent data block, go get it. 
if (newdb != curdb) { 
fore that we aren't saving 
if (curbp && (args->cmpresult == XFS_CMP_DIFFERENT || 
if (args->cmpresult != XFS_CMP_DIFFERENT && 
if (error) 
if it's an exact match, return 
if (cmp != XFS_CMP_DIFFERENT && cmp != args->cmpresult) { 
if (args->cmpresult != XFS_CMP_DIFFERENT && 
if (cmp == XFS_CMP_EXACT) 
if (curbp) { 
if (state->extrablk.bp != curbp) 
format leaf block. 
if (args->op_flags & XFS_DA_OP_ADDNAME) 
for_addname(bp, args, indexp, 
for_entry(bp, args, indexp, state); 
if nothing to do. 
if (count == 0) 
if (start_d < dhdr->count) { 
if (shdr->stale) { 
for (i = start_s, stale = 0; i < start_s + count; i++) { 
if (start_s + count < shdr->count) { 
if both are valid and leaf2 should be before leaf1, else 0. 
if (hdr1.count > 0 && hdr2.count > 0 && 
if defined(DEBUG) || defined(XFS_WARN) 
if 
if ((swap = xfs_dir2_leafn_order(dp, blk1->bp, blk2->bp))) { 
for block swap */ 
if defined(DEBUG) || defined(XFS_WARN) 
if 
if (oldsum & 1) { 
if (mid >= hdr1.count) 
for the new entry. 
if (count > 0) 
if (count < 0) 
if (hdr1.count < hdr2.count) 
if (hdr1.count > hdr2.count) 
for insertion. 
if (!state->inleaf) 
if (blk2->index < 0) { 
if (hdr) { 
if (findex == freehdr.nvalid - 1) { 
for (i = findex - 1; i >= 0; i--) { 
if we can. 
if (!freehdr.nused) { 
if (error == 0) { 
if (error != ENOSPC || args->total != 0) 
if there is no 
if (logfree) 
if necessary. 
for bestfree. 
if (needscan) 
if (needlog) 
if (longest < be16_to_cpu(bf[0].length)) { 
if (error) 
ifdef DEBUG 
if 
if (longest == args->geo->blksize - 
if (error == 0) { 
if there's no space reservation. 
if (!(error == ENOSPC && args->total == 0)) 
if (error) 
ify trying to join it with a neighbor. 
for a new leaf node. 
if (error) { 
if (error) 
if (error) { 
if (state->inleaf) 
if the block should be 
forward;	/* sibling block direction */ 
ift */ 
for the degenerate case of the block being over 50% full. 
if (bytes > (state->args->geo->blksize >> 1)) { 
for the degenerate case of the block being empty. 
forward block unless it is NULL. 
if (count == 0) { 
forward = (leafhdr.forw != 0); 
ift(state, &state->altpath, forward, 0, 
if (error) 
if we can coalesce with 
forward or the backward block. 
forward = leafhdr.forw < leafhdr.back; 
forward ? leafhdr.forw : leafhdr.back; 
if (error) 
if (bytes >= 0) 
if (i >= 2) { 
if (blkno < blk->blkno) 
forward, 0, 
ift(state, &state->path, forward, 0, 
if (error) { 
if (drophdr.stale) 
if (savehdr.stale) 
if (xfs_dir2_leafn_order(dp, save_blk->bp, drop_blk->bp)) 
form directory addname routine. 
for insert */ 
if (error) 
if (rval != ENOENT) { 
if (rval) { 
if (rval == 0) { 
if (!(args->op_flags & XFS_DA_OP_JUSTCHECK)) 
if (args->total == 0) { 
for a node-format directory name addition. 
ifbno;		/* initial freespace block no */ 
for that data entry. 
if (fblk) { 
ifbno = fblk->blkno; 
for our entry, so we remembered it. 
if (findex >= 0) { 
ifbno = dbno = -1; 
for one.  Figure out what the 
if (dbno == -1) { 
if ((error = xfs_bmap_last_offset(dp, &fo, XFS_DATA_FORK))) 
ifbno; 
ified a data block, search the freeblock 
for a good data block.  If we find a null freeblock entry, 
while (dbno == -1) { 
if (fbp == NULL) { 
if (++fbno == 0) 
ifbno we already looked at it. 
if (fbno == ifbno) 
if (fbno >= lastfbno) 
if (error) 
if (!fbp) 
if they are placed 
if (be16_to_cpu(bests[findex]) != NULLDATAOFF && 
if (++findex == freehdr.nvalid) { 
if (fblk && fblk->bp) 
if (unlikely(dbno == -1)) { 
if ((args->op_flags & XFS_DA_OP_JUSTCHECK) || args->total == 0) 
if (unlikely((error = xfs_dir2_grow_inode(args, 
if (fbp) 
if (fblk && fblk->bp) 
if (error) 
if (!fbp) { 
if (error) 
if (dp->d_ops->db_to_fdb(args->geo, dbno) != fbno) { 
for\n" 
ifbno, lastfbno); 
for the new block. 
if (error) 
if (findex >= freehdr.nvalid) { 
for an empty data block 
if (bests[findex] == cpu_to_be16(NULLDATAOFF)) { 
if (args->op_flags & XFS_DA_OP_JUSTCHECK) 
if (error) 
for us. 
if needed. 
if (needscan) 
if needed. 
if (needlog) 
if (be16_to_cpu(bests[findex]) != be16_to_cpu(bf[0].length)) { 
if needed. 
if (logfree) 
format directory. 
if (error) 
if (rval == ENOENT && args->cmpresult == XFS_CMP_CASE) { 
for (i = 0; i < state->path.active; i++) { 
if we have it. 
if (state->extravalid && state->extrablk.bp) { 
format directory. 
if (error) 
if (rval != EEXIST) { 
if (error) 
if (rval && state->path.active > 1) 
format. 
if (!error) 
format directory. 
if (error) { 
if (rval == EEXIST) { 
if (state->extravalid) { 
for (i = 0; i < state->path.active; i++) { 
if we did it, 0 if not. 
if (error) 
if (!bp) 
if (freehdr.nused > 0) { 
if (error) { 
file : ./test/kernel/fs/xfs/xfs_discard.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
if (error || !agbp) 
fore we took the AGF buffer lock are now on disk, and the 
force(mp, XFS_LOG_SYNC); 
if (error) 
while (i) { 
if (error) 
format for all range/len calculations as that is 
if (dlen < minlen) { 
for now. 
if (dbno + dlen < start || dbno > end) { 
if (xfs_extent_busy_search(mp, agno, fbno, flen)) { 
if (error) 
if (error) 
format we use for filesystem block 
while the incoming format 
for determining the correct offset and regions to trim. 
if (!capable(CAP_SYS_ADMIN)) 
if (!blk_queue_discard(q)) 
if (copy_from_user(&range, urange, sizeof(range))) 
for values 
if (range.start >= XFS_FSB_TO_B(mp, mp->m_sb.sb_dblocks) || 
if (end > XFS_FSB_TO_BB(mp, mp->m_sb.sb_dblocks) - 1) 
for (agno = start_agno; agno <= end_agno; agno++) { 
if (error) 
if (last_error) 
if (copy_to_user(urange, &range, sizeof(range))) 
for_each_entry(busyp, list, list) { 
if (error && error != EOPNOTSUPP) { 
for extent [0x%llu,%u], error %d", 
file : ./test/kernel/fs/xfs/xfs_ialloc.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
if (xfs_sb_version_hasalign(&args->mp->m_sb) && 
if (!error && *stat == 1) { 
for (thisino = newino; 
if (error) { 
if (error) { 
ify that the number of free inodes in the AGI is correct. 
ifdef DEBUG 
if (cur->bc_nlevels == 1) { 
if (error) 
if (error) 
if (i) { 
if (error) 
while (i == 1); 
if 
for writeback rather than the xfsbufd queue). 
format, then use the new inode version.  Otherwise use the old 
for v3 inode we log the entire buffer rather than just the 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
if they were physically logged. 
if (tp) 
for (j = 0; j < nbufs; j++) { 
if (!fbuf) 
for (i = 0; i < inodes_per_cluster; i++) { 
if (version == 3) { 
if (tp) { 
if (tp) { 
fore 
forward in the log). 
if (version == 3) { 
ified by agbp. 
for success, else error code. 
if (args.mp->m_maxicount && 
if (likely(newino != NULLAGINO && 
ify the free list if we fail to have an exact 
for alignment, 
for the inode btree to split. */ 
if ((error = xfs_alloc_vextent(&args))) 
if the AG can 
if (unlikely(args.fsbno == NULLFSBLOCK)) { 
for the allocation. 
for inodes in filesystem block size 
if (args.mp->m_sinoalign) { 
for the inode btree to split. 
if ((error = xfs_alloc_vextent(&args))) 
if (isaligned && args.fsbno == NULLFSBLOCK) { 
if ((error = xfs_alloc_vextent(&args))) 
if (args.fsbno == NULLFSBLOCK) { 
if a chunk is 
if (error) 
if (error) 
if (xfs_sb_version_hasfinobt(&args.mp->m_sb)) { 
if (error) 
ify/log superblock values for inode count and inode free count. 
if (++mp->m_agirotor >= mp->m_maxagi) 
for a free inode in, based on the parent 
for inode allocation */ 
if length > 0 
if (S_ISDIR(mode)) 
if (pagno >= agcount) 
for one with a little 
for them, 
for (;;) { 
if (!pag->pagi_inodeok) { 
if (!pag->pagi_init) { 
if (error) 
if (pag->pagi_freecount) { 
if (!okalloc) 
if (!pag->pagf_init) { 
if (error) 
for the file plus a block of 
if (!longest) 
if (pag->pagf_freeblks >= needspace + ineed && 
if we're shutting 
if (XFS_FORCED_SHUTDOWN(mp)) 
if (agno >= agcount) 
if (agno == pagno) { 
if (left) 
if (error) 
if (i) { 
if (error) 
if (error) 
if (i) { 
if (error) 
if (!pagino) 
if (error) 
if (pagno == agno) { 
if (error) 
if (error) 
if (rec.ir_freecount > 0) { 
if (error) 
if same parent inode. 
if (pagino != NULLAGINO && 
if (error) 
if (error) 
if (error) 
forward 1 record. */ 
if (error) 
while (!doneleft || !doneright) { 
if (!--searchdistance) { 
if both are valid. */ 
if (useleft && trec.ir_freecount) { 
if (!useleft && rec.ir_freecount) { 
if (useleft) { 
if (error) 
ifferent AG from the parent. 
if (agi->agi_newino != cpu_to_be32(NULLAGINO)) { 
if (error) 
if (i == 1) { 
if (error) 
if (j == 1 && rec.ir_freecount > 0) { 
if (error) 
for (;;) { 
if (error) 
if (rec.ir_freecount > 0) 
if (error) 
if (error) 
if (error) 
if (error) 
if (i == 1) { 
if (error) 
if we've landed in the parent inode record. The finobt 
if (pagino >= rec->ir_startino && 
if (error) 
if (error) 
if (j == 1) { 
if (error) 
if (i == 1 && j == 1) { 
if ((pagino - rec->ir_startino + XFS_INODES_PER_CHUNK - 1) > 
if (j == 1) { 
if (i == 1) { 
if (agi->agi_newino != cpu_to_be32(NULLAGINO)) { 
if (error) 
if (i == 1) { 
if (error) 
if (error) 
if (error) 
ification made to the finobt. Also ensure that 
if (error) 
if (error) 
if (error) 
if available. Otherwise, fall 
for us, and made sure that free inodes are 
if (!xfs_sb_version_hasfinobt(&mp->m_sb)) 
if (!pagino) 
if (error) 
if (agno == pagno) 
if (error) 
ify or remove the finobt record. 
if (rec.ir_freecount) 
if (error) 
formation. 
if (error) 
if (error) 
fore we can check the freecount for each btree. 
if (error) 
if (error) 
if it has to do an allocation 
formn an allocation, an inode 
if (*IO_agbp) { 
fore.  In this case, we 
for inode allocation. 
if (start_agno == NULLAGNUMBER) { 
for a free 
if (mp->m_maxicount && 
for (;;) { 
if (!pag->pagi_inodeok) { 
if (!pag->pagi_init) { 
if (error) 
if this AG is usable. 
if (!pag->pagi_freecount && !okalloc) 
if (error) 
if (pag->pagi_freecount) { 
if (!okalloc) 
if (error) { 
if (error != ENOSPC) 
if (ialloced) { 
if (++agno == mp->m_sb.sb_agcount) 
if (agno == start_agno) { 
ifree_inobt( 
if (error) 
for the entry describing this inode. 
if ((error = xfs_inobt_lookup(cur, agino, XFS_LOOKUP_LE, &i))) { 
if (error) { 
for removal 
if (!(mp->m_flags & XFS_MOUNT_IKEEP) && 
if ((error = xfs_btree_delete(cur, &i))) { 
if (error) { 
if (error) 
ifree_finobt( 
if (error) 
if (i == 0) { 
if (error) 
ifications independently, we can catch 
if (error) 
ifecycle of records in the finobt is different from 
if all of the inodes are free and we aren't 
formation. 
if (rec.ir_freecount == mp->m_ialloc_inos && 
if (error) 
if (error) 
if (error) 
ifree( 
if inode cluster was deleted */ 
for allocation group header */ 
for filesystem */ 
if (agno >= mp->m_sb.sb_agcount)  { 
if (inode != XFS_AGINO_TO_INO(mp, agno, agino))  { 
if (agbno >= mp->m_sb.sb_agblocks)  { 
if (error) { 
ifree_inobt(mp, tp, agbp, agino, flist, deleted, first_ino, 
if (error) 
if (xfs_sb_version_hasfinobt(&mp->m_sb)) { 
if (error) 
if (error) { 
for the given agino. If the record cannot be 
if (!error) { 
if (!error && i == 0) 
if (error) 
if (rec.ir_startino > agino || 
for untrusted inodes check it is allocated first */ 
for mapping it into a buffer. 
for inode btree lookup */ 
if (agno >= mp->m_sb.sb_agcount || agbno >= mp->m_sb.sb_agblocks || 
ifdef DEBUG 
formation for untrusted inodes 
if (flags & XFS_IGET_UNTRUSTED) 
if (agno >= mp->m_sb.sb_agcount) { 
if (agbno >= mp->m_sb.sb_agblocks) { 
if (ino != XFS_AGINO_TO_INO(mp, agno, agino)) { 
if /* DEBUG */ 
ify is valid. We cannot do this just by reading 
if (flags & XFS_IGET_UNTRUSTED) { 
if (error) 
if (blks_per_cluster == 1) { 
if (mp->m_inoalign_mask) { 
if (error) 
if ((imap->im_blkno + imap->im_len) > 
for (level = 1; maxblocks > 1; level++) 
ified fields for the ag hdr (inode section). The growth of the agi 
fore agi_unlinked and a field after 
ifdef DEBUG 
if 
for the first and last fields in the first 
if (fields & XFS_AGI_ALL_BITS_R1) { 
for any bits in the second region. 
if (fields) { 
ifdef DEBUG 
for (i = 0; i < XFS_AGI_UNLINKED_BUCKETS; i++) 
if 
ify( 
if (xfs_sb_version_hascrc(&mp->m_sb) && 
if (agi->agi_magicnum != cpu_to_be32(XFS_AGI_MAGIC)) 
if (!XFS_AGI_GOOD_VERSION(be32_to_cpu(agi->agi_versionnum))) 
for any useful checking. growfs ensures we can't 
if (bp->b_pag && be32_to_cpu(agi->agi_seqno) != bp->b_pag->pag_agno) 
ify( 
if (xfs_sb_version_hascrc(&mp->m_sb) && 
if (XFS_TEST_ERROR(!xfs_agi_verify(bp), mp, 
if (bp->b_error) 
ify( 
if (!xfs_agi_verify(bp)) { 
ifier_error(bp); 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (bip) 
ify_read = xfs_agi_read_verify, 
if (error) 
if (error) 
if (!pag->pagi_init) { 
if 
forced shutdown. 
if (error) 
if (bp) 
file : ./test/kernel/fs/xfs/xfs_attr_list.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
form_compare(const void *a, const void *b) 
if (sa->hash < sb->hash) { 
if (sa->hash > sb->hash) { 
form attribute lists for attr_list(). 
fore 
form_list(xfs_attr_list_context_t *context) 
form_t *sf; 
if_u1.if_data; 
if (!sf->hdr.count) 
if (context->bufsize == 0 || 
if_bytes + sf->hdr.count * 16) < context->bufsize)) { 
for (i = 0, sfe = &sf->list[0]; i < sf->hdr.count; i++) { 
if (context->seen_enough) 
if (error) 
for a search callback */ 
for the rest of the entries, storing 
for (i = 0, sfe = &sf->list[0]; i < sf->hdr.count; i++) { 
if_bytes)))) { 
form_list", 
form_compare); 
for (sbp = sbuf, i = 0; i < nsbuf; i++, sbp++) { 
if (cursor->offset == count) { 
if (sbp->hash > cursor->hashval) { 
if (i == nsbuf) { 
for ( ; i < nsbuf; i++, sbp++) { 
if (error) 
if (context->seen_enough) 
if (cursor->blkno > 0) { 
if ((error != 0) && (error != EFSCORRUPTED)) 
if (bp) { 
if (cursor->hashval > be32_to_cpu( 
if (cursor->hashval <= be32_to_cpu( 
if (bp == NULL) { 
for (;;) { 
if (error) 
if (magic == XFS_ATTR_LEAF_MAGIC || 
if (magic != XFS_DA_NODE_MAGIC && 
for (i = 0; i < nodehdr.count; btree++, i++) { 
fore); 
if (i == nodehdr.count) { 
formation. 
for (;;) { 
if (error) { 
if (context->seen_enough || leafhdr.forw == 0) 
forw; 
if (error) 
for attr_list(), for leaf attribute lists. 
if this is a new syscall. 
if (context->resynch) { 
for (i = 0; i < ichdr.count; entry++, i++) { 
if (cursor->offset == context->dupcnt) { 
if (be32_to_cpu(entry->hashval) > 
if (i == ichdr.count) { 
for (; i < ichdr.count; entry++, i++) { 
if (entry->flags & XFS_ATTR_INCOMPLETE) 
if (entry->flags & XFS_ATTR_LOCAL) { 
if (retval) 
if (context->put_value) { 
fork = XFS_ATTR_FORK; 
if (retval) 
if (retval) 
if (context->seen_enough) 
for attr_list(), for leaf attribute lists. 
if (error) 
if (XFS_FORCED_SHUTDOWN(dp->i_mount)) 
if (!xfs_inode_hasattr(dp)) { 
if (dp->i_d.di_aformat == XFS_DINODE_FMT_LOCAL) { 
form_list(context); 
if (((context->flags & ATTR_SECURE) == 0) != 
if (((context->flags & ATTR_ROOT) == 0) != 
if (context->firstu < arraytop) { 
if (cursor->pad1 || cursor->pad2) 
if ((cursor->initted == 0) && 
for a properly aligned buffer. 
if (((long)buffer) & (sizeof(int)-1)) 
if (flags & ATTR_KERNOVAL) 
file : ./test/kernel/fs/xfs/xfs_qm.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
for the entire 
while (1) { 
if (!nr_found) { 
for (i = 0; i < nr_found; i++) { 
if (error == EAGAIN) { 
if (error && last_error != EFSCORRUPTED) 
if the filesystem is corrupted.  */ 
if (skipped) { 
if ((dqp->dq_flags & XFS_DQ_FREEING) || dqp->q_nrefs != 0) { 
if 
if (XFS_DQ_IS_DIRTY(dqp)) { 
if (error) { 
if (flags & XFS_QMOPT_UQUOTA) 
if (flags & XFS_QMOPT_GQUOTA) 
if (flags & XFS_QMOPT_PQUOTA) 
if (mp->m_quotainfo) { 
for 
if (mp->m_sb.sb_rextents) { 
for realtime filesystem"); 
if necessary. 
if (error) { 
if (XFS_QM_NEED_QUOTACHECK(mp)) { 
if (error) { 
for 
if (!XFS_IS_UQUOTA_ON(mp)) 
if (!XFS_IS_GQUOTA_ON(mp)) 
if (!XFS_IS_PQUOTA_ON(mp)) 
if (sbf != (mp->m_qflags & XFS_MOUNT_QUOTA_ALL)) { 
if (error) { 
fore we flush quotas and blow away the quotainfo structure. 
if (mp->m_rbmip) 
if (mp->m_rsumip) 
if (mp->m_quotainfo) { 
if (mp->m_quotainfo->qi_gquotaip) { 
if (mp->m_quotainfo->qi_pquotaip) { 
if we already have it in the inode itself. IO_idqpp is &i_udquot 
if (dqp) { 
if dquot didn't 
if (error) 
if (!XFS_IS_QUOTA_RUNNING(mp)) 
if (!XFS_IS_QUOTA_ON(mp)) 
if (!XFS_NOT_DQATTACHED(mp, ip)) 
if (xfs_is_quota_inode(&mp->m_sb, ip->i_ino)) 
if needed. 
if (!xfs_qm_need_dqattach(ip)) 
if (XFS_IS_UQUOTA_ON(mp) && !ip->i_udquot) { 
if (error) 
if (XFS_IS_GQUOTA_ON(mp) && !ip->i_gdquot) { 
if (error) 
if (XFS_IS_PQUOTA_ON(mp) && !ip->i_pdquot) { 
if (error) 
fore any 
if (!xfs_qm_need_dqattach(ip)) 
if any. 
if (!(ip->i_udquot || ip->i_gdquot || ip->i_pdquot)) 
if (ip->i_udquot) { 
if (ip->i_gdquot) { 
if (ip->i_pdquot) { 
if (!xfs_dqlock_nowait(dqp)) 
if (dqp->q_nrefs) { 
for the IO to complete before we try to 
if (!xfs_dqflock_nowait(dqp)) { 
if (XFS_DQ_IS_DIRTY(dqp)) { 
if (error) { 
if ((sc->gfp_mask & (__GFP_FS|__GFP_WAIT)) != (__GFP_FS|__GFP_WAIT)) 
if (error) 
while (!list_empty(&isol.dispose)) { 
formation that's kept in the 
if (error) 
if quotainodes are setup, and if not, allocate them, 
if (error) 
if user quotas 
for user 
if (!error) { 
fore he or she can not perform any 
if (qi->qi_uquotaip) { 
if (qi->qi_gquotaip) { 
if (qi->qi_pquotaip) { 
for sb_pquotino and 
if (!xfs_sb_version_has_pquotino(&mp->m_sb) && 
if ((flags & XFS_QMOPT_PQUOTA) && 
if ((flags & XFS_QMOPT_GQUOTA) && 
if (ino != NULLFSINO) { 
if (error) 
if (error) { 
if (!*ip) { 
if (error) { 
for example. 
if (flags & XFS_QMOPT_SBVERSION) { 
if (flags & XFS_QMOPT_UQUOTA) 
if (flags & XFS_QMOPT_GQUOTA) 
if ((error = xfs_trans_commit(tp, XFS_TRANS_RELEASE_LOG_RES))) { 
ifdef DEBUG 
if 
for (j = 0; j < mp->m_quotainfo->qi_dqperchunk; j++) { 
if needed, repair the dqblk. Don't 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
for the whole thing up front, 
if we were to crash in the middle of this loop. 
while (blkcnt--) { 
if (error == EFSCORRUPTED) { 
if (error) 
for every chunk of dquots that we find. 
if (qip->i_d.di_nblocks == 0) 
if (error) 
for (i = 0; i < nmaps; i++) { 
if (map[i].br_startblock == HOLESTARTBLOCK) 
if ((i+1 < nmaps) && 
while (rablkcnt--) { 
if (error) 
while (nmaps > 0); 
if (error) { 
if (nblks) { 
if (rtblks) { 
for the default values set in the root dquot. 
if (dqp->q_core.d_id) { 
ifork_t	*ifp;			/* inode fork pointer */ 
ifp = XFS_IFORK_PTR(ip, XFS_DATA_FORK); 
if ((error = xfs_iread_extents(NULL, ip, XFS_DATA_FORK))) 
ifp->if_bytes / (uint)sizeof(xfs_bmbt_rec_t); 
for (idx = 0; idx < nextents; idx++) 
for resources taken by that inode. 
for filesystem */ 
for, not so with the quota 
if (xfs_is_quota_inode(&mp->m_sb, ino)) { 
if (error) { 
if (XFS_IS_REALTIME_INODE(ip)) { 
if (error) 
if we crash in the middle of a 'quotacheck' 
if (XFS_IS_UQUOTA_ON(mp)) { 
if (error) 
if (XFS_IS_GQUOTA_ON(mp)) { 
if (error) 
if (XFS_IS_PQUOTA_ON(mp)) { 
if (error) 
if (dqp->dq_flags & XFS_DQ_FREEING) 
if (!XFS_DQ_IS_DIRTY(dqp)) 
if (error) 
if (uip) { 
if (error) 
if (gip) { 
if (error) 
if (pip) { 
if (error) 
if (error) 
while (!done); 
if everything was updated successfully. 
if (XFS_IS_UQUOTA_ON(mp)) { 
if (XFS_IS_GQUOTA_ON(mp)) { 
if (!error) 
if (XFS_IS_PQUOTA_ON(mp)) { 
if (!error) 
if (!error) 
if we couldn't do a dquot allocation inside 
if (error) { 
for 
while (!list_empty(&buffer_list)) { 
if (error) { 
if (xfs_mount_reset_sbqflags(mp)) { 
if (xfs_sb_version_hasquota(&mp->m_sb)) { 
if (error) 
if (XFS_IS_GQUOTA_ON(mp) && 
if (error) 
if (XFS_IS_PQUOTA_ON(mp) && 
if (error) 
if they don't exist already. The changes 
if (XFS_IS_UQUOTA_ON(mp) && uip == NULL) { 
if (error) 
if (XFS_IS_GQUOTA_ON(mp) && gip == NULL) { 
if (error) 
if (XFS_IS_PQUOTA_ON(mp) && pip == NULL) { 
if (error) 
if (uip) 
if (gip) 
if (pip) 
if (error) { 
for vnodeops ---------------- */ 
if (!XFS_IS_QUOTA_RUNNING(mp) || !XFS_IS_QUOTA_ON(mp)) 
if ((flags & XFS_QMOPT_INHERIT) && XFS_INHERIT_GID(ip)) 
if necessary. The dquot(s) will not be locked. 
if (XFS_NOT_DQATTACHED(mp, ip)) { 
if (error) { 
if ((flags & XFS_QMOPT_UQUOTA) && XFS_IS_UQUOTA_ON(mp)) { 
if we send the inode to dqget, the uid of the inode 
if 
while 
if (error) { 
if ((flags & XFS_QMOPT_GQUOTA) && XFS_IS_GQUOTA_ON(mp)) { 
if (error) { 
if ((flags & XFS_QMOPT_PQUOTA) && XFS_IS_PQUOTA_ON(mp)) { 
if (error) { 
if (uq) 
if (O_udqpp) 
if (uq) 
if (O_gdqpp) 
if (gq) 
if (O_pdqpp) 
if (pq) 
if (gq) 
if (uq) 
ifications. 
for setattr(AT_UID|AT_GID|AT_PROJID). 
if (XFS_IS_UQUOTA_ON(mp) && udqp && 
if (delblks) { 
if (XFS_IS_GQUOTA_ON(ip->i_mount) && gdqp && 
if (delblks) { 
if (XFS_IS_PQUOTA_ON(ip->i_mount) && pdqp && 
if (delblks) { 
if (error) 
if a reservation fails 
if (delblks) { 
if (error) 
if (!XFS_IS_QUOTA_RUNNING(mp) || !XFS_IS_QUOTA_ON(mp)) 
for (i = 0; (i < 4 && i_tab[i]); i++) { 
for duplicate entries in the table. 
if (i == 0 || ip != i_tab[i-1]) { 
if (error) 
if (!XFS_IS_QUOTA_RUNNING(mp) || !XFS_IS_QUOTA_ON(mp)) 
if (udqp && XFS_IS_UQUOTA_ON(mp)) { 
if (gdqp && XFS_IS_GQUOTA_ON(mp)) { 
if (pdqp && XFS_IS_PQUOTA_ON(mp)) { 
file : ./test/kernel/fs/xfs/uuid.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
for user-level file handles. 
if (uuid == NULL) 
for (i = 0; i < sizeof *uuid; i++) 
file : ./test/kernel/fs/xfs/xfs_aops.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
if (buffer_unwritten(bh)) 
if (buffer_delay(bh)) 
while ((bh = bh->b_this_page) != head); 
for_inode( 
if (XFS_IS_REALTIME_INODE(ip)) 
for good with this ioend structure. 
for (bh = ioend->io_buffer_head; bh; bh = next) { 
if this write could update the on-disk inode size. 
if (error) { 
for freeze protection. 
if (!isize) { 
if (atomic_dec_and_test(&ioend->io_remaining)) { 
if (ioend->io_type == XFS_IO_UNWRITTEN) 
if (ioend->io_append_trans || 
if (XFS_FORCED_SHUTDOWN(ip->i_mount)) { 
if (ioend->io_error) 
if (ioend->io_type == XFS_IO_UNWRITTEN) { 
if (ioend->io_isdirect && xfs_ioend_is_append(ioend)) { 
if we need to allocate blocks 
for some time, we're stuck with doing 
if (error) 
if (ioend->io_append_trans) { 
if (error) 
if (atomic_dec_and_test(&ioend->io_remaining)) 
for updating the ondisk inode size later 
fore we have started 
if (XFS_FORCED_SHUTDOWN(mp)) 
if (type == XFS_IO_UNWRITTEN) 
if (!xfs_ilock_nowait(ip, XFS_ILOCK_SHARED)) { 
format != XFS_DINODE_FMT_BTREE || 
if (offset + count > mp->m_super->s_maxbytes) 
if (error) 
if (type == XFS_IO_DELALLOC && 
if (!error) 
ifdef DEBUG 
if 
for buffered IO. 
if (clear_dirty) 
for_io(page); 
if (!buffers) 
for all of the ioends we have saved up, covering the 
fore we submit them for I/O. If we mark the 
fore we mark the other 
for the one page as the code in end_buffer_async_write() 
for I/O on the second pass. 
for writeback 
for (bh = ioend->io_buffer_head; bh; bh = bh->b_private) 
while ((ioend = next) != NULL); 
if (fail) { 
for (bh = ioend->io_buffer_head; bh; bh = bh->b_private) { 
if (bh->b_blocknr != lastblock + 1) { 
if (xfs_bio_add_buffer(bio, bh) != bh->b_size) { 
if (bio) 
while ((ioend = next) != NULL); 
for the initial page 
while ((bh = next_bh) != NULL); 
while ((ioend = next) != NULL); 
if so, we try to append to this ioend if we 
if we've finished the given ioend. 
if (!ioend || need_ioend || type != ioend->io_type) { 
if (previous) 
if a given page contains at least one buffer of a given @type. 
for a match. 
if (PageWriteback(page)) 
if (!page->mapping) 
if (!page_has_buffers(page)) 
if (buffer_unwritten(bh)) { 
if (buffer_delay(bh)) { 
if (buffer_dirty(bh) && buffer_mapped(bh)) { 
if (!check_all_buffers) 
while ((bh = bh->b_this_page) != head); 
for page given the extent map. Write it out. 
for the original page it is possible 
if (page->index != tindex) 
if (!trylock_page(page)) 
if (PageWriteback(page)) 
if (page->mapping != inode->i_mapping) 
if (!xfs_check_page_type(page, (*ioendp)->io_type, false)) 
fore 
for more optimal IO patterns, we should always 
if (!xfs_imap_valid(inode, imap, end_offset)) 
ification or can't be written, abort the loop and start 
if (offset >= end_offset) 
if (!buffer_uptodate(bh)) 
if (!(PageUptodate(page) || buffer_uptodate(bh))) { 
if (buffer_unwritten(bh) || buffer_delay(bh) || 
if (buffer_unwritten(bh)) 
if (buffer_delay(bh)) 
if (type != XFS_IO_OVERWRITE) 
while (offset += len, (bh = bh->b_this_page) != head); 
if (count) { 
while (!done && tindex <= tlast) { 
if (!pagevec_lookup(&pvec, inode->i_mapping, tindex, len)) 
for (i = 0; i < pagevec_count(&pvec); i++) { 
if (done) 
fore we 
if a direct IO read 
fore 
if we get ENOSPC errors, we have to be able to do this 
for block 
formance critical path, so for now just do the punching a 
if (!xfs_check_page_type(page, XFS_IO_DELALLOC, true)) 
if (XFS_FORCED_SHUTDOWN(ip->i_mount)) 
if (!buffer_delay(bh)) 
if (error) { 
if (!XFS_FORCED_SHUTDOWN(ip->i_mount)) { 
while ((bh = bh->b_this_page) != head); 
if we are called from reclaim context. 
for direct reclaim or memcg reclaim.  We explicitly 
if (WARN_ON_ONCE((current->flags & (PF_MEMALLOC|PF_KSWAPD)) == 
while in a filesystem transaction. 
if (WARN_ON_ONCE(current->flags & PF_FSTRANS)) 
if (page->index < end_index) 
if it is fully outside i_size, e.g. due to a 
if the given offset is greater than 16TB on 32-bit system 
if (page->index >= end_index + 1)" as "end_index + 1" 
form this operation 
if the page to write is totally beyond the i_size or if it's 
if (page->index > end_index || 
if (wbc->sync_mode == WB_SYNC_NONE) 
if (offset >= end_offset) 
if (!buffer_uptodate(bh)) 
for holes (!mapped && uptodate), so skip 
if (!buffer_mapped(bh) && buffer_uptodate(bh)) { 
if (buffer_unwritten(bh)) { 
if (buffer_delay(bh)) { 
if (buffer_uptodate(bh)) { 
if (PageUptodate(page)) 
if (imap_valid) 
if (!imap_valid) { 
for unwritten extent conversion at I/O completion 
if (err) 
if (imap_valid) { 
if (type != XFS_IO_OVERWRITE) 
if (!iohead) 
while (offset += len, ((bh = bh->b_this_page) != head)); 
if there is no IO to be submitted for this page, we are done */ 
if (imap_valid) { 
if (end_index > last_index) 
if we might write beyond the on-disk inode size. 
if (ioend->io_type != XFS_IO_UNWRITTEN && xfs_ioend_is_append(ioend)) 
if (iohead) 
if (err == -EAGAIN) 
for_writepage(wbc, page); 
iflags_clear(XFS_I(mapping->host), XFS_ITRUNCATED); 
if the page is ok to release, 0 otherwise. 
if (WARN_ON_ONCE(delalloc)) 
if (WARN_ON_ONCE(unwritten)) 
if (XFS_FORCED_SHUTDOWN(mp)) 
if (!create && direct && offset >= i_size_read(inode)) 
if (create && !direct) { 
if (offset + size > mp->m_super->s_maxbytes) 
if (error) 
if (create && 
if (direct || xfs_get_extsz_hint(ip)) { 
for starting the block 
for the 
if (error) 
if the write 
if (nimaps && imap.br_startblock == HOLESTARTBLOCK) 
if (error) 
if (nimaps) { 
if (imap.br_startblock != HOLESTARTBLOCK && 
if we're reading into a hole). 
if (create || !ISUNWRITTEN(&imap)) 
if (create && ISUNWRITTEN(&imap)) { 
ifferent device. 
for_inode(inode); 
if it 
if (create && 
if (imap.br_startblock == DELAYSTARTBLOCK) { 
if (create) { 
for blocks beyond EOF must be marked new so that sub block 
for beyond EOF. 
if (direct || size > (1 << inode->i_blkbits)) { 
if (mapping_size > size) 
if (offset < i_size_read(inode) && 
if (mapping_size > LONG_MAX) 
for buffered I/O completion. 
if (offset + size > i_size_read(ioend->io_inode)) 
if (private && size > 0) 
for_inode(inode); 
if (rw & WRITE) { 
if we are 
if (offset + size > XFS_I(inode)->i_d.di_size) 
if (ret != -EIOCBQUEUED && iocb->private) 
if (end_fsb <= start_fsb) 
if (error) { 
if (!XFS_FORCED_SHUTDOWN(ip->i_mount)) { 
form.  However, for 64-bit pos request on 32-bit 
ifts rather than masks the mismatch 
for (bh = head; bh != head || !block_start; 
fore the write */ 
if the buffer is after the write, we're done */ 
if (!buffer_delay(bh)) 
if (!buffer_new(bh) && block_offset < i_size_read(inode)) 
for certain. 
if (!page) 
if (unlikely(status)) { 
if (pos + len > isize) { 
ific write because they will never be written. Previous writes 
if (unlikely(ret < len)) { 
if (to > isize) { 
if (pos > isize) 
file : ./test/kernel/fs/xfs/xfs_xattr.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
if (strcmp(name, "") == 0) 
if (!size) { 
if (error) 
if (strcmp(name, "") == 0) 
if (flags & XATTR_CREATE) 
if (flags & XATTR_REPLACE) 
if (!value) 
ifdef CONFIG_XFS_POSIX_ACL 
if 
if (flags & XFS_ATTR_SECURE) 
if (flags & XFS_ATTR_ROOT) 
if (flags & XFS_ATTR_SECURE) 
if (flags & XFS_ATTR_ROOT) 
if we are actually allowed to 
if ((flags & XFS_ATTR_ROOT) && !capable(CAP_SYS_ADMIN)) 
if (arraytop > context->firstu) { 
if (!size) 
if (*result > size) 
if (size) 
if (context.count < 0) 
if (posix_acl_access_exists(inode)) { 
if (error) 
if (posix_acl_default_exists(inode)) { 
if (error) 
file : ./test/kernel/fs/xfs/xfs_qm_bhv.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
if (limit && statp->f_blocks > limit) { 
if (limit && statp->f_files > limit) { 
ifier is inherited from parent directories. 
if they are filesystems in themselves. 
if (!xfs_qm_dqget(mp, NULL, xfs_get_projid(ip), XFS_DQ_PROJ, 0, &dqp)) { 
if (quotaondisk) { 
if (((uquotaondisk && !XFS_IS_UQUOTA_ON(mp)) || 
if (XFS_IS_QUOTA_ON(mp) || quotaondisk) { 
if we won't have to do 
if (quotaondisk && !XFS_QM_NEED_QUOTACHECK(mp)) { 
ife 
fore we're ready. This can happen when an 
file : ./test/kernel/fs/xfs/xfs_btree.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
form block pointer */ 
if any */ 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
if (unlikely(XFS_TEST_ERROR(!lblock_ok, mp, 
if (bp) 
form block pointer */ 
for ag. freespace struct */ 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
if (unlikely(XFS_TEST_ERROR(!sblock_ok, mp, 
if (bp) 
if any */ 
ifdef DEBUG 
if (cur->bc_flags & XFS_BTREE_LONG_PTRS) { 
if 
form btree header. 
ifcation was that made 
if (!xfs_sb_version_hascrc(&bp->b_target->bt_mount->m_sb)) 
if (bip) 
ify_crc( 
if (xfs_sb_version_hascrc(&bp->b_target->bt_mount->m_sb)) 
form btree header. 
ifcation was that made 
if (!xfs_sb_version_hascrc(&bp->b_target->bt_mount->m_sb)) 
if (bip) 
ify_crc( 
if (xfs_sb_version_hascrc(&bp->b_target->bt_mount->m_sb)) 
for buffers to be unlocked. 
if we get an error along 
for (i = 0; i < cur->bc_nlevels; i++) { 
if (!error) 
for filesystem */ 
for (i = 0; i < new->bc_nlevels; i++) { 
if (bp) { 
if (error) { 
for reasons better left unknown 
for this btree instance. 
if (cur->bc_flags & XFS_BTREE_LONG_PTRS) { 
if (cur->bc_flags & XFS_BTREE_CRC_BLOCKS) 
for this btree instance. 
if_broot field of an inode fork. 
ifork        *ifp; 
fork); 
if ((cur->bc_flags & XFS_BTREE_ROOT_IN_INODE) && 
for the block, return it with no data read. 
for fsbno */ 
for get_buf */ 
for the block, return it with no data read. 
for agno/agbno */ 
for get_buf */ 
for the cursor referring to the last block at the given level. 
if (cur->bc_flags & XFS_BTREE_LONG_PTRS) 
for this level. 
if (!block->bb_numrecs) 
for this level. 
if (!block->bb_numrecs) 
for the fields given. 
for current bit number */ 
for (i = 0, imask = 1LL; ; i++, imask <<= 1) { 
for (i = nbits - 1, imask = 1LL << i; ; i--, imask >>= 1) { 
for the block, return it read in. 
for read_buf */ 
for buffer */ 
if (error) 
if (bp) 
for it, don't return a buffer. 
for it, don't return a buffer. 
if ((lr & XFS_BTCUR_LEFTRA) && left != NULLDFSBNO) { 
if ((lr & XFS_BTCUR_RIGHTRA) && right != NULLDFSBNO) { 
if ((lr & XFS_BTCUR_LEFTRA) && left != NULLAGBLOCK) { 
if ((lr & XFS_BTCUR_RIGHTRA) && right != NULLAGBLOCK) { 
if we are at the root level and the 
if ((cur->bc_flags & XFS_BTREE_ROOT_IN_INODE) && 
if ((cur->bc_ra[lev] | lr) == cur->bc_ra[lev]) 
if (cur->bc_flags & XFS_BTREE_LONG_PTRS) 
if (cur->bc_flags & XFS_BTREE_LONG_PTRS) { 
form btrees here as we have a 
for level "lev" in the cursor to bp, releasing 
if (cur->bc_bufs[lev]) 
if (cur->bc_flags & XFS_BTREE_LONG_PTRS) { 
if (b->bb_u.l.bb_rightsib == cpu_to_be64(NULLDFSBNO)) 
if (b->bb_u.s.bb_leftsib == cpu_to_be32(NULLAGBLOCK)) 
if (b->bb_u.s.bb_rightsib == cpu_to_be32(NULLAGBLOCK)) 
if (cur->bc_flags & XFS_BTREE_LONG_PTRS) 
if (cur->bc_flags & XFS_BTREE_LONG_PTRS) 
if (cur->bc_flags & XFS_BTREE_LONG_PTRS) { 
if (lr == XFS_BB_RIGHTSIB) 
if (cur->bc_flags & XFS_BTREE_LONG_PTRS) { 
if (lr == XFS_BB_RIGHTSIB) 
if (flags & XFS_BTREE_LONG_PTRS) { 
if (flags & XFS_BTREE_CRC_BLOCKS) { 
if (flags & XFS_BTREE_CRC_BLOCKS) { 
ifferent 
for current users of the generic btree 
if (cur->bc_flags & XFS_BTREE_LONG_PTRS) 
if ptr is the last record in the btree and 
if (level > 0) 
if (!(cur->bc_flags & XFS_BTREE_LASTREC_UPDATE)) 
if (!xfs_btree_ptr_is_null(cur, &ptr)) 
if (cur->bc_flags & XFS_BTREE_LONG_PTRS) 
if (!*bpp) 
if (error) 
ift keys one index left/right inside a single btree block. 
ift_keys( 
ift records one index left/right inside a single btree block. 
ift_recs( 
ift block pointers one index left/right inside a single btree block. 
ift_ptrs( 
if (bp) { 
fork)); 
if (bp) { 
fork)); 
if (bp) { 
if (cur->bc_flags & XFS_BTREE_CRC_BLOCKS) { 
if (fields == XFS_BB_ALL_BITS) 
fork)); 
formation is untouched. 
ifdef DEBUG 
if (error) 
if 
if (++cur->bc_ptrs[level] <= xfs_btree_get_numrecs(block)) 
if we just went off the right edge of the tree. */ 
if (xfs_btree_ptr_is_null(cur, &ptr)) 
for (lev = level + 1; lev < cur->bc_nlevels; lev++) { 
ifdef DEBUG 
if (error) 
if 
for the next loop. */ 
if (lev == cur->bc_nlevels) { 
for (block = xfs_btree_get_block(cur, lev, &bp); lev > level; ) { 
if (error) 
formation is untouched. 
if we remain in the block after the decrement. */ 
ifdef DEBUG 
if (error) 
if 
if (xfs_btree_ptr_is_null(cur, &ptr)) 
for (lev = level + 1; lev < cur->bc_nlevels; lev++) { 
for the next loop. */ 
if (lev == cur->bc_nlevels) { 
for (block = xfs_btree_get_block(cur, lev, &bp); lev > level; ) { 
if (error) 
for btree block */ 
if in an inode */ 
for the disk address we are 
if (bp && XFS_BUF_ADDR(bp) == xfs_btree_ptr_to_daddr(cur, pp)) { 
if (error) 
if (level == 0) { 
if can't find any such record, 1 for success. 
iff;	/* difference for the current key */ 
iff = 1; level >= 0; level--) { 
if (error) 
if (diff == 0) { 
if (!high) { 
while (low <= high) { 
ifference to get next direction: 
iff = cur->bc_ops->key_diff(cur, kp); 
if (diff > 0) 
for the next level 
if (level > 0) { 
if (diff > 0 && --keyno < 1) 
ifdef DEBUG 
if (error) 
if 
if we need to adjust the results. */ 
if (dir == XFS_LOOKUP_GE && 
if (error) 
if (dir == XFS_LOOKUP_LE && diff > 0) 
if we succeeded or not. */ 
if (dir != XFS_LOOKUP_EQ || diff == 0) 
for (ptr = 1; ptr == 1 && level < cur->bc_nlevels; level++) { 
if 
ifdef DEBUG 
if (error) { 
if 
ifdef DEBUG 
if (error) 
if 
if (xfs_btree_is_lastrec(cur, block, 0)) { 
if (ptr == 1) { 
if (error) 
if possible. 
ift( 
if ((cur->bc_flags & XFS_BTREE_ROOT_IN_INODE) && 
for this block as "right". */ 
ifdef DEBUG 
if (error) 
if 
if (xfs_btree_ptr_is_null(cur, &lptr)) 
if (cur->bc_ptrs[level] <= 1) 
if (error) 
if (lrecs == cur->bc_ops->get_maxrecs(cur, level)) 
for the right side. 
ift); 
if (level > 0) { 
ifdef DEBUG 
if (error) 
if 
if (level > 0) { 
ifdef DEBUG 
for (i = 0; i < rrecs; i++) { 
if (error) 
if 
ift_ptrs(cur, 
ift_recs(cur, 
if (error) 
if possible. 
ift( 
if ((cur->bc_flags & XFS_BTREE_ROOT_IN_INODE) && 
for this block as "left". */ 
ifdef DEBUG 
if (error) 
if 
if (xfs_btree_ptr_is_null(cur, &rptr)) 
if (cur->bc_ptrs[level] >= lrecs) 
if (error) 
if (rrecs == cur->bc_ops->get_maxrecs(cur, level)) 
ift); 
if (level > 0) { 
ifdef DEBUG 
for (i = rrecs - 1; i >= 0; i--) { 
if (error) 
if 
ift_ptrs(cur, rpp, 1, rrecs); 
if (error) 
if 
ift_recs(cur, rrp, 1, rrecs); 
if (error) 
if (error) 
if (error) 
ifdef DEBUG 
if 
ifdef DEBUG 
if (error) 
if 
if (error) 
if (*stat == 0) 
if (error) 
for the new right block. */ 
if there's an odd number of entries now, that 
if ((lrecs & 1) && cur->bc_ptrs[level] <= rrecs + 1) 
if (level > 0) { 
ifdef DEBUG 
for (i = src_index; i < rrecs; i++) { 
if (error) 
if 
if (!xfs_btree_ptr_is_null(cur, &rrptr)) { 
if (error) 
if (cur->bc_ptrs[level] > lrecs + 1) { 
if (level + 1 < cur->bc_nlevels) { 
if (error) 
for inode */ 
for cblock */ 
ifdef DEBUG 
if 
if (error) 
if (*stat == 0) { 
if (error) 
for CRC enabled btree blocks. 
if (cur->bc_flags & XFS_BTREE_CRC_BLOCKS) { 
ifdef DEBUG 
for (i = 0; i < be16_to_cpu(cblock->bb_numrecs); i++) { 
if (error) 
if 
ifdef DEBUG 
if (error) 
if 
fork); 
fork); 
for key index, 1 or 2 */ 
if (error) 
if (*stat == 0) 
if (error) 
for each case. 
ifdef DEBUG 
if (error) 
if 
if (!xfs_btree_ptr_is_null(cur, &rptr)) { 
if (error) 
if (error) 
if (xfs_btree_get_level(left) > 0) { 
if ((cur->bc_flags & XFS_BTREE_ROOT_IN_INODE) && 
if (numrecs < cur->bc_ops->get_dmaxrecs(cur, level)) { 
fork); 
if (error || *stat == 0) 
ifting an entry to the right neighbor. */ 
if (error || *stat) 
ifting an entry to the left neighbor. */ 
if (error) 
if (*stat) { 
ifferent block now. 
if (error || *stat == 0) 
formation to the caller 
for block */ 
ifdef DEBUG 
if 
if (!(cur->bc_flags & XFS_BTREE_ROOT_IN_INODE) && 
if (ptr == 0) { 
ifdef DEBUG 
if (error) 
if (ptr <= numrecs) { 
if 
if (numrecs == cur->bc_ops->get_maxrecs(cur, level)) { 
if (error || *stat == 0) 
if the block was 
ifdef DEBUG 
if (error) 
if 
for our new entry in the block 
if (level > 0) { 
ifdef DEBUG 
for (i = numrecs - ptr; i >= 0; i--) { 
if (error) 
if 
ift_ptrs(cur, pp, 1, numrecs - ptr + 1); 
if (error) 
if 
ifdef DEBUG 
if 
ift_recs(cur, rp, 1, numrecs - ptr + 1); 
ifdef DEBUG 
if 
if (optr == 1) { 
if (error) 
if (xfs_btree_is_lastrec(cur, block, level)) { 
if any. 
if (!xfs_btree_ptr_is_null(cur, &nptr)) { 
for failure */ 
if we fail, nptr will be null. 
if (error) { 
if the cursor we just used is trash. 
if ncur is a new cursor or we're about to be done. 
if (pcur != cur && 
fore we trash it */ 
if (ncur) { 
while (!xfs_btree_ptr_is_null(cur, &nptr)); 
fork = cur->bc_private.b.whichfork; 
ifork	*ifp = XFS_IFORK_PTR(ip, whichfork); 
ifdef DEBUG 
if 
if (level == 1) 
if the root has multiple children. 
if (xfs_btree_get_numrecs(block) != 1) 
if the next level will fit. 
if (numrecs > cur->bc_ops->get_dmaxrecs(cur, level)) 
ifdef DEBUG 
if 
if (index) { 
fork); 
ifdef DEBUG 
for (i = 0; i < numrecs; i++) { 
if (error) { 
if 
fork)); 
if (error) { 
if (level > 0) { 
if (error) 
for error, 1 for done, 2 to go on to the next level. 
for block */ 
for keyp */ 
for nothing there. */ 
if (ptr == 0) { 
ifdef DEBUG 
if (error) 
if 
if (ptr > numrecs) { 
if (level > 0) { 
ifdef DEBUG 
for (i = 0; i < numrecs - ptr; i++) { 
if (error) 
if 
ift_keys(cur, lkp, -1, numrecs - ptr); 
if (ptr == 1) 
if (ptr < numrecs) { 
if (ptr == 1) { 
if (xfs_btree_is_lastrec(cur, block, level)) { 
if (level == cur->bc_nlevels - 1) { 
fork); 
if (error) 
if (error) 
if (numrecs == 1 && level > 0) { 
if (error) 
if (level > 0) { 
if (error) 
if (ptr == 1) { 
if (error) 
if (numrecs >= cur->bc_ops->get_minrecs(cur, level)) { 
if (error) 
if we can re-balance by moving only one record. 
if (cur->bc_flags & XFS_BTREE_ROOT_IN_INODE) { 
if (xfs_btree_ptr_is_null(cur, &rptr) && 
if (!error) 
if (error) 
if (error) 
if it's ok to shift an entry 
if (!xfs_btree_ptr_is_null(cur, &rptr)) { 
if (error) 
ifdef DEBUG 
if (error) 
if 
for future use. */ 
ifting an entry out 
if (xfs_btree_get_numrecs(right) - 1 >= 
ift(tcur, level, &i); 
if (i) { 
if (error) 
for 
if (!xfs_btree_ptr_is_null(cur, &lptr)) { 
if (error) 
if it's ok to shift an entry 
if (!xfs_btree_ptr_is_null(cur, &lptr)) { 
if (error) 
ifdef DEBUG 
if (error) 
if 
for future use. */ 
ifting an entry out 
if (xfs_btree_get_numrecs(left) - 1 >= 
ift(tcur, level, &i); 
if (i) { 
if (level == 0) 
for 
if (!xfs_btree_ptr_is_null(cur, &lptr) && 
if (error) 
if we can join with the right neighbor block. 
if (!xfs_btree_ptr_is_null(cur, &rptr) && 
if (error) 
if (error) 
if (level > 0) { 
ifdef DEBUG 
for (i = 1; i < rrecs; i++) { 
if (error) 
if 
if (!xfs_btree_ptr_is_null(cur, &cptr)) { 
if (error) 
if (error) 
if (bp != lbp) { 
if ((cur->bc_flags & XFS_BTREE_ROOT_IN_INODE) || 
if (error) 
if it's not a leaf, since it's 
if (level > 0) 
if (tcur) 
for (level = 0, i = 2; i == 2; level++) { 
if (error) 
if (i == 0) { 
for (level = 1; level < cur->bc_nlevels; level++) { 
if (error) 
ifdef DEBUG 
if 
ifdef DEBUG 
if (error) 
if 
if (ptr > xfs_btree_get_numrecs(block) || ptr <= 0) { 
ify, we don't really want to 
for the worst case of every buffer in a 
ifference to performance 
fore moving to the next block is 
ify the owner appropriately, set the 
if the buffer is relogged in 
ified buffer as delayed write buffer so the transaction 
ify the owner */ 
if (cur->bc_flags & XFS_BTREE_LONG_PTRS) 
formation is already held in the inode and discarded when the root 
if (bp) { 
for next iteration */ 
if (xfs_btree_ptr_is_null(cur, &rptr)) 
for each level */ 
if (error) 
for the next level down */ 
for the next iteration of the loop */ 
for each buffer in the level */ 
while (!error); 
file : ./test/kernel/fs/xfs/xfs_super.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
for log write and 
format */ 
force"/* user quota limit enforcement */ 
force"/* project quota limit enforcement */ 
for remount, but it will be used for mount 
ift_left_factor = 0, _res; 
if (value[last] == 'K' || value[last] == 'k') { 
if (value[last] == 'M' || value[last] == 'm') { 
if (value[last] == 'G' || value[last] == 'g') { 
if (kstrtoint(s, base, &_res)) 
ift_left_factor; 
if (!mp->m_fsname) 
if (sb->s_flags & MS_RDONLY) 
if (sb->s_flags & MS_DIRSYNC) 
if (sb->s_flags & MS_SYNCHRONOUS) 
if !XFS_BIG_INUMS 
if 
if (!options) 
while ((this_char = strsep(&options, ",")) != NULL) { 
if ((value = strchr(this_char, '=')) != NULL) 
if (!strcmp(this_char, MNTOPT_LOGBUFS)) { 
if (kstrtoint(value, 10, &mp->m_logbufs)) 
if (!strcmp(this_char, MNTOPT_LOGBSIZE)) { 
if (suffix_kstrtoint(value, 10, &mp->m_logbsize)) 
if (!strcmp(this_char, MNTOPT_LOGDEV)) { 
if (!mp->m_logname) 
if (!strcmp(this_char, MNTOPT_MTPT)) { 
if (!strcmp(this_char, MNTOPT_RTDEV)) { 
if (!mp->m_rtname) 
if (!strcmp(this_char, MNTOPT_BIOSIZE)) { 
if (kstrtoint(value, 10, &iosize)) 
if (!strcmp(this_char, MNTOPT_ALLOCSIZE)) { 
if (suffix_kstrtoint(value, 10, &iosize)) 
if (!strcmp(this_char, MNTOPT_GRPID) || 
if (!strcmp(this_char, MNTOPT_NOGRPID) || 
if (!strcmp(this_char, MNTOPT_WSYNC)) { 
if (!strcmp(this_char, MNTOPT_NORECOVERY)) { 
if (!strcmp(this_char, MNTOPT_NOALIGN)) { 
if (!strcmp(this_char, MNTOPT_SWALLOC)) { 
if (!strcmp(this_char, MNTOPT_SUNIT)) { 
if (kstrtoint(value, 10, &dsunit)) 
if (!strcmp(this_char, MNTOPT_SWIDTH)) { 
if (kstrtoint(value, 10, &dswidth)) 
if (!strcmp(this_char, MNTOPT_32BITINODE)) { 
if (!strcmp(this_char, MNTOPT_64BITINODE)) { 
if !XFS_BIG_INUMS 
if 
if (!strcmp(this_char, MNTOPT_BARRIER)) { 
if (!strcmp(this_char, MNTOPT_NOBARRIER)) { 
if (!strcmp(this_char, MNTOPT_IKEEP)) { 
if (!strcmp(this_char, MNTOPT_NOIKEEP)) { 
if (!strcmp(this_char, MNTOPT_LARGEIO)) { 
if (!strcmp(this_char, MNTOPT_NOLARGEIO)) { 
if (!strcmp(this_char, MNTOPT_ATTR2)) { 
if (!strcmp(this_char, MNTOPT_NOATTR2)) { 
if (!strcmp(this_char, MNTOPT_FILESTREAM)) { 
if (!strcmp(this_char, MNTOPT_NOQUOTA)) { 
if (!strcmp(this_char, MNTOPT_QUOTA) || 
if (!strcmp(this_char, MNTOPT_QUOTANOENF) || 
if (!strcmp(this_char, MNTOPT_PQUOTA) || 
if (!strcmp(this_char, MNTOPT_PQUOTANOENF)) { 
if (!strcmp(this_char, MNTOPT_GQUOTA) || 
if (!strcmp(this_char, MNTOPT_GQUOTANOENF)) { 
if (!strcmp(this_char, MNTOPT_DELAYLOG)) { 
if (!strcmp(this_char, MNTOPT_NODELAYLOG)) { 
if (!strcmp(this_char, MNTOPT_DISCARD)) { 
if (!strcmp(this_char, MNTOPT_NODISCARD)) { 
if (!strcmp(this_char, "ihashsize")) { 
if (!strcmp(this_char, "osyncisdsync")) { 
if (!strcmp(this_char, "osyncisosync")) { 
if (!strcmp(this_char, "irixsgid")) { 
if ((mp->m_flags & XFS_MOUNT_NORECOVERY) && 
if ((mp->m_flags & XFS_MOUNT_NOALIGN) && (dsunit || dswidth)) { 
ifndef CONFIG_XFS_QUOTA 
if 
ified together"); 
if (dsunit && (dswidth % dsunit != 0)) { 
if (dsunit && !(mp->m_flags & XFS_MOUNT_NOALIGN)) { 
fore we do not know the block size. 
if (mp->m_logbufs != -1 && 
if (mp->m_logbsize != -1 && 
if (iosizelog) { 
for (xfs_infop = xfs_info_set; xfs_infop->flag; xfs_infop++) { 
for (xfs_infop = xfs_info_unset; xfs_infop->flag; xfs_infop++) { 
if (mp->m_flags & XFS_MOUNT_DFLT_IOSIZE) 
if (mp->m_logbufs > 0) 
if (mp->m_logbsize > 0) 
if (mp->m_logname) 
if (mp->m_rtname) 
if (mp->m_dalign > 0) 
if (mp->m_swidth > 0) 
if (mp->m_qflags & (XFS_UQUOTA_ACCT|XFS_UQUOTA_ENFD)) 
if (mp->m_qflags & XFS_UQUOTA_ACCT) 
if (mp->m_qflags & XFS_PQUOTA_ACCT) { 
if (mp->m_qflags & XFS_GQUOTA_ACCT) { 
if (!(mp->m_qflags & XFS_ALL_QUOTA_ACCT)) 
ift) 
ift = BITS_PER_LONG - 1; 
forms). 
for page sized blocks (4K on 32 bit platforms), 
for smaller blocksizes it is less (bbits = log2 bsize). 
if BITS_PER_LONG == 32 
ift = BITS_PER_LONG; 
ift); 
if 
for inodes to meet 
if (mp->m_maxicount) { 
for (index = 0; index < sbp->sb_agcount; index++) { 
if (ino > XFS_MAXINUMBER_32) { 
if (index < max_metadata) 
for (index = 0; index < mp->m_sb.sb_agcount; index++) { 
for lock protection on m_flags, 
if (IS_ERR(*bdevp)) { 
if (bdev) 
if (mp->m_logdev_targp && mp->m_logdev_targp != mp->m_ddev_targp) { 
if (mp->m_rtdev_targp) { 
if 
if (mp->m_logname) { 
if (error) 
if (mp->m_rtname) { 
if (error) 
if (rtdev == ddev || rtdev == logdev) { 
if (!mp->m_ddev_targp) 
if (rtdev) { 
if (!mp->m_rtdev_targp) 
if (logdev && logdev != ddev) { 
if (!mp->m_logdev_targp) 
if (mp->m_rtdev_targp) 
if (rtdev) 
if (logdev && logdev != ddev) 
if (error) 
if (mp->m_logdev_targp && mp->m_logdev_targp != mp->m_ddev_targp) { 
if (xfs_sb_version_hassector(&mp->m_sb)) 
if (error) 
if (mp->m_rtdev_targp) { 
if (error) 
if (!mp->m_data_workqueue) 
if (!mp->m_unwritten_workqueue) 
if (!mp->m_cil_workqueue) 
if (!mp->m_reclaim_workqueue) 
if (!mp->m_log_workqueue) 
if (!mp->m_eofblocks_workqueue) 
while holding an XFS_ILOCK 
for IO to complete so that we effectively throttle multiple callers to the 
if (down_read_trylock(&sb->s_umount)) { 
iflags_test(ip, XFS_IRECLAIMABLE)); 
if the 
for the XFS inode. 
for XFS_IDONTCACHE here because we are already 
for every inode 
if (!wait) 
force(mp, XFS_LOG_SYNC); 
if (mp->m_maxicount) 
ifree); 
if ((ip->i_d.di_flags & XFS_DIFLAG_PROJINHERIT) && 
if (mp->m_resblks_save) { 
ifference between a sync and a quiesce. 
ifications to complete */ 
while (atomic_read(&mp->m_active_trans) > 0) 
force the log to unpin objects from the now complete transactions */ 
fore the freeze completes */ 
if (error) 
while ((p = strsep(&options, ",")) != NULL) { 
if (!*p) 
fortunately mount(8) adds all options from 
ified option if it actually 
if that's the case. 
for 
if 0 
for remount", p); 
if 
if ((mp->m_flags & XFS_MOUNT_RDONLY) && !(*flags & MS_RDONLY)) { 
if (mp->m_update_flags) { 
if (error) { 
if it is empty. Use the stashed 
if (!(mp->m_flags & XFS_MOUNT_RDONLY) && (*flags & MS_RDONLY)) { 
fore we sync the metadata, we need to free up the reserve 
if we get remounted rw, we can 
while frozen. 
if (xfs_sb_version_haslogv2(&mp->m_sb)) { 
if (mp->m_logbsize > 0 && 
if the logbuf is larger than 32K */ 
for version 1 logs must be 16K or 32K"); 
format for attributes. 
if (xfs_sb_version_hascrc(&mp->m_sb) && 
for V5 filesystems.", 
if (xfs_sb_version_hasattr2(&mp->m_sb) && 
if ((mp->m_sb.sb_flags & XFS_SBF_READONLY) && !ronly) { 
if ((mp->m_qflags & (XFS_GQUOTA_ACCT | XFS_GQUOTA_ACTIVE)) && 
if (!mp) 
if (error) 
ifdef CONFIG_XFS_QUOTA 
if 
if (silent) 
if (error) 
if (error) 
if (error) 
if (error) 
if (error) 
if (error) 
if (error) 
fore we run the 
if (XFS_SB_VERSION_NUM(&mp->m_sb) == XFS_SB_VERSION_5) 
if (error) 
if (!root) { 
if (!sb->s_root) { 
if (!xfs_ioend_zone) 
if (!xfs_ioend_pool) 
if (!xfs_log_ticket_zone) 
if (!xfs_bmap_free_item_zone) 
if (!xfs_btree_cur_zone) 
if (!xfs_da_state_zone) 
ifork_zone = kmem_zone_init(sizeof(xfs_ifork_t), "xfs_ifork"); 
fork_zone) 
if (!xfs_trans_zone) 
fork_zone; 
if (!xfs_log_item_desc_zone) 
if (!xfs_buf_item_zone) 
if (!xfs_efd_zone) 
if (!xfs_efi_zone) 
if (!xfs_inode_zone) 
if (!xfs_ili_zone) 
if (!xfs_icreate_zone) 
ifork_zone: 
fork_zone); 
fore we 
ifork_zone); 
for this workqueue. 
if (!xfs_alloc_wq) 
if (error) 
if (error) 
if (error) 
if (error) 
if (error) 
if (error) 
if (error) 
if (error) 
file : ./test/kernel/fs/xfs/xfs_trans.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
if (tp->t_flags & XFS_TRANS_FREEZE_PROT) 
for the 
if there are not enough blocks available. 
for available log space. 
if the count would go below zero. 
if (blocks > 0) { 
if (error != 0) { 
for this transaction. 
if (resp->tr_logres > 0) { 
if (resp->tr_logflags & XFS_TRANS_PERM_LOG_RES) { 
if (tp->t_ticket != NULL) { 
if (error) 
if the count would go below zero. 
if (rtextents > 0) { 
if (error) { 
formed. 
if (resp->tr_logres > 0) { 
if (resp->tr_logflags & XFS_TRANS_PERM_LOG_RES) { 
if (blocks > 0) { 
for application 
fore committing. 
if we modify these fields. 
if these are the 
if (xfs_sb_version_haslazysbcount(&mp->m_sb)) 
ifree_delta += delta; 
if (delta < 0) { 
if (xfs_sb_version_haslazysbcount(&mp->m_sb)) 
if (xfs_sb_version_haslazysbcount(&mp->m_sb)) 
if (delta < 0) { 
ify it as requested by earlier calls to xfs_trans_mod_sb(). 
if necessary. 
if we are logging them 
if (!xfs_sb_version_haslazysbcount(&(tp->t_mountp->m_sb))) { 
if (tp->t_ifree_delta) 
if (tp->t_fdblocks_delta) 
if (tp->t_res_fdblocks_delta) 
if (tp->t_frextents_delta) 
if (tp->t_res_frextents_delta) 
if (tp->t_dblocks_delta) { 
if (tp->t_agcount_delta) { 
if (tp->t_imaxpct_delta) { 
if (tp->t_rextsize_delta) { 
if (tp->t_rbmblocks_delta) { 
if (tp->t_rblocks_delta) { 
if (tp->t_rextents_delta) { 
if (tp->t_rextslog_delta) { 
if (whole) 
ifiable fields are contiguous, we 
ify each superblock field only 
if we have two separate modifcations of the 
ifreedelta = 0; 
if (tp->t_blk_res > 0) 
if ((tp->t_fdblocks_delta != 0) && 
if (tp->t_rtx_res > 0) 
if ((tp->t_frextents_delta != 0) && 
if (xfs_sb_version_haslazysbcount(&mp->m_sb) || 
ifreedelta = tp->t_ifree_delta; 
if (blkdelta) { 
if (error) 
if (idelta) { 
if (error) 
if (ifreedelta) { 
ifreedelta, rsvd); 
if (rtxdelta != 0) { 
if (tp->t_flags & XFS_TRANS_SB_DIRTY) { 
if (tp->t_agcount_delta != 0) { 
if (tp->t_imaxpct_delta != 0) { 
if (tp->t_rextsize_delta != 0) { 
if (tp->t_rbmblocks_delta != 0) { 
if (tp->t_rblocks_delta != 0) { 
if (tp->t_rextents_delta != 0) { 
if (tp->t_rextslog_delta != 0) { 
if (msbp > msb) { 
if (error) 
ifreecount: 
ify_counters(mp, XFS_SBS_IFREE, -ifreedelta, rsvd); 
if (idelta) 
if (blkdelta) 
for_each_entry_safe(lidp, next, &tp->t_items, lid_trans) { 
if (commit_lsn != NULLCOMMITLSN) 
if (flags & XFS_TRANS_ABORT) 
for (i = 0; i < nr_items; i++) { 
for (lv = log_vector; lv; lv = lv->lv_next ) { 
if (aborted) 
if (XFS_LSN_CMP(item_lsn, (xfs_lsn_t)-1) == 0) 
if we are aborting the operation, no point in inserting the 
if (aborted) { 
if (item_lsn != commit_lsn) { 
if (XFS_LSN_CMP(item_lsn, lip->li_lsn) > 0) 
for bulk AIL insert.  */ 
if (i >= LOG_ITEM_BATCH_SIZE) { 
if (i) 
if the commit had succeeded. 
if (flags & XFS_TRANS_RELEASE_LOG_RES) { 
if (!(tp->t_flags & XFS_TRANS_DIRTY)) 
if (XFS_FORCED_SHUTDOWN(mp)) { 
if (tp->t_flags & XFS_TRANS_SB_DIRTY) 
force the 
if (sync) { 
force_lsn(mp, commit_lsn, XFS_LOG_SYNC, NULL); 
for the transaction to be not dirty but 
if (tp->t_ticket) { 
if (commit_lsn == -1 && !error) 
ified any of its items, because 
if the caller is being too lazy to figure out if 
if ((flags & XFS_TRANS_ABORT) && !(tp->t_flags & XFS_TRANS_DIRTY)) 
if the caller is relying on us to shut down the 
if ((tp->t_flags & XFS_TRANS_DIRTY) && !XFS_FORCED_SHUTDOWN(mp)) { 
force_shutdown(mp, SHUTDOWN_CORRUPT_INCORE); 
if (!(flags & XFS_TRANS_ABORT) && !XFS_FORCED_SHUTDOWN(mp)) { 
for_each_entry(lidp, &tp->t_items, lid_trans) 
if 
if (tp->t_ticket) { 
if (error) 
for th next transaction. 
if they are taking up space at the tail of the log 
if (error) 
file : ./test/kernel/fs/xfs/xfs_dquot.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
fore group/project, 
ifdef DEBUG 
if 
force, push them into the dquot now. 
if (q->qi_bsoftlimit && !d->d_blk_softlimit) { 
if (q->qi_bhardlimit && !d->d_blk_hardlimit) { 
if (q->qi_isoftlimit && !d->d_ino_softlimit) 
if (q->qi_ihardlimit && !d->d_ino_hardlimit) 
if (q->qi_rtbsoftlimit && !d->d_rtb_softlimit) 
if (q->qi_rtbhardlimit && !d->d_rtb_hardlimit) 
if (prealloc) 
if necessary. 
forcement is OFF, which makes our 
forcement is off). 
forcement's off. 
ifdef DEBUG 
if (d->d_ino_hardlimit) 
if (d->d_rtb_hardlimit) 
if 
if ((d->d_blk_softlimit && 
if ((!d->d_blk_softlimit || 
if (!d->d_itimer) { 
if ((!d->d_ino_softlimit || 
if (!d->d_rtbtimer) { 
if ((!d->d_rtb_softlimit || 
for (i = 0; i < q->qi_dqperchunk; i++, d++, curid++) { 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
ified, we use 95% of the hard limit. 
if (!dqp->q_prealloc_lo_wmark) { 
if this type of quotas is turned off while we didn't 
if (!xfs_this_quota_on(dqp->q_mount, dqp->dq_flags)) { 
if (error) 
if (!bp) { 
if the freelist is not empty. 
ify this buffer, we need to 
if ((error = xfs_bmap_finish(tpp, &flist, &committed))) { 
if (committed) { 
ification so we get the corrupted 
if (error) { 
for (i = 0; i < mp->m_quotainfo->qi_dqperchunk; i++) { 
if (error) { 
if (!xfs_this_quota_on(dqp->q_mount, dqp->dq_flags)) { 
if this type of quotas is turned off while we 
if (error) 
if (map.br_startblock == HOLESTARTBLOCK) { 
if (!(flags & XFS_QMOPT_DQALLOC)) 
if (error) 
if (error == EFSCORRUPTED && (flags & XFS_QMOPT_DQREPAIR)) { 
if (error) { 
if it needed. 
ifferent lock class than user 
if (flags & XFS_QMOPT_DQALLOC) { 
if (error) 
if (error) { 
if quotas got turned off (ESRCH), 
fore making it accessible to 
for dquot accesses. 
if (tp) { 
if (error) 
if (tp) 
if requested) as needed. 
if the id changes while we don't hold the ilock inside this 
if ((! XFS_IS_UQUOTA_ON(mp) && type == XFS_DQ_USER) || 
ifdef DEBUG 
if ((xfs_dqerror_target == mp->m_ddev_targp) && 
if (ip) { 
if 
if (dqp) { 
if (dqp->dq_flags & XFS_DQ_FREEING) { 
fore 
if (ip) 
if (ip) 
if (error) 
if (ip) { 
if (xfs_this_quota_on(mp, type)) { 
if (dqp1) { 
if (unlikely(error)) { 
if (--dqp->q_nrefs == 0) { 
if (list_lru_add(&qi->qi_lru, &dqp->q_lru)) 
if dirty, then dqput() it. 
if (!dqp) 
if the dquot is dirty here. 
for removing the dquot logitem 
if its 
if the dquot's lsn has 
while 
if ((lip->li_flags & XFS_LI_IN_AIL) && 
if (lip->li_lsn == qip->qli_flush_lsn) 
ified dquot to disk. 
ified by the caller 
forcibly. If that's the case we must not write this dquot 
for an emptry AIL as part of the unmount process. 
if (XFS_FORCED_SHUTDOWN(mp)) { 
if (lip->li_flags & XFS_LI_IN_AIL) 
if (error) 
if (error) { 
force_shutdown(mp, SHUTDOWN_CORRUPT_INCORE); 
for later use. 
while we have the in memory 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
for too long. 
if (xfs_buf_ispinned(bp)) { 
force(dqp); 
if (d1 && d2) { 
if (be32_to_cpu(d1->q_core.d_id) > 
if (d1) { 
if (d2) { 
if (!xfs_qm_dqzone) 
if (!xfs_qm_dqtrxzone) 
file : ./test/kernel/fs/xfs/xfs_bit.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
for empty, 0 for non-empty. 
for (i = 0; i < size; i++) { 
if (start_bit) { 
if (tmp != ~0U) 
while (size) { 
if there are no more bits set or the start bit is 
if (start_bit >= size) 
if (start_bit) { 
if (tmp != 0U) 
while (size) { 
file : ./test/kernel/fs/xfs/xfs_rtalloc.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
formation for a given extent size, 
for this block */ 
if (rbpp && *rbpp && *rsb == sb) 
if (rbpp && *rbpp) 
if (error) { 
for the next call. 
if (rbpp) { 
formation & copy it out. 
if we're not asked to remember it. 
if (!rbpp) 
for the bitmap block bbno. 
for (log = low; log <= high; log++) { 
if (error) { 
if (sum) { 
form the summary file, given the old and new 
for (log = omp->m_rsumlevels - 1; log >= 0; log--) { 
if (error) 
if (sum == 0) 
ify_summary(omp, tp, log, bbno, -sum, 
if (error) 
ify_summary(nmp, tp, log, bbno, sum, 
if (error) 
ified by start and len allocated. 
formation as well as the bitmap. 
if (error) { 
forw(mp, tp, end, mp->m_sb.sb_rextents - 1, 
if (error) { 
formation corresponding to the entire 
ify_summary(mp, tp, 
if (error) { 
for them to be free. 
if (preblock < start) { 
if (error) { 
for them to be free. 
if (postblock > end) { 
if (error) { 
ify the bitmap to mark this extent allocated. 
ify_range(mp, tp, start, len, 0); 
if given.  Returns error; returns starting block in *rtblock. 
for one that's long enough. 
for (i = XFS_BLOCKTOBIT(mp, bbno), besti = -1, bestlen = 0, 
if there's a free extent of maxlen starting at i. 
if (error) { 
if (stat) { 
for maxlen is all free, allocate and return that. 
if (error) { 
if it's big enough for the minimum, and the best 
if (minlen < maxlen) { 
if (thislen >= minlen && thislen > bestlen) { 
if (next < end) { 
forw(mp, tp, next, end, &i); 
if (minlen < maxlen && besti != -1) { 
if (prod > 1 && (p = do_mod(bestlen, prod))) 
for bestlen & return that. 
if (error) { 
if given. 
if the range in question (for maxlen) is free. 
if (error) { 
if (isfree) { 
if (error) { 
if it's at least minlen. 
if (maxlen < minlen) { 
if prod is specified. 
if (prod > 1 && (i = maxlen % prod)) { 
if (maxlen < minlen) { 
if (error) { 
if given.  The lengths are all in rtextents. 
if (bno >= mp->m_sb.sb_rextents) 
if (error) { 
if (r != NULLRTBLOCK) { 
for (;;) { 
formation of extents of all useful levels 
if (error) { 
if (any) { 
if (i >= 0) { 
if (error) { 
if (r != NULLRTBLOCK) { 
for (j = -1; j > i; j--) { 
formation for 
if (error) { 
if (any) 
if (error) { 
if (r != NULLRTBLOCK) { 
for some reason 
if (error) { 
if (r != NULLRTBLOCK) { 
if (i > 0 && (int)bbno - i >= 0) 
if (i > 0 && (int)bbno + i < mp->m_sb.sb_rbmblocks - 1) 
if (i <= 0 && (int)bbno - i < mp->m_sb.sb_rbmblocks - 1) 
if (i <= 0 && (int)bbno + i > 0) 
ified.  If we don't get maxlen then use prod to trim 
formation for extents */ 
if there 
if 
for (l = xfs_highbit32(maxlen); l < mp->m_rsumlevels; l++) { 
for (i = 0; i < mp->m_sb.sb_rbmblocks; i++) { 
for this level/block. 
if (error) { 
if (!sum) 
if (error) { 
if (r != NULLRTBLOCK) { 
if (XFS_BITTOBLOCK(mp, n) > i + 1) 
for a fixed size extent. 
if (minlen > --maxlen) { 
for (l = xfs_highbit32(maxlen); l >= xfs_highbit32(minlen); l--) { 
for (i = 0; i < mp->m_sb.sb_rbmblocks; i++) { 
formation for this level/block. 
if (error) { 
if (!sum) 
ified 
for 
if (error) { 
if (r != NULLRTBLOCK) { 
if (XFS_BITTOBLOCK(mp, n) > i + 1) 
for growfs. 
for zeroing */ 
for bno */ 
while (oblocks < nblocks) { 
for one extent added to the file. 
if (error) 
if (!error && nmap < 1) 
if (error) 
if (error) 
if (error) 
for (bno = map.br_startoff, fsbno = map.br_startblock; 
for one block zeroing. 
if (error) 
for the block. 
if (bp == NULL) { 
if (error) 
if any. 
for filesystem */ 
if (!capable(CAP_SYS_ADMIN)) 
if (mp->m_rtdev_targp == NULL || mp->m_rbmip == NULL || 
if ((error = xfs_sb_validate_fsb_count(sbp, nrblocks))) 
if (!bp) 
if (bp->b_error) { 
if (nrsumblocks > (mp->m_sb.sb_logblocks >> 1)) 
for bitmap and summary inodes. 
if (error) 
if (error) 
if it is exactly full. 
fore. 
for (bmbno = sbp->sb_rbmblocks - 
for this round. 
if (error) 
if (sbp->sb_rbmblocks != nsbp->sb_rbmblocks || 
if (error) 
if (nsbp->sb_rextsize != sbp->sb_rextsize) 
if (nsbp->sb_rbmblocks != sbp->sb_rbmblocks) 
if (nsbp->sb_rblocks != sbp->sb_rblocks) 
if (nsbp->sb_rextents != sbp->sb_rextents) 
if (nsbp->sb_rextslog != sbp->sb_rextslog) 
if (error) { 
if (error) 
if (prod > 1) { 
if ((i = maxlen % prod)) 
if ((i = minlen % prod)) 
if (maxlen < minlen) { 
if (error) 
if (r != NULLRTBLOCK) { 
if (wasdel) 
for last block of subvolume */ 
if (sbp->sb_rblocks == 0) 
if (mp->m_rtdev_targp == NULL) { 
if (XFS_BB_TO_FSB(mp, d) != mp->m_sb.sb_rblocks) { 
if (!bp || bp->b_error) { 
if (bp) 
if (sbp->sb_rbmino == NULLFSINO) 
if (error) 
if (error) { 
if (mp->m_rbmip) 
if (mp->m_rsumip) 
for allocation at the start of a new realtime file. 
if (!(mp->m_rbmip->i_d.di_flags & XFS_DIFLAG_NEWRTBM)) { 
if ((log2 = xfs_highbit64(seq)) == -1) 
if (b >= mp->m_sb.sb_rextents) 
if (b + len > mp->m_sb.sb_rextents) 
file : ./test/kernel/fs/xfs/xfs_trace.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
for the trace 
file : ./test/kernel/fs/xfs/xfs_error.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
ifdef DEBUG 
if (!e) 
for (i = 0; i < XFS_ERROR_NTRAP; i++) { 
if (e != xfs_etrap[i]) 
if (prandom_u32() % randfactor) 
for (i = 0; i < XFS_NUM_INJECT_ERROR; i++)  { 
for (i = 0; i < XFS_NUM_INJECT_ERROR; i++)  { 
for (i = 0; i < XFS_NUM_INJECT_ERROR; i++)  { 
for (i = 0; i < XFS_NUM_INJECT_ERROR; i++) { 
if (loud || cleared) 
for filesystem"); 
if /* DEBUG */ 
if (level <= xfs_error_level) { 
if (level <= xfs_error_level) 
ifically for verifier errors.  Differentiate CRC vs. invalid 
ifier_error( 
if (xfs_error_level >= XFS_ERRLEVEL_LOW) { 
if (xfs_error_level >= XFS_ERRLEVEL_HIGH) 
file : ./test/kernel/fs/xfs/xfs_log.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
if defined(DEBUG) 
ify_dest_ptr( 
ify_grant_tail( 
ify_iclog( 
ify_tail_lsn( 
ify_dest_ptr(a,b) 
ify_iclog(a,b,c,d) 
if 
if (space < 0) { 
while (head_val != old); 
if (tmp > bytes) 
while (head_val != old); 
for_each_entry(tic, &head->waiters, t_queue) 
if (head == &log->l_write_head) { 
if (tic->t_flags & XLOG_TIC_PERM_RESERV) 
for_each_entry(tic, &head->waiters, t_queue) { 
if (*free_bytes < need_bytes) 
if (XLOG_FORCED_SHUTDOWN(log)) 
if (XLOG_FORCED_SHUTDOWN(log)) 
while (xlog_space_left(log, &head->grant) < need_bytes); 
for a log ticket. 
if we take it unconditionally on 
if we are going to add the ticket to the queue 
if we do not wake 
for more free space, 
if (!list_empty_careful(&head->waiters)) { 
if (!xlog_grant_head_wake(log, head, &free_bytes) || 
if (free_bytes < *need_bytes) { 
if (tic->t_res_num == XLOG_TIC_LEN_MAX) { 
if (XLOG_FORCED_SHUTDOWN(log)) 
ifferent TID in 
if (tic->t_cnt > 0) 
if (error) 
ify_grant_tail(log); 
for a log record header. 
if (XLOG_FORCED_SHUTDOWN(log)) 
if (!tic) 
if (error) 
ify_grant_tail(log); 
for 
ifferently.  Permanent reservation tickets by 
fore permanent reservations are actually released. 
if (XLOG_FORCED_SHUTDOWN(log) || 
if (ticket->t_flags & XLOG_TIC_PERM_RESERV) { 
if ((ticket->t_flags & XLOG_TIC_PERM_RESERV) == 0 || 
if not permanent reservation or a specific 
for 
ify( 
if (!abortflg) { 
if (xlog_state_release_iclog(mp->m_log, iclog)) { 
force_shutdown(mp, SHUTDOWN_LOG_IO_ERROR); 
if (!(mp->m_flags & XFS_MOUNT_NORECOVERY)) { 
if (IS_ERR(mp->m_log)) { 
if the log size is too small that would lead to some unexpected 
if the validation fails.  This 
for CRC format filesystems, as the 
if (mp->m_sb.sb_logblocks < min_logfsbs) { 
if (mp->m_sb.sb_logblocks > XFS_MAX_LOG_BLOCKS) { 
if (XFS_FSB_TO_B(mp, mp->m_sb.sb_logblocks) > XFS_MAX_LOG_BYTES) { 
if (error) { 
if log hangs are\n" 
if (error) { 
if (!(mp->m_flags & XFS_MOUNT_NORECOVERY)) { 
if (readonly) 
if (readonly) 
if (error) { 
for delayed logging to work. 
if (!(mp->m_flags & XFS_MOUNT_NORECOVERY)) { 
if (!error) 
while the 
ifdef DEBUG 
if 
if we are doing a forced umount (typically because of IO errors). 
if (mp->m_flags & XFS_MOUNT_RDONLY) 
force(mp, XFS_LOG_SYNC, NULL); 
ifdef DEBUG 
if (!(iclog->ic_state & XLOG_STATE_IOERROR)) { 
while (iclog != first_iclog); 
if (! (XLOG_FORCED_SHUTDOWN(log))) { 
if (!error) { 
for space used */ 
if (error) 
if (!(iclog->ic_state == XLOG_STATE_ACTIVE || 
if (!XLOG_FORCED_SHUTDOWN(log)) { 
force_wait, 
if (tic) { 
forced_shutdown mode, couldn't 
for other log I/Os that may already 
if we ever get stuck here that 
forced_shutdown as 
if ( ! (   iclog->ic_state == XLOG_STATE_ACTIVE 
force_wait, 
for unmount/freeze. 
force(mp, XFS_LOG_SYNC); 
while xfs_ail_push_all_sync() 
for 
fore we write the unmount record to 
for log space after we have moved the log tail. 
if (XLOG_FORCED_SHUTDOWN(log)) 
if (!list_empty_careful(&log->l_write_head.waiters)) { 
if (!list_empty_careful(&log->l_reserve_head.waiters)) { 
if we have a transaction that has gone to disk that needs to be 
fore 
if we are then in a state where covering is needed, the caller is 
formed that dummy transactions are required to move the log into the idle 
ified in the CIL.  Hence 
if (!xfs_fs_writable(mp)) 
if (!xlog_cil_empty(log)) 
if (xfs_ail_min_lsn(log->l_ailp)) 
if (!xlog_iclogs_empty(log)) 
if (log->l_covered_state == XLOG_STATE_COVER_NEED) 
for the log tail we keep 
if (lip) 
formal parms.  In the special case where 
for all places where this function is called 
if (tail_cycle == head_cycle && head_bytes >= tail_bytes) 
if (tail_cycle + 1 < head_cycle) 
if (tail_cycle < head_cycle) { 
if we see an error. 
if (XFS_TEST_ERROR(bp->b_error, l->l_mp, 
force_shutdown(l->l_mp, SHUTDOWN_LOG_IO_ERROR); 
if (iclog->ic_state & XLOG_STATE_IOERROR) { 
if (mp->m_logbufs <= 0) 
if (mp->m_logbsize > 0) { 
while (size != 1) { 
if (xfs_sb_version_haslogv2(&mp->m_sb)) { 
if (mp->m_logbsize % XLOG_HEADER_CYCLE_SIZE) 
if (mp->m_logbufs == 0) 
if (mp->m_logbsize == 0) 
iffies(xfs_syncd_centisecs * 10)); 
if (xfs_log_need_covered(mp)) 
force(mp, 0); 
for a given mount point. 
if (!log) { 
if (xfs_sb_version_hassector(&mp->m_sb)) { 
if (log2_size < BBSHIFT) { 
if (log2_size > mp->m_sectbb_log) { 
for larger sector sizes, must have v2 or external log */ 
for configuration.", 
if (!bp) 
for the iclog structure is 
ifferent sizes for machines 
for details. 
for (i=0; i < log->l_iclog_bufs; i++) { 
if (!*iclogp) 
if (!bp) 
ifdef DEBUG 
if 
force_wait); 
if (error) 
for (iclog = log->l_iclog; iclog; iclog = prev_iclog) { 
if (iclog->ic_bp) 
if (error) 
force_shutdown(mp, SHUTDOWN_LOG_IO_ERROR); 
if we ever use more than 75% of the on-disk 
for the minimum number of free blocks in the 
if (free_blocks >= free_threshold) 
if (threshold_block >= log->l_logBBsize) { 
if (XFS_LSN_CMP(threshold_lsn, last_sync_lsn) > 0) 
if 
if (!XLOG_FORCED_SHUTDOWN(log)) 
for (i = 0; i < BTOBB(size); i++) { 
if (xfs_sb_version_haslogv2(&log->l_mp->m_sb)) { 
for ( ; i < BTOBB(size); i++) { 
for (i = 1; i < log->l_iclog_heads; i++) 
for a log buffer. 
for the record header ... */ 
for additional cycle data for v2 logs ... */ 
for (i = 1; i < log->l_iclog_heads; i++) { 
for the payload */ 
for log bufs. This gives us a central 
fore tearing down the iclogbufs. Hence we need to 
if (iclog->ic_state & XLOG_STATE_IOERROR) { 
while this code syncs out an iclog ready to go. 
fore be done without grabbing the state machine lock.  Updating the global 
fore calling bwrite(). 
fore roundup */ 
for LR header */ 
if (v2 && log->l_mp->m_sb.sb_logsunit > 1) { 
if (v2) 
if (XFS_BUF_ADDR(bp) + BTOBB(count) > log->l_logBBsize) { 
for the header magic number case, though. 
for (i = 0; i < split; i += BBSIZE) { 
if (++cycle == XLOG_HEADER_MAGIC_NUM) 
if (log->l_mp->m_flags & XFS_MOUNT_BARRIER) { 
fore flushing the log to make 
fore stamping the new log tail LSN into the 
fortunately synchronously here; 
for preflushes. 
if (log->l_mp->m_logdev_targp != log->l_mp->m_ddev_targp) 
ify_iclog(log, iclog, count, true); 
for log which doesn't start at block #0 */ 
if (error) { 
if (split) { 
if (log->l_mp->m_flags & XFS_MOUNT_BARRIER) 
for internal log which doesn't start at block #0 */ 
if (error) { 
fore we tear down these buffers. 
for (i = 0; i < log->l_iclog_bufs; i++) { 
fore we free it. Also, cycle the lock 
for (i = 0; i < log->l_iclog_bufs; i++) { 
format", 
format", 
iformat", 
format", 
for (i = 0; i < ticket->t_res_num; i++) { 
force_shutdown(mp, SHUTDOWN_LOG_IO_ERROR); 
for start rec of xact */ 
for (lv = log_vector; lv; lv = lv->lv_next) { 
if (lv->lv_buf_len == XFS_LOG_VEC_ORDERED) 
for (i = 0; i < lv->lv_niovecs; i++) { 
for transaction, insert start record  We can't be trying to 
if (!(ticket->t_flags & XLOG_TIC_INITED)) 
if (still_to_copy <= space_available) { 
if (*last_was_partial_copy) 
if (*last_was_partial_copy) 
for new log op header */ 
if (*partial_copy) { 
if (iclog->ic_size - log_offset <= sizeof(xlog_op_header_t)) { 
if (!commit_iclog) 
for a given transaction. 
if we can fit entire region into this iclog 
for potential flush to on-disk log. 
if reservation is overrun.  This should never happen since 
for the in-core log 
for. 
if (ticket->t_flags & XLOG_TIC_INITED) 
for. These 
if (flags & (XLOG_COMMIT_TRANS | XLOG_UNMOUNT_TRANS)) 
if (ticket->t_curr_res < 0) 
while (lv && (!lv->lv_niovecs || index < lv->lv_niovecs)) { 
if (error) 
if (!*start_lsn) 
while (lv && (!lv->lv_niovecs || index < lv->lv_niovecs)) { 
if (lv->lv_buf_len == XFS_LOG_VEC_ORDERED) { 
if (start_rec_copy) { 
if (!ophdr) 
ify_dest_ptr(log, ptr); 
if (error) 
if we had a partial copy, we need to get more iclog 
if (partial_copy) 
if (++index == lv->lv_niovecs) { 
if (lv) 
if (record_cnt == 0 && ordered == false) { 
if (!commit_iclog) 
if (iclog->ic_state == XLOG_STATE_DIRTY) { 
if (!changed && 
if (iclog->ic_state == XLOG_STATE_ACTIVE) 
while (iclog != log->l_iclog); 
for the dummy log recording. 
if (changed) { 
if (changed == 1) 
if (changed == 1) 
if (!(lsn_log->ic_state & (XLOG_STATE_ACTIVE|XLOG_STATE_DIRTY))) { 
if ((lsn && !lowest_lsn) || 
while (lsn_log != log->l_iclog); 
if 
if (iclog->ic_state & 
if there 
if (!(iclog->ic_state & XLOG_STATE_IOERROR)) { 
form callbacks in order.  Since 
if (!(iclog->ic_state & 
if (ciclog && (ciclog->ic_state == 
if and are going to 
if. 
if we 
if (lowest_lsn && 
for 
if this iclog contains 
fore we drop the 
if (iclog->ic_callback) 
while (cb) { 
form callbacks in the order given */ 
if (!(iclog->ic_state & XLOG_STATE_IOERROR)) 
if applicable. 
force() */ 
while (first_iclog != iclog); 
while (!ioerrors && loopdidcallbacks); 
if iclogs are being left in 
ifdef DEBUG 
if iclogs are found in states 
for 
if (iclog->ic_state == XLOG_STATE_WANT_SYNC || 
while (first_iclog != iclog); 
if (log->l_iclog->ic_state & (XLOG_STATE_ACTIVE|XLOG_STATE_IOERROR)) 
if (wake) 
if (iclog->ic_state != XLOG_STATE_IOERROR) { 
for the result. 
if all iclogs are syncing, 
if (XLOG_FORCED_SHUTDOWN(log)) { 
if (iclog->ic_state != XLOG_STATE_ACTIVE) { 
for log writes to have flushed */ 
if iclogs marked XLOG_STATE_WANT_SYNC always write out what they are 
if (log_offset == 0) { 
fore, wait 
if (iclog->ic_size - iclog->ic_offset < 2*sizeof(xlog_op_header_t)) { 
if (!atomic_add_unless(&iclog->ic_refcnt, -1, 1)) { 
if (error) 
if (len <= iclog->ic_size - iclog->ic_offset) { 
forward. 
if (ticket->t_cnt > 0) 
if we still have some of the pre-reserved space */ 
formation we need to make a correct determination of space left 
fore we need to ask for more space.  The first 
if (ticket->t_cnt > 0) 
if (ticket->t_cnt > 0) { 
if this is the last reference to the given iclog and 
if (iclog->ic_state & XLOG_STATE_IOERROR) 
if (!atomic_dec_and_lock(&iclog->ic_refcnt, &log->l_icloglock)) 
if (iclog->ic_state & XLOG_STATE_IOERROR) { 
if (iclog->ic_state == XLOG_STATE_WANT_SYNC) { 
fore writing to iclog */ 
ify_tail_lsn(log, iclog, tail_lsn); 
fore the bwrite. However, we know that 
if (sync) 
if (!eventual_size) 
if (xfs_sb_version_haslogv2(&log->l_mp->m_sb) && 
if (log->l_curr_block >= log->l_logBBsize) { 
if (log->l_curr_cycle == XLOG_HEADER_MAGIC_NUM) 
form an intelligent scan of the in-core logs. 
if: 
if: 
force( 
force); 
if (iclog->ic_state & XLOG_STATE_IOERROR) { 
if (iclog->ic_state == XLOG_STATE_ACTIVE || 
if (iclog->ic_state == XLOG_STATE_DIRTY || 
if (iclog->ic_state == XLOG_STATE_ACTIVE || 
if (atomic_read(&iclog->ic_refcnt) == 0) { 
if (xlog_state_release_iclog(log, iclog)) 
if (log_flushed) 
if (be64_to_cpu(iclog->ic_header.h_lsn) == lsn && 
force out this LR, 
if (flags & XFS_LOG_SYNC) { 
if we're shutting down here, before 
while we're holding the l_icloglock. 
if (iclog->ic_state & XLOG_STATE_IOERROR) { 
force_sleep); 
if (iclog->ic_state & XLOG_STATE_IOERROR) 
if (log_flushed) 
for _xfs_log_force(), to be used when caller doesn't care 
forward. 
force( 
force(mp, 0); 
if (error) 
ific LSN. 
forces are implemented with a signal variable. All callers 
ific in-core log.  When given in-core log finally completes its 
force_lsn( 
force); 
if (lsn == NULLCOMMITLSN) 
if (iclog->ic_state & XLOG_STATE_IOERROR) { 
if (be64_to_cpu(iclog->ic_header.h_lsn) != lsn) { 
if (iclog->ic_state == XLOG_STATE_DIRTY) { 
if (iclog->ic_state == XLOG_STATE_ACTIVE) { 
if we haven't already slept (e.g. 
fore us is going to 
for 
fore we close 
if (!already_slept && 
force_sleep); 
if (log_flushed) 
if (xlog_state_release_iclog(log, iclog)) 
if (log_flushed) 
if ((flags & XFS_LOG_SYNC) && /* sleep */ 
if we know that we've 
if (iclog->ic_state & XLOG_STATE_IOERROR) { 
force_sleep); 
if (iclog->ic_state & XLOG_STATE_IOERROR) 
if (log_flushed) 
while (iclog != log->l_iclog); 
for _xfs_log_force_lsn(), to be used when caller doesn't care 
forward. 
force_lsn( 
force(mp, lsn); 
if (error) 
if (iclog->ic_state == XLOG_STATE_ACTIVE) { 
if (atomic_dec_and_test(&ticket->t_ref)) 
for a log ticket. 
for one 
for non-transaction data 
form of a transaction is: 
for all the leadup data and trailer data 
for the worst case in terms of using 
if: 
fore the commit record is synced 
fore the commit record is in its own Log Record. 
for 
for trans header */ 
for start-rec */ 
for LR headers - the space for data in an iclog is the size minus 
for split-recs might 
if this 
if the transaction is larger 
for split-recs - ophdrs added when data split over LRs */ 
if we overrun */ 
while (!num_headers || 
for commit-rec LR header - note: padding will subsume the ophdr */ 
for roundoff padding for transaction data and one for commit record */ 
if (!tic) 
if (permanent) 
if defined(DEBUG) 
ifferent 
ify_dest_ptr( 
for (i = 0; i < log->l_iclog_bufs; i++) { 
if (!good_ptr) 
iffer by exactly one and check the byte count. 
if they want to panic the machine when such an error occurs. For 
ify_grant_tail( 
if (tail_cycle != cycle) { 
if (space > BBTOB(tail_blocks) && 
if it will fit */ 
ify_tail_lsn( 
if (CYCLE_LSN(tail_lsn) == log->l_prev_cycle) { 
if (blocks < BTOBB(iclog->ic_offset)+BTOBB(log->l_iclog_hsize)) 
if (BLOCK_LSN(tail_lsn) == log->l_prev_block) 
if (blocks < BTOBB(iclog->ic_offset) + 1) 
ify_tail_lsn */ 
form a number of checks on the iclog before writing to disk. 
for: 
ify_iclog( 
for (i = 0; i < log->l_iclog_bufs; i++, icptr = icptr->ic_next) 
if (icptr != log->l_iclog) 
if (iclog->ic_header.h_magicno != cpu_to_be32(XLOG_HEADER_MAGIC_NUM)) 
for (ptr += BBSIZE; ptr < ((xfs_caddr_t)&iclog->ic_header) + count; 
if (*(__be32 *)ptr == cpu_to_be32(XLOG_HEADER_MAGIC_NUM)) 
for (i = 0; i < len; i++) { 
if (!syncing || (field_offset & 0x1ff)) { 
if (idx >= (XLOG_HEADER_CYCLE_SIZE / BBSIZE)) { 
if (clientid != XFS_TRANSACTION && clientid != XFS_LOG) 
if (!syncing || (field_offset & 0x1ff)) { 
if (idx >= (XLOG_HEADER_CYCLE_SIZE / BBSIZE)) { 
ify_iclog */ 
if (! (iclog->ic_state & XLOG_STATE_IOERROR)) { 
while (ic != iclog); 
if state transition has already happened. 
force_shutdown, when we're forcibly 
for all interested 
if !logerror, flush the iclogs to disk, then seal them off 
for business. 
fore flushing them to disk. This needs 
force_umount( 
for business yet. 
if (!log || 
if (mp->m_sb_bp) 
for us. 
if (logerror && log->l_iclog->ic_state & XLOG_STATE_IOERROR) { 
fore marking the log as 
force() 
if (!logerror) 
force(log); 
if (mp->m_sb_bp) 
while we're still holding the loglock. 
if (logerror) 
for log reservations after this. That 
if (!(log->l_iclog->ic_state & XLOG_STATE_IOERROR)) { 
fore shutting the 
force(mp, XFS_LOG_SYNC, NULL); 
force. Wake the CIL push first 
ifdef XFSERRORDEBUG 
while (iclog != log->l_iclog); 
if 
if (iclog->ic_header.h_num_logops) 
while (iclog != log->l_iclog); 
file : ./test/kernel/fs/xfs/xfs_acl.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
fore 
if (count > max_entries) 
if (!acl) 
for (i = 0; i < count; i++) { 
format first this is not a problem. 
for (i = 0; i < acl->a_count; i++) { 
if (!xfs_acl) 
if (error) { 
for any other error assume it is transient and 
if (error == -ENOATTR) 
if (IS_ERR(acl)) 
if (!S_ISDIR(inode->i_mode)) 
if (acl) { 
if (!xfs_acl) 
if (error == -ENOATTR) 
if (!error) 
if (mode != inode->i_mode) { 
if (!S_ISDIR(inode->i_mode)) 
if (!acl) 
if (acl->a_count > XFS_ACL_MAX_ENTRIES(XFS_M(inode->i_sb))) 
if (type == ACL_TYPE_ACCESS) { 
if (error <= 0) { 
if (error < 0) 
if (error) 
file : ./test/kernel/fs/xfs/xfs_inode.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
iflush_int(xfs_inode_t *, xfs_buf_t *); 
if ((ip->i_d.di_flags & XFS_DIFLAG_EXTSIZE) && ip->i_d.di_extsize) 
if (XFS_IS_REALTIME_INODE(ip)) 
for reading the extents.  The reason these places can't just 
for a file in b-tree format.  If the 
if the extents have been read in yet, and only lock the inode exclusively 
if (ip->i_d.di_format == XFS_DINODE_FMT_BTREE && 
if (ip->i_d.di_aformat == XFS_DINODE_FMT_BTREE && 
for the same lock, 
if (lock_flags & XFS_IOLOCK_EXCL) 
if (lock_flags & XFS_IOLOCK_SHARED) 
if (lock_flags & XFS_ILOCK_EXCL) 
if (lock_flags & XFS_ILOCK_SHARED) 
if it gets 
fore returning. 
for xfs_ilock() for a list 
for the same lock, 
if (lock_flags & XFS_IOLOCK_EXCL) { 
if (lock_flags & XFS_IOLOCK_SHARED) { 
if (lock_flags & XFS_ILOCK_EXCL) { 
if (lock_flags & XFS_ILOCK_SHARED) { 
if (lock_flags & XFS_IOLOCK_EXCL) 
if (lock_flags & XFS_IOLOCK_SHARED) 
for xfs_ilock() for a list 
for the same lock, 
if (lock_flags & XFS_IOLOCK_EXCL) 
if (lock_flags & XFS_IOLOCK_SHARED) 
if (lock_flags & XFS_ILOCK_EXCL) 
if (lock_flags & XFS_ILOCK_SHARED) 
if it is being demoted. 
if (lock_flags & XFS_ILOCK_EXCL) 
if (lock_flags & XFS_IOLOCK_EXCL) 
if defined(DEBUG) || defined(XFS_WARN) 
if (lock_flags & (XFS_ILOCK_EXCL|XFS_ILOCK_SHARED)) { 
if (lock_flags & (XFS_IOLOCK_EXCL|XFS_IOLOCK_SHARED)) { 
if 
if 
ifferent value 
if (lock_mode & (XFS_IOLOCK_SHARED|XFS_IOLOCK_EXCL)) 
if (lock_mode & (XFS_ILOCK_SHARED|XFS_ILOCK_EXCL)) 
for another inode that is locked 
for the inode we just locked in order to push the tail and free space 
for (; i < inodes; i++) { 
if (i && (ips[i] == ips[i-1]))	/* Already locked */ 
if (!try_lock) { 
for (j = (i - 1); j >= 0 && !try_lock; j--) { 
if (lp && (lp->li_flags & XFS_LI_IN_AIL)) { 
if (try_lock) { 
if (!xfs_ilock_nowait(ips[i], xfs_lock_inumorder(lock_mode, i))) { 
if the inode is in the AIL. 
for(j = i - 1; j >= 0; j--) { 
if we've already 
if ((j != (i - 1)) && ips[j] == 
if ((attempts % 5) == 0) { 
ifdef DEBUG 
if 
ifdef DEBUG 
if (attempts < 5) xfs_small_retries++; 
if 
if (lock_mode & (XFS_IOLOCK_SHARED|XFS_IOLOCK_EXCL)) 
if (ip0->i_ino > ip1->i_ino) { 
if (lp && (lp->li_flags & XFS_LI_IN_AIL)) { 
if ((++attempts % 5) == 0) 
iflock( 
if (xfs_isiflocked(ip)) 
iflock_nowait(ip)); 
if (di_flags & XFS_DIFLAG_ANY) { 
if (di_flags & XFS_DIFLAG_PREALLOC) 
if (di_flags & XFS_DIFLAG_IMMUTABLE) 
if (di_flags & XFS_DIFLAG_APPEND) 
if (di_flags & XFS_DIFLAG_SYNC) 
if (di_flags & XFS_DIFLAG_NOATIME) 
if (di_flags & XFS_DIFLAG_NODUMP) 
if (di_flags & XFS_DIFLAG_RTINHERIT) 
if (di_flags & XFS_DIFLAG_PROJINHERIT) 
if (di_flags & XFS_DIFLAG_NOSYMLINKS) 
if (di_flags & XFS_DIFLAG_EXTSIZE) 
if (di_flags & XFS_DIFLAG_EXTSZINHERIT) 
if (di_flags & XFS_DIFLAG_NODEFRAG) 
if (di_flags & XFS_DIFLAG_FILESTREAM) 
if an exact match is found. 
if (XFS_FORCED_SHUTDOWN(dp->i_mount)) 
if (error) 
if (error) 
if (ci_name) 
for the inode are 
fore returning the inode itself. 
if (error) 
if (*ialloc_context || ino == NULLFSINO) { 
if (error) 
for ever leaving 
if (ip->i_d.di_version == 1) 
if (pip && XFS_INHERIT_GID(pip)) { 
if ((pip->i_d.di_mode & S_ISGID) && S_ISDIR(mode)) { 
if the irix_sgid_inherit compatibility variable is set). 
if ((irix_sgid_inherit) && 
if (ip->i_d.di_version == 3) { 
format = XFS_DINODE_FMT_DEV; 
if_flags = 0; 
if (pip && (pip->i_d.di_flags & XFS_DIFLAG_ANY)) { 
if (S_ISDIR(mode)) { 
if (pip->i_d.di_flags & XFS_DIFLAG_EXTSZINHERIT) { 
if (S_ISREG(mode)) { 
if (pip->i_d.di_flags & XFS_DIFLAG_EXTSZINHERIT) { 
if ((pip->i_d.di_flags & XFS_DIFLAG_NOATIME) && 
if ((pip->i_d.di_flags & XFS_DIFLAG_NODUMP) && 
if ((pip->i_d.di_flags & XFS_DIFLAG_SYNC) && 
if ((pip->i_d.di_flags & XFS_DIFLAG_NOSYMLINKS) && 
if (pip->i_d.di_flags & XFS_DIFLAG_PROJINHERIT) 
if ((pip->i_d.di_flags & XFS_DIFLAG_NODEFRAG) && 
if (pip->i_d.di_flags & XFS_DIFLAG_FILESTREAM) 
format = XFS_DINODE_FMT_EXTENTS; 
if_bytes = ip->i_df.if_real_bytes = 0; 
fork settings for new inode. 
format = XFS_DINODE_FMT_EXTENTS; 
if the Space Manager needed 
if 
if we were unable to allocate a new inode. 
if (code) { 
if (!ialloc_context && !ip) { 
if (ialloc_context) { 
if (tp->t_dqinfo) { 
if (committed != NULL) { 
if (code) { 
if (dqinfo) { 
if (dqinfo) { 
if (code) { 
if (code) { 
if (committed != NULL) 
if (ip->i_d.di_nlink == 0) { 
if (XFS_FORCED_SHUTDOWN(mp)) 
if (error) 
if (is_dir) { 
for that case.  If that is not 
if (error == ENOSPC) { 
if (error == ENOSPC) { 
if (error) { 
if (error) 
if (error) 
if (error) { 
if (error) { 
if (is_dir) { 
if (error) 
if (error) 
fore returning to 
if (mp->m_flags & (XFS_MOUNT_WSYNC|XFS_MOUNT_DIRSYNC)) 
ify them incore. 
if (error) 
if (error) 
if (ip) 
if (unlock_dp_on_error) 
if (XFS_FORCED_SHUTDOWN(mp)) 
if (error) 
if (error == ENOSPC) { 
if (error) { 
if (error) 
if (error) { 
if (mp->m_flags & XFS_MOUNT_WSYNC) 
ify them incore. 
if (error) 
if (error) 
if (ip) 
if (XFS_FORCED_SHUTDOWN(mp)) 
if (error) 
if (error) 
if (error == ENOSPC) { 
if (error) { 
if (unlikely((tdp->i_d.di_flags & XFS_DIFLAG_PROJINHERIT) && 
if (error) 
if (sip->i_d.di_nlink == 0) { 
if (error) 
if (error) 
if (error) 
fore returning to 
if (mp->m_flags & (XFS_MOUNT_WSYNC|XFS_MOUNT_DIRSYNC)) { 
if (error) { 
for the attribute and 
fore calling here.  Some transaction will be 
for it within the transaction. 
for the higher level code, 
if possible. 
fork, 
for space to become allocated beyond 
if (first_unmap_block == last_block) 
while (!done) { 
fork), 
if (error) 
if (committed) 
if (error) 
if (committed) { 
forward in the log as part of every commit. 
if (error) 
if (error) 
forward in the log. 
if (!S_ISREG(ip->i_d.di_mode) || (ip->i_d.di_mode == 0)) 
if (mp->m_flags & XFS_MOUNT_RDONLY) 
if (!XFS_FORCED_SHUTDOWN(mp)) { 
ificantly reducing the time window where we'd otherwise 
iflags_test_and_clear(ip, XFS_ITRUNCATED); 
iflags_clear(ip, XFS_IDIRTY_RELEASE); 
if (error) 
if (ip->i_d.di_nlink == 0) 
if (xfs_can_free_eofblocks(ip, false)) { 
if the inode is being opened, written and 
fore checking for a dirty 
if (xfs_iflags_test(ip, XFS_IDIRTY_RELEASE)) 
if (error && error != EAGAIN) 
if (ip->i_delayed_blks) 
form a truncate when an inode becomes unlinked. 
if (error) { 
fore the truncate completes. See the related 
if (error) 
if (error) 
ifree() 
form the inode free when an inode is unlinked. 
ifree( 
ifree transaction might need to allocate blocks for record 
ifree to dip into the reserved block pool if necessary. 
while 
if the reservation does happen to fail, as the inode 
ifree, 
if (error) { 
ifree(tp, ip, &free_list); 
for a long time or forever. 
if (!XFS_FORCED_SHUTDOWN(mp)) { 
force_shutdown(mp, SHUTDOWN_META_IO_ERROR); 
if (error) 
if (error) 
for the vnode 
for the inode here since the file is now closed. 
if (ip->i_d.di_mode == 0) { 
if_broot_bytes == 0); 
if (mp->m_flags & XFS_MOUNT_RDONLY) 
if (ip->i_d.di_nlink != 0) { 
force is true because we are evicting an inode from the 
if (xfs_can_free_eofblocks(ip, true)) 
if (S_ISREG(ip->i_d.di_mode) && 
if (error) 
if (S_ISLNK(ip->i_d.di_mode)) 
if (truncate) 
if (error) 
fork.  We need to just commit the current transaction 
if (ip->i_d.di_anextents > 0) { 
forkoff != 0); 
if (error) 
if (ip->i_afp) 
fork(ip, XFS_ATTR_FORK); 
ifree(ip); 
if any. 
if (error) 
for the 
if (agi->agi_unlinked[bucket_index] != cpu_to_be32(NULLAGINO)) { 
if (error) 
if appropriate */ 
if (error) 
for the 
if (be32_to_cpu(agi->agi_unlinked[bucket_index]) == agino) { 
if there is anyone after us on the list. 
if (error) { 
if (next_agino != NULLAGINO) { 
if appropriate */ 
for the inode being freed. 
while (next_agino != agino) { 
if (last_ibp) 
if (error) { 
if (error) { 
if (error) { 
if (next_agino != NULLAGINO) { 
if appropriate */ 
if appropriate */ 
ifree_cluster( 
for (j = 0; j < nbufs; j++, inum += inodes_per_cluster) { 
fore we get a lock on it, and hence we may fail 
if (!bp) 
for IO. If it is, we want to know about it, and we 
ifier to the buffer. 
while (lip) { 
iflags_set(iip->ili_inode, XFS_ISTALE); 
for being staled on buffer IO 
for (i = 0; i < inodes_per_cluster; i++) { 
if (!ip) { 
if it 
if (ip->i_ino != inum + i || 
if (ip != free_ip && 
iflock(ip); 
if (!iip || xfs_inode_clean(ip)) { 
ifunlock(ip); 
if (ip != free_ip) 
ifree( 
if (error) 
ifree(tp, ip->i_ino, flist, &delete, &first_ino); 
forkoff = 0;		/* mark the attr fork not in use */ 
format = XFS_DINODE_FMT_EXTENTS; 
if (delete) 
for it to be unpinned. 
force_lsn(ip->i_mount, ip->i_itemp->ili_last_lsn, 0); 
if (xfs_ipincount(ip)) 
while (xfs_ipincount(ip)); 
if (xfs_ipincount(ip)) 
for 
fore 
fore we must drop the link counts before we remove the 
if (XFS_FORCED_SHUTDOWN(mp)) 
if (error) 
if (error) 
if (is_dir) { 
for directory btree deletion(s) implying 
if the bmap 
if (error == ENOSPC) { 
if (error) { 
form some additional validation. 
if (is_dir) { 
if (ip->i_d.di_nlink != 2) { 
if (!xfs_dir_isempty(ip)) { 
if (error) 
if (error) 
for the ".." entry. 
if (error) 
if this is the last link while the inode is locked */ 
if (error) { 
fore returning to 
if (mp->m_flags & (XFS_MOUNT_WSYNC|XFS_MOUNT_DIRSYNC)) 
if (error) 
if (error) 
if (is_dir && xfs_inode_is_filestream(ip)) 
for a rename transaction into a sorted array. 
for_rename( 
if it 
if (ip2) { 
for (i = 0; i < *num_inodes; i++) { 
if (i_tab[j]->i_ino < i_tab[j-1]->i_ino) { 
for_rename(src_dp, target_dp, src_ip, target_ip, 
if (error == ENOSPC) { 
if (error) { 
if (error) { 
if (new_parent) 
if (target_ip) 
if (unlikely((target_dp->i_d.di_flags & XFS_DIFLAG_PROJINHERIT) && 
if (target_ip == NULL) { 
fore actually inserting it. 
if (error) 
for the ".." reference from the new entry. 
if (error == ENOSPC) 
if (error) 
if (new_parent && src_is_directory) { 
if (error) 
if (S_ISDIR(target_ip->i_d.di_mode)) { 
if (!(xfs_dir_isempty(target_ip)) || 
if (error) 
if (error) 
if (src_is_directory) { 
if (error) 
if (new_parent && src_is_directory) { 
if (error) 
if (src_is_directory && (new_parent || target_ip != NULL)) { 
if (error) 
if (error) 
if (new_parent) 
fore returning to 
if (mp->m_flags & (XFS_MOUNT_WSYNC|XFS_MOUNT_DIRSYNC)) { 
if (error) { 
iflush_cluster( 
if (!ilist) 
if (nr_found == 0) 
for (i = 0; i < nr_found; i++) { 
if (iq == ip) 
for a valid inode 
if (!ip->i_ino || 
if the inode is dirty and 
for flushing.  These checks will be repeated 
if (xfs_inode_clean(iq) && xfs_ipincount(iq) == 0) 
if (!xfs_ilock_nowait(iq, XFS_ILOCK_SHARED)) 
if (!xfs_iflock_nowait(iq)) { 
if (xfs_ipincount(iq)) { 
fore flushing. 
if (!xfs_inode_clean(iq)) { 
iflush_int(iq, bp); 
ifunlock(iq); 
if (clcount) { 
fore releasing the buffer. 
if (bufwasdelwri) 
force_shutdown(mp, SHUTDOWN_CORRUPT_INCORE); 
if we have b_iodone functions, 
if (bp->b_iodone) { 
iflush_abort(iq, false); 
iflush( 
iflush_count); 
iflocked(ip)); 
format != XFS_DINODE_FMT_BTREE || 
ife of the stale inode and so 
if (xfs_iflags_test(ip, XFS_ISTALE)) { 
forcibly. If that's the case we must not write this inode 
for an empty AIL as part of the unmount process. 
if (XFS_FORCED_SHUTDOWN(mp)) { 
if (error || !bp) { 
iflush was called with. 
iflush_int(ip, bp); 
for too long. 
if (xfs_buf_ispinned(bp)) 
force(mp, 0); 
if other inodes can be gathered into this write 
iflush_cluster(ip, bp); 
force_shutdown(mp, SHUTDOWN_CORRUPT_INCORE); 
iflush_abort(ip, false); 
iflush_int( 
iflocked(ip)); 
format != XFS_DINODE_FMT_BTREE || 
if (XFS_TEST_ERROR(dip->di_magic != cpu_to_be16(XFS_DINODE_MAGIC), 
if (XFS_TEST_ERROR(ip->i_d.di_magic != XFS_DINODE_MAGIC, 
if (S_ISREG(ip->i_d.di_mode)) { 
format != XFS_DINODE_FMT_EXTENTS) && 
if (S_ISDIR(ip->i_d.di_mode)) { 
format != XFS_DINODE_FMT_EXTENTS) && 
format != XFS_DINODE_FMT_LOCAL), 
if (XFS_TEST_ERROR(ip->i_d.di_nextents + ip->i_d.di_anextents > 
if (XFS_TEST_ERROR(ip->i_d.di_forkoff > mp->m_sb.sb_inodesize, 
forkoff 0x%x, ptr 0x%p", 
for v2 inodes are dependent on the 
if (ip->i_d.di_version < 3) 
if the inode is dirty at all the core must 
if (ip->i_d.di_flushiter == DI_MAX_FLUSH) 
iflush_fork(ip, dip, iip, XFS_DATA_FORK); 
iflush_fork(ip, dip, iip, XFS_ATTR_FORK); 
formation until the data 
iflush_done() routine we clear ili_last_fields, since we 
formation those bits represent is permanently on 
iflush_done().  In order to read the lsn we 
iflush_done to the inode's 
iflush_done, &iip->ili_item); 
if (ip->i_d.di_version == 3) 
file : ./test/kernel/fs/xfs/xfs_trans_buf.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
if a buffer matching the given parameters is already 
for (i = 0; i < nmaps; i++) 
for_each_entry(lidp, &tp->t_items, lid_trans) { 
if (blip->bli_item.li_type == XFS_LI_BUF && 
for it.  Then add the buf item to the transaction. 
if one is there are in xfs_buf_item_init(). 
format.blf_flags & XFS_BLF_CANCEL)); 
if (reset_recur) 
for this transaction on the buf item. 
if it is not already 
if (!tp) 
if (bp != NULL) { 
if (XFS_FORCED_SHUTDOWN(tp->t_mountp)) { 
if (bp == NULL) { 
for the 
if tp is NULL. 
if (tp == NULL) { 
if (bp->b_transp == tp) { 
if (bp == NULL) 
ifdef DEBUG 
if 
for the caller if it is not already 
if (!tp) { 
if (!bp) 
if (bp->b_error) { 
if (error == EFSBADCRC) 
ifdef DEBUG 
if (xfs_error_target == target) { 
if 
if (bp != NULL) { 
if (!(XFS_BUF_ISDONE(bp))) { 
if (XFS_FORCED_SHUTDOWN(mp)) { 
if (error) { 
if (tp->t_flags & XFS_TRANS_DIRTY) 
force_shutdown(tp->t_mountp, 
if (error == EFSBADCRC) 
if (XFS_FORCED_SHUTDOWN(mp)) { 
if (bp == NULL) { 
if (bp->b_error) { 
if (tp->t_flags & XFS_TRANS_DIRTY) 
force_shutdown(tp->t_mountp, SHUTDOWN_META_IO_ERROR); 
if (error == EFSBADCRC) 
ifdef DEBUG 
if (xfs_error_target == target) { 
force_shutdown(tp->t_mountp, 
if 
if the buffer has not 
if the count goes to 0.  If the buffer is not 
if the recursion count goes to 0. 
fore 
if the tp is NULL. 
if (tp == NULL) { 
format.blf_flags & XFS_BLF_CANCEL)); 
for a recursive lock, 
if (bip->bli_recur > 0) { 
if (bip->bli_item.li_desc->lid_flags & XFS_LID_DIRTY) 
fore we should. 
if (bip->bli_flags & XFS_BLI_STALE) 
if it is set. 
if (bip->bli_flags & XFS_BLI_HOLD) { 
fore releasing the buffer back to the 
if (!xfs_buf_item_dirty(bip)) { 
format.blf_flags & XFS_BLF_CANCEL)); 
for this transaction. 
format.blf_flags & XFS_BLF_CANCEL)); 
for more details 
if (bip->bli_flags & XFS_BLI_STALE) { 
format.blf_flags &= ~XFS_BLF_CANCEL; 
if (!(bip->bli_flags & XFS_BLI_ORDERED)) 
if they are 
for the last time (we can tell by the ref 
format structure and log 
fore this should not be replayed. 
if (bip->bli_flags & XFS_BLI_STALE) { 
format.blf_flags & XFS_BLF_INODE_BUF)); 
format.blf_flags & XFS_BLF_CANCEL); 
format.blf_flags &= ~XFS_BLF_INODE_BUF; 
format.blf_flags &= ~XFS_BLFT_MASK; 
formats[i].blf_data_map, 0, 
format structure so that we'll know what to 
if this buffer is 
for this transaction. This means 
ify 
if (!tp) 
format, type); 
format); 
ifferent from regular 
if a _corresponding_ quotaoff has happened. We also have to distinguish 
format.blf_flags |= type; 
file : ./test/kernel/fs/xfs/xfs_itable.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
formation for one inode. 
for filesystem */ 
formatter,	/* formatter, copy to user */ 
if (!buffer || xfs_internal_inum(mp, ino)) 
if (!buf) 
if (error) { 
forkoff = XFS_IFORK_BOFF(ip); 
if_u2.if_rdev; 
formatter(buffer, ubsize, ubused, buf); 
if (ubsize < sizeof(*buffer)) 
if (copy_to_user(ubuffer, buffer, sizeof(*buffer))) 
if (ubused) 
for filesystem */ 
formation in bulk (by-inode) for the filesystem. 
for filesystem */ 
formatter, /* func that'd fill a single buf */ 
if there are more stats to get */ 
for ialloc btree */ 
formatter result */ 
formatter */ 
if there's nothing to do. 
if (agno >= mp->m_sb.sb_agcount || 
if (!ubcountp || *ubcountp <= 0) { 
if (!irbuf) 
while (XFS_BULKSTAT_UBLEFT(ubleft) && agno < mp->m_sb.sb_agcount) { 
if (error) { 
for ialloc btree. 
if (agino > 0) { 
if (!error &&	/* no I/O error */ 
fore our start point) free. 
for (i = 0; i < chunkidx; i++) { 
if (!error) 
while (irbp < irbufend && icount < ubcount) { 
while (error) { 
if (XFS_AGINO_TO_AGBNO(mp, agino) >= 
if (error) { 
if (error || i == 0) { 
for this chunk. 
if (r.ir_freecount < XFS_INODES_PER_CHUNK) { 
if there are any allocated 
for (chunkidx = 0; 
if (xfs_inobt_maskn(chunkidx, 
format all the good inodes into the user's buffer. 
for (irbp = irbuf; 
for (agino = irbp->ir_startino, chunkidx = clustidx = 0; 
if this inode is free. 
if (XFS_INOBT_MASK(chunkidx) & irbp->ir_free) { 
formatter(mp, ino, ubufp, ubleft, 
if (fmterror == BULKSTAT_RV_NOTHING) { 
if (fmterror == BULKSTAT_RV_GIVEUP) { 
if (ubufp) 
for the next loop iteration. 
if (XFS_BULKSTAT_UBLEFT(ubleft)) { 
if (ubelem) 
if (agno >= mp->m_sb.sb_agcount) { 
formation in bulk (by-inode) for the filesystem. 
for filesystem */ 
if there are more stats to get */ 
for bulkstat call */ 
if (error) { 
if that works. 
if (xfs_bulkstat(mp, lastinop, &count, xfs_bulkstat_one, 
if (count == 0 || (xfs_ino_t)*lastinop != ino) 
if (copy_to_user(ubuffer, buffer, count * sizeof(*buffer))) 
for the filesystem. 
for filesystem */ 
formatter) 
while (left > 0 && agno < mp->m_sb.sb_agcount) { 
if (error) { 
if (error) { 
if (error || i == 0) { 
if (bufidx == bcount) { 
if (formatter(ubuffer, buffer, bufidx, &written)) { 
if (left) { 
if (error) { 
if (!error) { 
if (formatter(ubuffer, buffer, bufidx, &written)) 
if (cur) 
if (agbp) 
file : ./test/kernel/fs/xfs/xfs_fsops.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
if (new_version >= 2) { 
if (new_version >= 3) { 
if (new_version >= 4) { 
if (!bp) 
for filesystem */ 
if (nb < mp->m_sb.sb_dblocks || pct < 0 || pct > 100) 
if ((error = xfs_sb_validate_fsb_count(&mp->m_sb, nb))) 
if (!bp) 
if (bp->b_error) { 
if (nb_mod && nb_mod < XFS_MIN_AG_BLOCKS) { 
if (nb < mp->m_sb.sb_dblocks) 
if (nagcount > oagcount) { 
if (error) 
if (error) { 
for (agno = nagcount - 1; agno >= oagcount; agno--, new -= agsize) { 
if (!bp) { 
if (agno == nagcount - 1) 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
if (error) 
if (!bp) { 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
for (bucket = 0; bucket < XFS_AGFL_SIZE(mp); bucket++) 
if (error) 
if (!bp) { 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
if (xfs_sb_version_hasfinobt(&mp->m_sb)) { 
for (bucket = 0; bucket < XFS_AGI_UNLINKED_BUCKETS; bucket++) 
if (error) 
if (!bp) { 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
if (error) 
if (!bp) { 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
if (error) 
if (!bp) { 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
if (error) 
if (xfs_sb_version_hasfinobt(&mp->m_sb)) { 
if (!bp) { 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
if (error) 
if (new) { 
if (error) { 
if (error) { 
if (error) { 
if (nagcount > oagcount) 
if (nb > mp->m_sb.sb_dblocks) 
if (nfree) 
if (dpct) 
if (error) 
if (nagimax) 
if (mp->m_sb.sb_imax_pct) { 
for (agno = 1; agno < nagcount; agno++) { 
if (agno < oagcount) { 
if (bp) { 
if we break early, we'll leave more 
if (error) { 
for ag %d", 
if (error) { 
for ag %d", 
for filesystem */ 
if (nb < XFS_MIN_LOG_BLOCKS || nb < XFS_B_TO_FSB(mp, XFS_MIN_LOG_BYTES)) 
if (nb == mp->m_sb.sb_logblocks && 
while moving it. 
form internal to external log or vice versa. 
if (!capable(CAP_SYS_ADMIN)) 
if (!mutex_trylock(&mp->m_growlock)) 
if (!capable(CAP_SYS_ADMIN)) 
if (!mutex_trylock(&mp->m_growlock)) 
ifree; 
if available. Otherwise return 
if (inval == (__uint64_t *)NULL) { 
if we are freeing or allocation 
if we are near 
while we work out 
if we end up 
if (mp->m_resblks > request) { 
if (lcounter  > 0) {		/* release unused blocks */ 
if (!free) 
if (lcounter < 0) { 
if (outval) { 
if (fdblks_delta) { 
while we were 
if there is anything left to reserve. 
ify_counters(mp, XFS_SBS_FDBLOCKS, 
if (error == ENOSPC) 
for this - that will push dirty state back up 
if (error) { 
if (sb && !IS_ERR(sb)) { 
force_shutdown(mp, SHUTDOWN_FORCE_UMOUNT); 
force_shutdown(mp, SHUTDOWN_FORCE_UMOUNT); 
force_shutdown(mp, 
while keeping the filesystem 
force_shutdown( 
if (!(flags & SHUTDOWN_FORCE_UMOUNT)) { 
forts. 
if (XFS_FORCED_SHUTDOWN(mp) && !logerror) 
if (xfs_log_force_umount(mp, logerror)) 
if (flags & SHUTDOWN_CORRUPT_INCORE) { 
if (XFS_ERRLEVEL_HIGH <= xfs_error_level) 
if (!(flags & SHUTDOWN_FORCE_UMOUNT)) { 
if (flags & SHUTDOWN_DEVICE_REQ) { 
if (!(flags & SHUTDOWN_REMOTE_REQ)) { 
if (!(flags & SHUTDOWN_FORCE_UMOUNT)) { 
ify the problem(s)"); 
file : ./test/kernel/fs/xfs/xfs_dir2_leaf.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
if something is wrong. 
ifdef DEBUG 
if (!xfs_dir3_leaf1_check((dp), (bp))) \ 
while (0); 
if (leafhdr.magic == XFS_DIR3_LEAF1_MAGIC) { 
if (be64_to_cpu(leaf3->info.blkno) != bp->b_bn) 
if (leafhdr.magic != XFS_DIR2_LEAF1_MAGIC) 
if 
ifier, so we need to go the 
if (!hdr) { 
for that from di_size. 
if (hdr->count > ops->leaf_max_ents(geo)) 
format. */ 
for (i = stale = 0; i < hdr->count; i++) { 
if (be32_to_cpu(ents[i].hashval) > 
if (ents[i].address == cpu_to_be32(XFS_DIR2_NULL_DATAPTR)) 
if (hdr->stale != stale) 
ify the magic numbers before decoding the leaf header so that on debug 
ify( 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
if (leaf3->info.hdr.magic != cpu_to_be16(magic3)) 
if (!uuid_equal(&leaf3->info.uuid, &mp->m_sb.sb_uuid)) 
if (be64_to_cpu(leaf3->info.blkno) != bp->b_bn) 
if (leaf->hdr.info.magic != cpu_to_be16(magic)) 
ify( 
if (xfs_sb_version_hascrc(&mp->m_sb) && 
if (!xfs_dir3_leaf_verify(bp, magic)) 
if (bp->b_error) 
ify( 
if (!xfs_dir3_leaf_verify(bp, magic)) { 
ifier_error(bp); 
if (!xfs_sb_version_hascrc(&mp->m_sb)) 
if (bip) 
ify( 
ify(bp, XFS_DIR2_LEAF1_MAGIC); 
ify( 
ify(bp, XFS_DIR2_LEAF1_MAGIC); 
ify( 
ify(bp, XFS_DIR2_LEAFN_MAGIC); 
ify( 
ify(bp, XFS_DIR2_LEAFN_MAGIC); 
ify_read = xfs_dir3_leaf1_read_verify, 
ify_read = xfs_dir3_leafn_read_verify, 
if (!err && tp) 
if (!err && tp) 
if (xfs_sb_version_hascrc(&mp->m_sb)) { 
format directory initialize the tail. 
if (type == XFS_DIR2_LEAF1_MAGIC) { 
if (error) 
if (magic == XFS_DIR2_LEAF1_MAGIC) 
form directory to a leaf form directory. 
if ((error = xfs_da_grow_inode(args, &blkno))) { 
for it. 
if (error) 
formerly occupied by the leaf entries and block 
if (hdr->magic == cpu_to_be32(XFS_DIR2_BLOCK_MAGIC)) 
if (needscan) 
if (needlog) 
if any. 
for (*lowstale = index - 1; *lowstale >= 0; --*lowstale) { 
if any. 
for (*highstale = index; *highstale < leafhdr->count; ++*highstale) { 
if (*lowstale >= 0 && index - *lowstale <= *highstale - index) 
if (!leafhdr->stale) { 
if (index < leafhdr->count) 
for the leaf. 
for the new entry.  It's probably not at 
fore, we need to find the nearest stale 
if (compact == 0) 
if (lowstale >= 0 && 
for the new entry. 
if (index - lowstale - 1 > 0) { 
for the 
if (highstale - index > 0) { 
form directory. 
if (error) 
if there are dup hash values the index is of the first of those. 
if there are any entries with the same hash value 
for the new entry. 
for (use_block = -1, lep = &ents[index]; 
if (be32_to_cpu(lep->address) == XFS_DIR2_NULL_DATAPTR) 
if (be16_to_cpu(bestsp[i]) >= length) { 
if (use_block == -1) { 
for (i = 0; i < be32_to_cpu(ltp->bestcount); i++) { 
if (bestsp[i] == cpu_to_be16(NULLDATAOFF) && 
if (be16_to_cpu(bestsp[i]) >= length) { 
if (!leafhdr.stale) 
if (use_block == -1) 
if it refers to a missing block, so we 
if (use_block != -1 && bestsp[use_block] == cpu_to_be16(NULLDATAOFF)) 
if ((char *)bestsp - (char *)&ents[leafhdr.count] < needbytes && 
if we don't have enough free bytes we need to 
form. 
if ((char *)bestsp - (char *)&ents[leafhdr.count] < needbytes) { 
if ((args->op_flags & XFS_DA_OP_JUSTCHECK) || 
form. 
if (error) 
if (args->op_flags & XFS_DA_OP_JUSTCHECK) { 
fore we've 
if (args->total == 0 && use_block == -1) { 
ift that one to our insertion 
if (compact) { 
if (leafhdr.stale) { 
if (use_block == -1) { 
if ((error = xfs_dir2_grow_inode(args, XFS_DIR2_DATA_SPACE, 
if ((error = xfs_dir3_data_init(args, use_block, &dbp))) { 
if (use_block >= be32_to_cpu(ltp->bestcount)) { 
if (error) { 
for the new entry. 
if (needscan) 
if (needlog) 
if (be16_to_cpu(bestsp[use_block]) != be16_to_cpu(bf[0].length)) { 
if (!grown) 
if any. 
if (!leafhdr->stale) 
for (from = to = 0, loglow = -1; from < leafhdr->count; from++) { 
ifferent. 
if (from > to) { 
if (loglow != -1) 
ift that one to our insertion 
fore us */ 
fore index */ 
if (lowstale >= 0 && 
for (from = to = 0; from < leafhdr->count; from++) { 
if (index == from) 
if (from != keepstale && 
if (from == to) 
for the insertion. 
if (from == keepstale) 
if (from > to) 
if (index == from) 
if (lowstale >= newindex) 
format directory. 
format code. 
if ((error = xfs_dir2_leaf_lookup_int(args, &lbp, &index, &dbp))) { 
if appropriate 
if (error) 
for the first leaf entry with our hash value. 
for (lep = &ents[index]; 
if (be32_to_cpu(lep->address) == XFS_DIR2_NULL_DATAPTR) 
if (newdb != curdb) { 
if (error) { 
if it's an exact match, return the index 
for an exact match. 
if (cmp != XFS_CMP_DIFFERENT && cmp != args->cmpresult) { 
if (cmp == XFS_CMP_EXACT) { 
if required and return it. 
if (args->cmpresult == XFS_CMP_CASE) { 
if (cidb != curdb) { 
if (error) { 
if (dbp) 
format directory. 
if ((error = xfs_dir2_leaf_lookup_int(args, &lbp, &index, &dbp))) { 
former data entry unused. 
if necessary, 
if (needscan) 
if (needlog) 
if (be16_to_cpu(bf[0].length) != oldbest) { 
if (be16_to_cpu(bf[0].length) == 
if ((error = xfs_dir2_shrink_inode(args, db, dbp))) { 
if (error == ENOSPC && args->total == 0) 
if (db == be32_to_cpu(ltp->bestcount) - 1) { 
for the last active entry (i). 
for (i = db - 1; i > 0; i--) { 
if (db != args->geo->datablk) 
if we can convert to block form. 
format directory entry. 
if ((error = xfs_dir2_leaf_lookup_int(args, &lbp, &index, &dbp))) { 
if there are none, the insert point 
for that hash value. 
for */ 
for our hash value. 
for (lep = ents, low = 0, high = leafhdr.count - 1, 
if ((hash = be32_to_cpu(lep[mid].hashval)) == hashwant) 
if (hash < hashwant) 
if (hash == hashwant) { 
while (mid > 0 && be32_to_cpu(lep[mid - 1].hashval) == hashwant) { 
if (hash < hashwant) 
if (error) 
ifdef DEBUG 
if 
if ((error = xfs_dir2_shrink_inode(args, db, dbp))) { 
if (hdr->magic == XFS_DIR2_LEAF1_MAGIC || 
form directory to leaf form directory. 
if we can't do anything. 
for freespace block */ 
for leaf block */ 
if (state->path.active > 1) 
if ((error = xfs_bmap_last_offset(dp, &fo, XFS_DATA_FORK))) { 
while (fo > args->geo->freeblk) { 
if (rval) 
fore the freespace block. 
if ((error = xfs_bmap_last_before(tp, dp, &fo, XFS_DATA_FORK))) { 
if (XFS_FSB_TO_B(mp, fo) > XFS_DIR2_LEAF_OFFSET + args->geo->blksize) 
if (error) 
if the leafn and free data will fit in a leaf1. 
if (xfs_dir3_leaf_size(&leafhdr, freehdr.nvalid) > args->geo->blksize) { 
if (leafhdr.stale) 
if (error) { 
if we can convert the single-leaf directory 
form directory. 
file : ./test/kernel/fs/xfs/xfs_attr.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
for the kernel. 
form_addname(xfs_da_args_t *args); 
if (!name) 
fork = XFS_ATTR_FORK; 
if (args->namelen >= MAXNAMELEN) 
if (!XFS_IFORK_Q(ip) || 
format == XFS_DINODE_FMT_EXTENTS && 
if (XFS_FORCED_SHUTDOWN(ip->i_mount)) 
if (!xfs_inode_hasattr(ip)) 
if (error) 
if (!xfs_inode_hasattr(ip)) 
if (ip->i_d.di_aformat == XFS_DINODE_FMT_LOCAL) 
form_getvalue(&args); 
for the new attribute, 
if it would be 
if (*local) { 
for the attribute value itself. 
if (XFS_FORCED_SHUTDOWN(dp->i_mount)) 
if (error) 
if (error) 
fork, add one. 
if (XFS_IFORK_Q(dp) == 0) { 
fork(dp, sf_size, rsvd); 
fork attributes can use reserved data blocks for this 
if (rsvd) 
if (error) { 
if (error) { 
form list, 
if (dp->i_d.di_aformat == XFS_DINODE_FMT_LOCAL || 
format == XFS_DINODE_FMT_EXTENTS && 
if required). 
if (dp->i_d.di_aformat == XFS_DINODE_FMT_EXTENTS) 
form_create(&args); 
form_addname(&args); 
form mods, and we're done. 
fore returning 
if (mp->m_flags & XFS_MOUNT_WSYNC) 
if (!error && (flags & ATTR_KERNOTIME) == 0) { 
form, transform to a leaf block. 
form_to_leaf(&args); 
if (error) { 
if (committed) 
formation.  We'll need another (linked) 
if (error) 
if (xfs_bmap_one_block(dp, XFS_ATTR_FORK)) 
if (error) 
fore returning to the user. 
if (mp->m_flags & XFS_MOUNT_WSYNC) 
if ((flags & ATTR_KERNOTIME) == 0) 
if (args.trans) { 
form as necessary. 
if (XFS_FORCED_SHUTDOWN(dp->i_mount)) 
if (!xfs_inode_hasattr(dp)) 
if (error) 
if (error) 
fork attributes can use reserved data blocks for this 
if (flags & ATTR_ROOT) 
if (error) { 
if (!xfs_inode_hasattr(dp)) { 
if (dp->i_d.di_aformat == XFS_DINODE_FMT_LOCAL) { 
form_remove(&args); 
if (error) 
fore returning to the user. 
if (mp->m_flags & XFS_MOUNT_WSYNC) 
if ((flags & ATTR_KERNOTIME) == 0) 
if (args.trans) { 
form attribute list structure 
form_addname(xfs_da_args_t *args) 
form_lookup(args); 
if (retval == EEXIST) { 
form_remove(args); 
if (args->namelen >= XFS_ATTR_SF_ENTSIZE_MAX || 
forkoff = xfs_attr_shortform_bytesfit(args->dp, newsize); 
form_add(args, forkoff); 
if bmap_one_block() says there is only one block (ie: no remote blks). 
forkoff; 
if (error) 
if 
for an atomic rename. 
if ((args->flags & ATTR_REPLACE) && (retval == ENOATTR)) { 
if (retval == EEXIST) { 
for later removal*/ 
if required. 
if (retval == ENOSPC) { 
format, then 
if (!error) { 
if (error) { 
if (committed) 
if (error) 
if (error) 
ified for its storage and copy the value.  This is done 
if (args->rmtblkno > 0) { 
if (error) 
if (args->op_flags & XFS_DA_OP_RENAME) { 
if (error) 
if it exists). 
if (args->rmtblkno) { 
if (error) 
if (error) 
if ((forkoff = xfs_attr_shortform_allfit(bp, dp))) { 
form(bp, args, forkoff); 
if (!error) { 
if (error) { 
if (committed) 
if (args->rmtblkno > 0) { 
if bmap_one_block() says there is only one block (ie: no remote blks). 
forkoff; 
if (error) 
if (error == ENOATTR) { 
if ((forkoff = xfs_attr_shortform_allfit(bp, dp))) { 
form(bp, args, forkoff); 
if (!error) { 
if (error) { 
if (committed) 
if bmap_one_block() says there is only one block (ie: no remote blks). 
if (error) 
if (error != EEXIST)  { 
if (!error && (args->rmtblkno > 0) && !(args->flags & ATTR_KERNOVAL)) { 
format attribute list. 
if name already exists, and get back a pointer 
if (error) 
if ((args->flags & ATTR_REPLACE) && (retval == ENOATTR)) { 
if (retval == EEXIST) { 
for later removal*/ 
if (retval == ENOSPC) { 
if (!error) { 
if (error) { 
if (committed) 
if (error) 
if (!error) { 
if (error) { 
if (committed) 
if (error) 
ified for its storage and copy the value.  This is done 
if (args->rmtblkno > 0) { 
if (error) 
if (args->op_flags & XFS_DA_OP_RENAME) { 
if (error) 
if it exists). 
if (args->rmtblkno) { 
if (error) 
if (error) 
if the tree needs to be collapsed. 
if (retval && (state->path.active > 1)) { 
if (!error) { 
if (error) { 
if (committed) 
if (error) 
if (args->rmtblkno > 0) { 
if (error) 
if (state) 
if (error) 
forkoff; 
if name exists, and get back a pointer to it. 
if (error || (retval != EEXIST)) { 
fore we remove the attribute so that we don't 
if (args->rmtblkno > 0) { 
if (error) 
if (error) 
if (error) 
if (error) 
if the tree needs to be collapsed. 
if (retval && (state->path.active > 1)) { 
if (!error) { 
if (error) { 
if (committed) 
if (error) 
if (xfs_bmap_one_block(dp, XFS_ATTR_FORK)) { 
if (error) 
if ((forkoff = xfs_attr_shortform_allfit(bp, dp))) { 
form(bp, args, forkoff); 
if (!error) { 
if (error) { 
if (committed) 
for the buffers 
for those buffers in the "path". 
for (blk = path->blk, level = 0; level < path->active; blk++, level++) { 
for those buffers in the "altpath". 
for (blk = path->blk, level = 0; level < path->active; blk++, level++) { 
for those buffers in the "path". 
for (blk = path->blk, level = 0; level < path->active; blk++, level++) { 
if (error) 
for those buffers in the "altpath". 
for (blk = path->blk, level = 0; level < path->active; blk++, level++) { 
if (error) 
for any attribute fork that has more than one 
if name exists, and get back a pointer to it. 
if (error) { 
if (retval == EEXIST) { 
if (!retval && (args->rmtblkno > 0) 
for (i = 0; i < state->path.active; i++) { 
file : ./test/kernel/fs/xfs/xfs_buf_item.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format_size( 
format, blf_data_map) + 
for the buf log format structure 
format *blfp, 
if (last_bit == -1) 
for a dirty buffer is 2 vectors - the format structure 
format_size(blfp) + XFS_BLF_CHUNK; 
while (last_bit != -1) { 
if there are no more bits set or the start bit is 
if we find a new set of bits bump the number of vecs, 
if (next_bit == -1) { 
if (next_bit != last_bit + 1) { 
if (xfs_buf_offset(bp, next_bit * XFS_BLF_CHUNK) != 
for the buf log format structure and 1 for each 
format structure per region that that is being 
if multiple buffers 
format structures. 
if (bip->bli_flags & XFS_BLI_STALE) { 
format structure with the 
format.blf_flags & XFS_BLF_CANCEL); 
for (i = 0; i < bip->bli_format_count; i++) { 
if (bip->bli_flags & XFS_BLI_ORDERED) { 
for compound 
for the extra buf log format structure that will need to be 
for (i = 0; i < bip->bli_format_count; i++) { 
format_segment( 
format *blfp) 
format item */ 
format_size(blfp); 
if (!(bip->bli_flags & XFS_BLI_STALE) && first_bit == -1) { 
if (bip->bli_flags & XFS_BLI_STALE) { 
format structure with the 
format_stale(bip); 
for each set of contiguous chunks. 
for (;;) { 
if there are no more bits set or the start bit is 
if we start a new set of bits then fill in 
for the series we were looking at and start 
if (next_bit == -1) { 
if (next_bit != last_bit + 1 || 
for the 
format structure, and the rest point to contiguous chunks 
format( 
format flags and clear the in-memory state. 
if the inode buffer allocation has not yet been committed 
if (bip->bli_flags & XFS_BLI_INODE_BUF) { 
format.blf_flags |= XFS_BLF_INODE_BUF; 
if ((bip->bli_flags & (XFS_BLI_ORDERED|XFS_BLI_STALE)) == 
format it. 
format_ordered(bip); 
for (i = 0; i < bip->bli_format_count; i++) { 
formats[i]); 
format(bip); 
while the item is pinned in memory. This means that we can 
for the current transaction. 
forced-shutdown path.  If that is true and the reference count on 
if (atomic_dec_and_test(&bp->b_pin_count)) 
if (freed && stale) { 
format.blf_flags & XFS_BLF_CANCEL); 
if (remove) { 
if (lip->li_desc) 
if (bip->bli_flags & XFS_BLI_STALE_INODE) { 
if (freed && remove) { 
if the buffer 
if (xfs_buf_ispinned(bp)) 
if (!xfs_buf_trylock(bp)) { 
force to unpin the stale buffer. Check for the 
force to move it along. 
if (xfs_buf_ispinned(bp)) 
if ((bp->b_flags & XBF_WRITE_FAIL) && 
if (!xfs_buf_delwri_queue(bp, buffer_list)) 
if necessary but do not unlock the buffer.  This is for support of 
for stale 
if we abort inside commit. 
fore possibly freeing the buf item, copy the per-transaction state 
for the last time. 
if (flags & XFS_BLI_STALE) { 
format.blf_flags & XFS_BLF_CANCEL); 
if (clean) { 
for (i = 0; i < bip->bli_format_count; i++) { 
formats[i].blf_map_size)) { 
if we are 
if it is in the AIL before freeing it. We need to free 
if (atomic_dec_and_test(&bip->bli_refcount)) { 
if (aborted) { 
if (lip->li_flags & XFS_LI_IN_AIL) { 
if (!(flags & XFS_BLI_HOLD)) 
for buffers full of newly allocated 
if ((bip->bli_flags & XFS_BLI_INODE_ALLOC_BUF) && lip->li_lsn != 0) 
format	= xfs_buf_item_format, 
format( 
formats == NULL); 
if (count == 1) { 
formats = &bip->__bli_format; 
formats = kmem_zalloc(count * sizeof(struct xfs_buf_log_format), 
if (!bip->bli_formats) 
format( 
if (bip->bli_formats != &bip->__bli_format) { 
formats); 
if there is already a buf log item for 
if (lip != NULL && lip->li_type == XFS_LI_BUF) 
format(bip, bp->b_map_count); 
for (i = 0; i < bip->bli_format_count; i++) { 
formats[i].blf_type = XFS_LI_BUF; 
formats[i].blf_len = bp->b_maps[i].bm_len; 
if (bp->b_fspriv) 
if (bit) { 
while ((bits_to_set - bits_set) >= NBWORD) { 
if (end_bit) { 
for (i = 0; i < bip->bli_format_count; i++) { 
if (first > end) { 
if (first < start) 
if (end > last) 
formats[i].blf_data_map[0]); 
if the buffer has been logged or ordered in a transaction (at any 
format(bip); 
if (bp->b_fspriv == NULL) 
if there is a first, because the buf item code 
if (head_lip) { 
for a single 
ify the 
ify the list attached to the buffer and we don't 
while ((lip = bp->b_fspriv) != NULL) { 
if the item is added to another buf. 
for buffers which have had callbacks 
if (likely(!bp->b_error)) 
if (XFS_FORCED_SHUTDOWN(mp)) { 
if (bp->b_target != lasttarg || 
iffies; 
for the 
if the writes keep failing. 
if (XFS_BUF_ISASYNC(bp)) { 
if (!(bp->b_flags & (XBF_STALE|XBF_WRITE_FAIL))) { 
for buffers which have been 
forcibly shutting down, this may well be 
forcibly. xfs_trans_ail_delete() takes care of these. 
if we're forcing a shutdown. 
file : ./test/kernel/fs/xfs/xfs_extfree_item.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
if (efip->efi_format.efi_nextents > XFS_EFI_MAX_FAST_EXTENTS) 
if it has already 
if (atomic_dec_and_test(&efip->efi_refcount)) { 
for an efi item.  It just logs the efi_log_format 
format) + 
for the 
format structure embedded in the efi item. 
format( 
format.efi_nextents); 
format.efi_size = 1; 
format, 
for an efi item, so just return. 
if (remove) { 
if (lip->li_desc) 
if (lip->li_flags & XFS_LI_ABORTED) 
for inodes, the inode is locked throughout the extent freeing 
format	= xfs_efi_item_format, 
if (nextents > XFS_EFI_MAX_FAST_EXTENTS) { 
format.efi_nextents = nextents; 
format buffer from the given buf, and into the destination 
ifferent padding), 
format for this kernel. 
format(xfs_log_iovec_t *buf, xfs_efi_log_format_t *dst_efi_fmt) 
format_t) +  
format_32_t) +  
format_64_t) +  
if (buf->i_len == len) { 
if (buf->i_len == len32) { 
format_32_t *src_efi_fmt_32 = buf->i_addr; 
for (i = 0; i < dst_efi_fmt->efi_nextents; i++) { 
if (buf->i_len == len64) { 
format_64_t *src_efi_fmt_64 = buf->i_addr; 
for (i = 0; i < dst_efi_fmt->efi_nextents; i++) { 
if (atomic_sub_and_test(nextents, &efip->efi_next_extent)) { 
if (test_bit(XFS_EFI_RECOVERED, &efip->efi_flags)) 
if (efdp->efd_format.efd_nextents > XFS_EFD_MAX_FAST_EXTENTS) 
for an efd item.  It just logs the efd_log_format 
format_t) + 
for the 
format structure embedded in the efd item. 
format( 
format.efd_nextents); 
format.efd_size = 1; 
format, 
for an efd item, so just return. 
for an efd item, unpinning does 
for the log to be flushed to disk. 
if (lip->li_flags & XFS_LI_ABORTED) 
fore the EFD got aborted. 
if (!(lip->li_flags & XFS_LI_ABORTED)) 
format.efd_nextents); 
for inodes, the inode is locked throughout the extent freeing 
format	= xfs_efd_item_format, 
if (nextents > XFS_EFD_MAX_FAST_EXTENTS) { 
format.efd_nextents = nextents; 
file : ./test/kernel/fs/xfs/xfs_ioctl.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
for a mount point or path within that mount point 
for a FD opened in user space 
for a path 
if (cmd == XFS_IOC_FD_TO_HANDLE) { 
if (!f.file) 
if (error) 
for inodes residing on a XFS filesystem, 
if (inode->i_sb->s_magic != XFS_SB_MAGIC) 
if (!S_ISREG(inode->i_mode) && 
if (cmd == XFS_IOC_PATH_TO_FSHANDLE) { 
if (copy_to_user(hreq->ohandle, &handle, hsize) || 
if (cmd == XFS_IOC_FD_TO_HANDLE) 
if (!S_ISDIR(file_inode(parfilp)->i_mode)) 
if (hlen != sizeof(xfs_handle_t)) 
if (copy_from_user(&handle, uhandle, hlen)) 
if (handle.ha_fid.fid_len != 
if (!capable(CAP_SYS_ADMIN)) 
if (IS_ERR(dentry)) 
if (!(S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode))) { 
if BITS_PER_LONG != 32 
if 
if ((!(permflag & O_APPEND) || (permflag & O_TRUNC)) && 
if ((fmode & FMODE_WRITE) && IS_IMMUTABLE(inode)) { 
if (S_ISDIR(inode->i_mode) && (fmode & FMODE_WRITE)) { 
if (fd < 0) { 
if (IS_ERR(filp)) { 
if (S_ISREG(inode->i_mode)) { 
if (!capable(CAP_SYS_ADMIN)) 
if (IS_ERR(dentry)) 
if (!S_ISLNK(dentry->d_inode->i_mode)) { 
if (copy_from_user(&olen, hreq->ohandlen, sizeof(__u32))) { 
if (!link) { 
if (error) 
if (error) 
if (!capable(CAP_SYS_ADMIN)) 
if (XFS_FORCED_SHUTDOWN(mp)) 
if (error) { 
if (!capable(CAP_MKNOD)) 
if (copy_from_user(&dmhreq, arg, sizeof(xfs_fsop_setdm_handlereq_t))) 
if (error) 
if (IS_ERR(dentry)) { 
if (IS_IMMUTABLE(dentry->d_inode) || IS_APPEND(dentry->d_inode)) { 
if (copy_from_user(&fsd, dmhreq.data, sizeof(fsd))) { 
if (!capable(CAP_SYS_ADMIN)) 
if (copy_from_user(&al_hreq, arg, sizeof(xfs_fsop_attrlist_handlereq_t))) 
if (al_hreq.buflen < sizeof(struct attrlist) || 
if (al_hreq.flags & ~(ATTR_ROOT | ATTR_SECURE)) 
if (IS_ERR(dentry)) 
if (!kbuf) 
if (error) 
if (copy_to_user(al_hreq.buffer, kbuf, al_hreq.buflen)) 
if (*len > XATTR_SIZE_MAX) 
if (!kbuf) 
if (error) 
if (copy_to_user(ubuf, kbuf, *len)) 
if (IS_IMMUTABLE(inode) || IS_APPEND(inode)) 
if (len > XATTR_SIZE_MAX) 
if (IS_ERR(kbuf)) 
if (IS_IMMUTABLE(inode) || IS_APPEND(inode)) 
if (!capable(CAP_SYS_ADMIN)) 
if (copy_from_user(&am_hreq, arg, sizeof(xfs_fsop_attrmulti_handlereq_t))) 
if (am_hreq.opcount >= INT_MAX / sizeof(xfs_attr_multiop_t)) 
if (IS_ERR(dentry)) 
if (!size || size > 16 * PAGE_SIZE) 
if (IS_ERR(ops)) { 
if (!attr_name) 
for (i = 0; i < am_hreq.opcount; i++) { 
if (ops[i].am_error == 0 || ops[i].am_error == MAXNAMELEN) 
if (ops[i].am_error < 0) 
if (ops[i].am_error) 
if (ops[i].am_error) 
if (copy_to_user(am_hreq.ops, ops, size)) 
if (!xfs_sb_version_hasextflgbit(&ip->i_mount->m_sb) && 
if (inode->i_flags & (S_IMMUTABLE|S_APPEND)) 
if (!(filp->f_mode & FMODE_WRITE)) 
if (!S_ISREG(inode->i_mode)) 
if (error) 
for resv/unresv/zero is invalid.  length for 
if (bf->l_len <= 0) { 
if (bf->l_start < 0 || 
if (!error) 
if (!error) 
if (bf->l_start > XFS_ISIZE(ip)) { 
if (error) 
if (!error) 
if (error) 
if (error) { 
if (!(ioflags & IO_INVIS)) { 
if (ip->i_d.di_mode & S_IXGRP) 
if (setprealloc) 
if (clrprealloc) 
if (filp->f_flags & O_DSYNC) 
if there are more stats to get and if bulkstat */ 
if (!capable(CAP_SYS_ADMIN)) 
if (XFS_FORCED_SHUTDOWN(mp)) 
if (copy_from_user(&bulkreq, arg, sizeof(xfs_fsop_bulkreq_t))) 
if (copy_from_user(&inlast, bulkreq.lastip, sizeof(__s64))) 
if ((count = bulkreq.icount) <= 0) 
if (bulkreq.ubuffer == NULL) 
if (cmd == XFS_IOC_FSINUMBERS) 
if (cmd == XFS_IOC_FSBULKSTAT_SINGLE) 
if (error) 
if (bulkreq.ocount != NULL) { 
if (copy_to_user(bulkreq.ocount, &count, sizeof(count))) 
if (error) 
if (copy_to_user(arg, &fsgeo, sizeof(xfs_fsop_geom_v1_t))) 
if (error) 
if (copy_to_user(arg, &fsgeo, sizeof(fsgeo))) 
if (flags & FS_IMMUTABLE_FL) 
if (flags & FS_APPEND_FL) 
if (flags & FS_SYNC_FL) 
if (flags & FS_NOATIME_FL) 
if (flags & FS_NODUMP_FL) 
if (di_flags & XFS_DIFLAG_IMMUTABLE) 
if (di_flags & XFS_DIFLAG_APPEND) 
if (di_flags & XFS_DIFLAG_SYNC) 
if (di_flags & XFS_DIFLAG_NOATIME) 
if (di_flags & XFS_DIFLAG_NODUMP) 
if (attr) { 
if (ip->i_afp->if_flags & XFS_IFEXTENTS) 
if (ip->i_df.if_flags & XFS_IFEXTENTS) 
if (copy_to_user(arg, &fa, sizeof(fa))) 
iflags( 
if (xflags & XFS_XFLAG_IMMUTABLE) 
if (xflags & XFS_XFLAG_APPEND) 
if (xflags & XFS_XFLAG_SYNC) 
if (xflags & XFS_XFLAG_NOATIME) 
if (xflags & XFS_XFLAG_NODUMP) 
if (xflags & XFS_XFLAG_PROJINHERIT) 
if (xflags & XFS_XFLAG_NODEFRAG) 
if (xflags & XFS_XFLAG_FILESTREAM) 
if (S_ISDIR(ip->i_d.di_mode)) { 
if (xflags & XFS_XFLAG_NOSYMLINKS) 
if (xflags & XFS_XFLAG_EXTSZINHERIT) 
if (S_ISREG(ip->i_d.di_mode)) { 
if (xflags & XFS_XFLAG_EXTSIZE) 
iflags_to_linux( 
if (xflags & XFS_XFLAG_IMMUTABLE) 
if (xflags & XFS_XFLAG_APPEND) 
if (xflags & XFS_XFLAG_SYNC) 
if (xflags & XFS_XFLAG_NOATIME) 
if (mp->m_flags & XFS_MOUNT_RDONLY) 
if (XFS_FORCED_SHUTDOWN(mp)) 
if ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) && 
fore we start any other transactions. Trying to do this later 
fore we take the ilock, we're covered 
if (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) { 
if (code) 
if (code) 
if (!inode_owner_or_capable(VFS_I(ip))) { 
if projid is actually going to change. 
ifier. 
if (mask & FSX_PROJID) { 
if (XFS_IS_QUOTA_RUNNING(mp) && 
if (code)	/* out of quota */ 
if (mask & FSX_EXTSIZE) { 
if any extents are allocated. 
if (ip->i_d.di_nextents && 
if set at all. It must also be smaller than the 
for non-realtime files, limit the extent size hint to 
if (fa->fsx_extsize != 0) { 
if (extsize_fsb > MAXEXTLEN) { 
if (XFS_IS_REALTIME_INODE(ip) || 
if (extsize_fsb > mp->m_sb.sb_agblocks / 2) { 
if (fa->fsx_extsize % size) { 
if (mask & FSX_XFLAGS) { 
if any extents are allocated. 
if ((ip->i_d.di_nextents || ip->i_delayed_blks) && 
if ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) { 
ify an immutable/append-only file unless 
if ((ip->i_d.di_flags & 
if (mask & FSX_PROJID) { 
if ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) && 
ifications 
if (xfs_get_projid(ip) != fa->fsx_projid) { 
if (mask & FSX_EXTSIZE) 
if (mask & FSX_XFLAGS) { 
iflags_to_linux(ip); 
fore returning to the user. 
for wsync filesystems. 
if (mp->m_flags & XFS_MOUNT_WSYNC) 
fore chown. 
if (lock_flags) 
if (copy_from_user(&fa, arg, sizeof(fa))) 
if (filp->f_flags & (O_NDELAY|O_NONBLOCK)) 
if (error) 
if (copy_to_user(arg, &flags, sizeof(flags))) 
if (copy_from_user(&flags, arg, sizeof(flags))) 
if (flags & ~(FS_IMMUTABLE_FL | FS_APPEND_FL | \ 
if (filp->f_flags & (O_NDELAY|O_NONBLOCK)) 
if (error) 
format(void **ap, struct getbmapx *bmv, int *full) 
if (copy_to_user(base, bmv, sizeof(struct getbmap))) 
if (copy_from_user(&bmx, arg, sizeof(struct getbmapx))) 
if (bmx.bmv_count < 2) 
iflags = (cmd == XFS_IOC_GETBMAPA ? BMV_IF_ATTRFORK : 0); 
iflags |= BMV_IF_NO_DMAPI_READ; 
format, 
if (error) 
if (copy_to_user(arg, &bmx, sizeof(struct getbmap))) 
format(void **ap, struct getbmapx *bmv, int *full) 
if (copy_to_user(base, bmv, sizeof(struct getbmapx))) 
if (copy_from_user(&bmx, arg, sizeof(bmx))) 
if (bmx.bmv_count < 2) 
if (bmx.bmv_iflags & (~BMV_IF_VALID)) 
format, 
if (error) 
if (copy_to_user(arg, &bmx, sizeof(struct getbmapx))) 
formation for the target fd */ 
if (!f.file) { 
if (!(f.file->f_mode & FMODE_WRITE) || 
if (!tmp.file) { 
if (!(tmp.file->f_mode & FMODE_WRITE) || 
if (IS_SWAPFILE(file_inode(f.file)) || 
if (ip->i_mount != tip->i_mount) { 
if (ip->i_ino == tip->i_ino) { 
if (XFS_FORCED_SHUTDOWN(ip->i_mount)) { 
if (filp->f_mode & FMODE_NOCMTIME) 
if (copy_from_user(&bf, arg, sizeof(bf))) 
if (copy_to_user(arg, &da, sizeof(da))) 
if (copy_from_user(&dmi, arg, sizeof(dmi))) 
if (error) 
if (copy_from_user(&hreq, arg, sizeof(hreq))) 
if (copy_from_user(&hreq, arg, sizeof(xfs_fsop_handlereq_t))) 
if (copy_from_user(&hreq, arg, sizeof(xfs_fsop_handlereq_t))) 
if (copy_from_user(&sxp, arg, sizeof(xfs_swapext_t))) 
if (error) 
if (error) 
if (copy_to_user(arg, &out, sizeof(out))) 
if (!capable(CAP_SYS_ADMIN)) 
if (mp->m_flags & XFS_MOUNT_RDONLY) 
if (copy_from_user(&inout, arg, sizeof(inout))) 
if (error) 
if (error) 
if (copy_to_user(arg, &inout, sizeof(inout))) 
if (!capable(CAP_SYS_ADMIN)) 
if (error) 
if (copy_to_user(arg, &out, sizeof(out))) 
if (copy_from_user(&in, arg, sizeof(in))) 
if (error) 
if (copy_from_user(&in, arg, sizeof(in))) 
if (error) 
if (copy_from_user(&in, arg, sizeof(in))) 
if (error) 
if (!capable(CAP_SYS_ADMIN)) 
if (get_user(in, (__uint32_t __user *)arg)) 
if (!capable(CAP_SYS_ADMIN)) 
if (copy_from_user(&in, arg, sizeof(in))) 
if (!capable(CAP_SYS_ADMIN)) 
if (!capable(CAP_SYS_ADMIN)) 
if (mp->m_flags & XFS_MOUNT_RDONLY) 
if (copy_from_user(&eofb, arg, sizeof(eofb))) 
if (error) 
file : ./test/kernel/fs/xfs/xfs_qm_syscalls.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
forcement for all udquots and/or 
ifies the ondisk dquot directly. Therefore, for example, 
if ((mp->m_qflags & flags) == 0) 
formance 
forcement, change mp and go. 
if ((flags & XFS_ALL_QUOTA_ACCT) == 0) { 
if error ? Revert back to old vals incore ? */ 
forcement off, clear the 
if (flags & XFS_UQUOTA_ACCT) { 
if (flags & XFS_GQUOTA_ACCT) { 
if (flags & XFS_PQUOTA_ACCT) { 
forcement. 
if ((mp->m_qflags & flags) == 0) 
if we crash. 
if (error) 
if the particular 
for quotaon, we can 
fore ACTIVE state bit was cleared 
ification, those dquots __will__ 
if (error) { 
force_shutdown(mp, SHUTDOWN_CORRUPT_INCORE); 
if (mp->m_qflags == 0) { 
if we don't need them anymore. 
if ((dqtype & XFS_QMOPT_UQUOTA) && q->qi_uquotaip) { 
if ((dqtype & XFS_QMOPT_GQUOTA) && q->qi_gquotaip) { 
if ((dqtype & XFS_QMOPT_PQUOTA) && q->qi_pquotaip) { 
if (ino == NULLFSINO) 
if (error) 
if (error) { 
if (error) { 
if (!xfs_sb_version_hasquota(&mp->m_sb) || flags == 0 || 
if (flags & XFS_DQ_USER) { 
if (error) 
if (flags & XFS_DQ_GROUP) { 
if (error) 
if (flags & XFS_DQ_PROJ) 
forcement for a filesystem.  This takes 
if (flags == 0) { 
force without accounting. We check the superblock 
if (((flags & XFS_UQUOTA_ACCT) == 0 && 
force without acct, flags=%x sbflags=%x", 
if ((mp->m_qflags & flags) == flags) 
if this is the root filesystem. 
if it's the same. 
if ((qf & flags) == flags && sbflags == 0) 
if ((error = xfs_qm_write_sb_changes(mp, sbflags))) 
forcement, we are done. 
if  (((mp->m_sb.sb_qflags & XFS_UQUOTA_ACCT) != 
if (! XFS_IS_QUOTA_RUNNING(mp)) 
forcement in core. 
formation, such as uquota-off, enforcements, etc. 
if (!xfs_sb_version_hasquota(&mp->m_sb)) { 
if (q) { 
if (!uip && mp->m_sb.sb_uquotino != NULLFSINO) { 
if (!gip && mp->m_sb.sb_gquotino != NULLFSINO) { 
for both group and project quotas. 
formation available. 
if (!gip) { 
if (xfs_iget(mp, NULL, mp->m_sb.sb_pquotino, 
if (uip) { 
if (tempuqip) 
if (gip) { 
if (tempgqip) 
if (pip) { 
if (temppqip) 
if (q) { 
formation, such as uquota-off, enforcements, etc. 
if (!xfs_sb_version_hasquota(&mp->m_sb)) { 
if (q) { 
if (!uip && mp->m_sb.sb_uquotino != NULLFSINO) { 
if (!gip && mp->m_sb.sb_gquotino != NULLFSINO) { 
if (!pip && mp->m_sb.sb_pquotino != NULLFSINO) { 
if (uip) { 
if (tempuqip) 
if (gip) { 
if (tempgqip) 
if (pip) { 
if (temppqip) 
if (q) { 
if (newlim->d_fieldmask & ~XFS_DQ_MASK) 
if ((newlim->d_fieldmask & XFS_DQ_MASK) == 0) 
fore we start, as we need to do a 
if (error) { 
if (error) { 
fore changing. 
if (hard == 0 || hard >= soft) { 
if (id == 0) { 
if (hard == 0 || hard >= soft) { 
if (id == 0) { 
if (hard == 0 || hard >= soft) { 
if (id == 0) { 
if requested 
if (newlim->d_fieldmask & FS_DQ_BWARNS) 
if (newlim->d_fieldmask & FS_DQ_IWARNS) 
if (newlim->d_fieldmask & FS_DQ_RTBWARNS) 
if (id == 0) { 
for the super user set the relative time 
for the default 
for warnings. 
if (newlim->d_fieldmask & FS_DQ_BTIMER) { 
if (newlim->d_fieldmask & FS_DQ_ITIMER) { 
if (newlim->d_fieldmask & FS_DQ_RTBTIMER) { 
if (newlim->d_fieldmask & FS_DQ_BWARNS) 
if (newlim->d_fieldmask & FS_DQ_IWARNS) 
if (newlim->d_fieldmask & FS_DQ_RTBWARNS) 
forcement 
if (error) { 
fore we 
formance. 
if (error) 
fore we 
formance. 
if (error) { 
ifying sb_qflags, so this is OK. 
if (error) 
if (XFS_IS_DQUOT_UNINITIALIZED(dqp)) { 
forcement 
if ((!XFS_IS_UQUOTA_ENFORCED(mp) && 
ifdef DEBUG 
if ((dst->d_bcount > dst->d_blk_softlimit) && 
if ((dst->d_icount > dst->d_ino_softlimit) && 
if 
if (flags & XFS_UQUOTA_ACCT) 
if (flags & XFS_GQUOTA_ACCT) 
if (flags & XFS_PQUOTA_ACCT) 
if (flags & XFS_UQUOTA_ENFD) 
if (flags & XFS_GQUOTA_ENFD) 
if (flags & XFS_PQUOTA_ENFD) 
if (ip == ip->i_mount->m_quotainfo->qi_uquotaip || 
if ((flags & XFS_UQUOTA_ACCT) && ip->i_udquot) { 
if ((flags & XFS_GQUOTA_ACCT) && ip->i_gdquot) { 
if ((flags & XFS_PQUOTA_ACCT) && ip->i_pdquot) { 
ified to indicate that quotas are off 
file : ./test/kernel/fs/xfs/xfs_log_cil.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
for the ticket - we are going to steal whatever 
fore we can 
for space used by log headers and additional 
ifference in 
iff_len, 
for the new LV being passed in */ 
iff_len += lv->lv_bytes; 
for and free it. 
if (!old_lv) 
if (old_lv != lv) { 
iff_len -= old_lv->lv_bytes; 
if (!lv->lv_item->li_seq) 
formatted buffer containing all the 
ified at the time it gets written into the iclog. 
for the changes in each log item in the 
formats the vector for the item into the buffer. 
for tracking until the next checkpoint is written out. 
formatting step to write the regions into the iclog buffer.  Writing the 
format of the 
format the regions into the iclog as though they are being formatted 
format_items( 
iff_len, 
if we didn't find a log item.  */ 
for_each_entry(lidp, &tp->t_items, lid_trans) { 
if (!(lidp->lid_flags & XFS_LID_DIRTY)) 
for writing */ 
for copying data. 
if (niovecs == XFS_LOG_VEC_ORDERED) { 
for that slack space here. Then round nbytes up 
ify. 
if it exists for reservation accounting */ 
if (lip->li_lv && buf_size <= lip->li_lv->lv_size) { 
if (ordered) 
iff_iovecs -= lv->lv_niovecs; 
if (ordered) { 
format(lip, lv); 
iff_len, diff_iovecs); 
if the change requires additional log metadata. If it does, take that space 
iff_iovecs = 0; 
iff_iovecs); 
ified at the tail of the CIL. 
for_each_entry(lidp, &tp->t_items, lid_trans) { 
if (!(lidp->lid_flags & XFS_LID_DIRTY)) 
for space used by new iovec headers  */ 
iff_iovecs; 
if (!list_empty(&tp->t_busy)) 
for the checkpoint. The context ticket is special - the unit 
if (ctx->ticket->t_curr_res == 0) { 
for more log record headers? */ 
if (len > 0 && (ctx->space_used / iclog_space != 
for (lv = log_vector; lv; ) { 
force_lsn() waiting on a sequence commit that 
if (abort) 
if (!list_empty(&ctx->busy_extents)) { 
if the 
for it to 
for a match. Hence we can allows log 
for the same sequence they will block on 
if (!cil) 
if we've anything to push. If there is nothing, then we don't 
if (list_empty(&cil->xc_cil)) { 
for a previously pushed seqeunce */ 
while (!list_empty(&cil->xc_cil)) { 
if (!ctx->lv_chain) 
forces to extract the commit lsn of the sequence that 
if we get an EFI in one checkpoint and the EFD in the 
forces), we do not want the checkpoint with 
for us to write out a commit record 
force_lsn requires us to mirror the new sequence into the cil 
forces without risking 
for the space used by the 
if (error) 
for_each_entry(new_ctx, &cil->xc_committing, committing) { 
if (XLOG_FORCED_SHUTDOWN(log)) { 
for this one so skip them. 
if (new_ctx->sequence >= ctx->sequence) 
if (!new_ctx->commit_lsn) { 
for the push to 
if (commit_lsn == -1) 
ify(log->l_mp, commit_iclog, &ctx->log_cb); 
for the commit to complete. 
while holding the 
if we haven't used up all the 
if (cil->xc_ctx->space_used < XLOG_CIL_SPACE_LIMIT(log)) 
if (cil->xc_push_seq < cil->xc_current_sequence) { 
for 
if it is required. 
if (!cil) 
if (list_empty(&cil->xc_cil) || push_seq <= cil->xc_push_seq) { 
if (list_empty(&cil->xc_cil)) 
if required and 
for the space used by the transaction. Once we have done that we 
if (flags & XFS_TRANS_RELEASE_LOG_RES) 
if (tp->t_ticket->t_curr_res < 0) 
if (commit_lsn) 
fore we drop the CIL context lock because we 
fore we've updated and unlocked 
if we haven't already pushed the sequence 
if the push sequence is the same as the current context. 
force_lsn( 
if we need to force out the current context. 
for the same sequence, 
if we can find a previous sequence still committing. 
for all previous sequence commits to complete 
for those as well. 
for_each_entry(ctx, &cil->xc_committing, committing) { 
if (XLOG_FORCED_SHUTDOWN(log)) 
if (ctx->sequence > sequence) 
if (!ctx->commit_lsn) { 
for the push to 
if (ctx->sequence != sequence) 
if the current sequence still matches the 
if the CIL is clean at the time of the 
for this sequence again from the start just in case. 
if (sequence == cil->xc_current_sequence && 
force 
forces (i.e. 
if the current log item was first committed in this sequence. 
for this to be used in a non-racy manner, it has to be called with 
format into the item. 
if (list_empty(&lip->li_cil)) 
if it is different to the 
if (XFS_LSN_CMP(lip->li_seq, ctx->sequence) != 0) 
form initial CIL structure initialisation. 
if (!cil) 
if (!ctx) { 
if (log->l_cilp->xc_ctx) { 
file : ./test/kernel/fs/xfs/xfs_export.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
if (!parent) 
if (!(XFS_M(inode->i_sb)->m_flags & XFS_MOUNT_SMALL_INUMS) || 
if there is enough space given.  In practice 
if (*max_len < len) { 
for ino 0.  Fail them gracefully. 
if (ino == 0) 
if (error) { 
if (error == EINVAL || error == ENOENT) 
if (ip->i_d.di_gen != generation) { 
if (fh_len < xfs_fileid_length(fileid_type)) 
if (fh_len < xfs_fileid_length(fileid_type)) 
if (unlikely(error)) 
if (xfs_ipincount(ip)) 
if (!lsn) 
force_lsn(mp, lsn, XFS_LOG_SYNC, NULL); 
file : ./test/kernel/fs/xfs/xfs_icreate_item.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
for the icreate log structure. 
for the 
format( 
format, 
for the create item, so just return. */ 
for the create item, so just return. */ 
if (icp->ic_item.li_flags & XFS_LI_ABORTED) 
for the inode 
format	= xfs_icreate_item_format, 
for a newly allocated (in-core) inode. 
for the inode chunk by offset within an AG as well as the 
format.icl_type = XFS_LI_ICREATE; 
format.icl_ag = cpu_to_be32(agno); 
format.icl_count = cpu_to_be32(count); 
format.icl_length = cpu_to_be32(length); 
file : ./test/kernel/fs/xfs/xfs_bmap.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as 
for more details. 
if not, write the Free Software Foundation, 
format.h" 
format.h" 
fork)	/* data or attr fork */ 
if we are in ATTR1 that 
fork offset of all the inodes will be 
forkoff's fixed but probably at various positions. Therefore, 
if (whichfork == XFS_DATA_FORK) { 
for (level = 1; maxblocks > 1; level++) { 
fork] = level; 
if the inode needs to be converted to btree format. 
fork) 
fork) > 
if the inode should be converted to extent format. 
fork) 
fork) <= 
for ip's delayed extent of length "len". 
for (level = 0, rval = 0; 
if (len == 1) 
if (level == 0) 
fork offset for newly created inodes. 
if (mp->m_sb.sb_inodesize == 256) { 
forkoff field when switching 
for inline data fork extents. 
forkoff_reset( 
fork) 
format != XFS_DINODE_FMT_DEV && 
format != XFS_DINODE_FMT_BTREE) { 
if (dfl_forkoff > ip->i_d.di_forkoff) 
forkoff = dfl_forkoff; 
if (block->bb_magic != cpu_to_be32(XFS_BMAP_CRC_MAGIC) && 
if (be16_to_cpu(block->bb_level) != level || 
ifdef DEBUG 
if (!cur) 
for (i = 0; i < XFS_BTREE_MAXLEVELS; i++) { 
if (XFS_BUF_ADDR(cur->bc_bufs[i]) == bno) 
if the bp is there */ 
for_each_entry(lidp, &cur->bc_tp->t_items, lid_trans) { 
if (bip->bli_item.li_type == XFS_LI_BUF && 
for( i = 1; i <= xfs_btree_get_numrecs(block); i++) { 
if (prevp) { 
if there are dups. 
if (root) 
for (j = i+1; j <= be16_to_cpu(block->bb_numrecs); j++) { 
if (*thispa == *pp) { 
for the inode ip are in the right order in all 
fork)	/* data or attr fork */ 
for "block" */ 
ifork_t		*ifp;	/* fork structure */ 
for checking */ 
if (XFS_IFORK_FORMAT(ip, whichfork) != XFS_DINODE_FMT_BTREE) { 
ifp = XFS_IFORK_PTR(ip, whichfork); 
ifp->if_broot_bytes); 
while (level-- > 0) { 
if (!bp) { 
if (error) 
if (level == 0) 
for basic sanity (increasing keys and 
if (bp_release) { 
for (;;) { 
if any. 
form with the first entry in this one. 
if (i) { 
for (j = 1; j < num_recs; j++) { 
if (bp_release) { 
if (bno == NULLFSBLOCK) 
if (!bp) { 
if (error) 
if (bp_release) { 
if (bp_release) 
for %d extents", 
for all the contents of the extent records. 
fork,	/* data or attr fork */ 
ifork_t	*ifp;		/* inode fork pointer */ 
if (whichfork == XFS_ATTR_FORK) 
ifp = XFS_IFORK_PTR(ip, whichfork); 
for (idx = 0; idx < cnt; idx++) 
ifically check the 
if the XFS_BMAPI_ENTIRE flag was set. 
for (i = 0; i < ret_nmap; i++) { 
if (!(flags & XFS_BMAPI_ENTIRE)) { 
fork)		do { } while (0) 
if /* DEBUG */ 
ifdef DEBUG 
if 
for (prev = NULL, cur = flist->xbf_first; 
if (cur->xbfi_startblock >= bno) 
if (prev) 
if any */ 
if (prev) 
if (flist->xbf_count == 0) 
for (free = flist->xbf_first; free; free = next) { 
fork format manipulation functions 
form a btree format file with only one leaf node, where the 
for the btree root and pitch the leaf block. 
fork)  /* data or attr fork */ 
ifork_t		*ifp;	/* inode fork data */ 
ifp = XFS_IFORK_PTR(ip, whichfork); 
fork) == XFS_DINODE_FMT_BTREE); 
ifp->if_broot_bytes, 0) == 1); 
ifdef DEBUG 
if 
if (error) 
if ((error = xfs_btree_check_block(cur, cblock, 0, cbp))) 
if (cur->bc_bufs[0] == cbp) 
fork); 
ifp->if_flags & XFS_IFBROOT) == 0); 
fork, XFS_DINODE_FMT_EXTENTS); 
format file into a btree-format file. 
fork)	/* data or attr fork */ 
for ablock */ 
ifork_t		*ifp;		/* inode fork pointer */ 
ifp = XFS_IFORK_PTR(ip, whichfork); 
fork) == XFS_DINODE_FMT_EXTENTS); 
fork); 
ifp->if_broot; 
fork); 
fork, XFS_DINODE_FMT_BTREE); 
if (*firstblock == NULLFSBLOCK) { 
if (flist->xbf_low) { 
if ((error = xfs_alloc_vextent(&args))) { 
fork); 
if (xfs_sb_version_hascrc(&mp->m_sb)) 
ifp->if_bytes / (uint)sizeof(xfs_bmbt_rec_t); 
for (cnt = i = 0; i < nextents; i++) { 
if (!isnullstartblock(xfs_bmbt_get_startblock(ep))) { 
fork)); 
fork); 
for data forks of regular files, 
fork) 
fork) == XFS_DINODE_FMT_LOCAL); 
fork) == 0); 
ifp->if_flags &= ~XFS_IFINLINE; 
fork, XFS_DINODE_FMT_EXTENTS); 
fork, 
ifork *ifp)) 
ifork_t	*ifp;		/* inode fork pointer */ 
for extent block */ 
fork of a regular inode is invalid. 
fork == XFS_DATA_FORK)); 
fork) == XFS_DINODE_FMT_LOCAL); 
fork); 
ifp->if_flags & (XFS_IFINLINE|XFS_IFEXTENTS|XFS_IFEXTIREC)) == 
if (*firstblock == NULLFSBLOCK) { 
if (error) 
ifp); 
for the change in fork size and log everything */ 
ifp->if_bytes, whichfork); 
fork); 
ifp, 0, 1); 
fork == XFS_ATTR_FORK ? BMAP_ATTRFORK : 0, 
fork, 1); 
fork); 
fork to handle btree format files. 
fork_btree( 
if (ip->i_df.if_broot_bytes <= XFS_IFORK_DSIZE(ip)) 
if ((error = xfs_bmbt_lookup_ge(cur, 0, 0, 0, &stat))) 
if ((error = xfs_btree_new_iroot(cur, flags, &stat))) 
if (stat == 0) { 
fork to handle extents format files. 
fork_extents( 
if (ip->i_d.di_nextents * sizeof(xfs_bmbt_rec_t) <= XFS_IFORK_DSIZE(ip)) 
if (cur) { 
fork to handle local format files. Each 
for the data formating, others (directories) are so specialised they 
formatting callout. It should be possible - it's just a very complex 
fork_local( 
for dir/attr code */ 
if (S_ISDIR(ip->i_d.di_mode)) { 
fork = XFS_DATA_FORK; 
if (S_ISLNK(ip->i_d.di_mode)) 
for types that support local format data */ 
fork( 
if (rsvd) 
fork, blks, 0); 
if (error) 
if (XFS_IFORK_Q(ip)) 
if (ip->i_d.di_aformat != XFS_DINODE_FMT_EXTENTS) { 
format == 0); 
format) { 
forkoff = roundup(sizeof(xfs_dev_t), 8) >> 3; 
forkoff = roundup(sizeof(uuid_t), 8) >> 3; 
forkoff = xfs_attr_shortform_bytesfit(ip, size); 
forkoff = xfs_default_attroffset(ip) >> 3; 
ifork_zone, KM_SLEEP); 
format) { 
fork_local(tp, ip, &firstblock, &flist, 
fork_extents(tp, ip, &firstblock, 
fork_btree(tp, ip, &firstblock, &flist, 
if (logflags) 
if (error) 
if (!xfs_sb_version_hasattr(&mp->m_sb) || 
if (!xfs_sb_version_hasattr(&mp->m_sb)) { 
if (!xfs_sb_version_hasattr2(&mp->m_sb) && version == 2) { 
if (sbfields) { 
if (error) 
if_extents. 
for no "state" flags. 
fork) /* data or attr fork */ 
for "block" */ 
if checking */ 
ifork_t		*ifp;	/* fork structure */ 
for checking */ 
for */ 
ifp = XFS_IFORK_PTR(ip, whichfork); 
fork != XFS_DATA_FORK) ? XFS_EXTFMT_NOSTATE : 
ifp->if_broot; 
ifp->if_broot_bytes); 
while (level-- > 0) { 
if (error) 
if (level == 0) 
ifp->if_bytes / (uint)sizeof(xfs_bmbt_rec_t); 
formation to the extent records. 
for (;;) { 
if (unlikely(i + num_recs > room)) { 
if any. 
if (nextbno != NULLFSBLOCK) 
for (j = 0; j < num_recs; j++, i++, frp++) { 
if (exntf == XFS_EXTFMT_NOSTATE) { 
for a 
if (unlikely(xfs_check_nostate_extents(ifp, 
if (bno == NULLFSBLOCK) 
if (error) 
ifp->if_bytes / (uint)sizeof(xfs_bmbt_rec_t))); 
fork)); 
for the entry containing block bno. 
if none).  Else, *lastxp will be set to the index 
ifork_t	*ifp,		/* inode fork pointer */ 
for */ 
if XFS_BIG_BLKNOS 
if 
ifp, bno, &lastx); 
ifp, lastx - 1), prevp); 
if (lastx > 0) { 
for the inode, for the extent containing bno. 
if none). 
for */ 
ifork_t	*ifp;		/* inode fork pointer */ 
ifp = XFS_IFORK_PTR(ip, fork); 
if (unlikely(!(gotp->br_startblock) && (*lastxp != NULLEXTNUM) && 
fork == XFS_DATA_FORK))) { 
if the file has holes, else the first block 
if the file is currently local (in-inode). 
fork)		/* data or attr fork */ 
ifork_t	*ifp;			/* inode fork pointer */ 
for this block */ 
fork) == XFS_DINODE_FMT_BTREE || 
fork) == XFS_DINODE_FMT_LOCAL); 
ifp = XFS_IFORK_PTR(ip, whichfork); 
fork))) 
ifp->if_bytes / (uint)sizeof(xfs_bmbt_rec_t); 
for (idx = 0, lastaddr = 0, max = lowest; idx < nextents; idx++) { 
if the hole before this extent will work. 
if (off >= lowest + len && off - max >= len) { 
fore 
for local files, as they do not have extent records. 
fore( 
fork)		/* data or attr fork */ 
ifork_t	*ifp;			/* inode fork pointer */ 
if (XFS_IFORK_FORMAT(ip, whichfork) != XFS_DINODE_FMT_BTREE && 
fork) != XFS_DINODE_FMT_EXTENTS && 
if (XFS_IFORK_FORMAT(ip, whichfork) == XFS_DINODE_FMT_LOCAL) { 
ifp = XFS_IFORK_PTR(ip, whichfork); 
fork))) 
fork, &eof, &lastx, &got, 
if (eof || xfs_bmbt_get_startoff(ep) > bno) { 
fork, 
ifork	*ifp = XFS_IFORK_PTR(ip, whichfork); 
if (!(ifp->if_flags & XFS_IFEXTENTS)) { 
fork); 
ifp->if_bytes / sizeof(xfs_bmbt_rec_t); 
ifp, nextents - 1), rec); 
if the file (fork) is empty as any new write will be 
fork) 
fork, &rec, 
if (error) 
if (is_empty) { 
if we are allocation or past the last extent, or at least into 
for local files, as they do not have extent records. 
fork) 
if (XFS_IFORK_FORMAT(ip, whichfork) == XFS_DINODE_FMT_LOCAL) 
if (XFS_IFORK_FORMAT(ip, whichfork) != XFS_DINODE_FMT_BTREE && 
fork) != XFS_DINODE_FMT_EXTENTS) 
fork, &rec, &is_empty); 
fork of the inode has exactly one 
fork)	/* data or attr fork */ 
ifork_t	*ifp;		/* inode fork pointer */ 
ifndef DEBUG 
fork