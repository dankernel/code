list print :      (nil) (  0x6fa040       INIT)   0x6fa060 
list print :   0x6fa040 (  0x6fa060         if)   0x6fa080 
list print :   0x6fa060 (  0x6fa080        for)   0x6fa0a0 
list print :   0x6fa080 (  0x6fa0a0      while)      (nil) 
[ OK ] open : 3 ok... 
 *  1996-12-23  Modified by Dave Grothe to fix bugs in semaphores and
#include <linux/notifier.h>
#ifdef CONFIG_PARAVIRT
#endif
		if (hrtimer_active(period_timer))
	if (rq->skip_clock_update > 0)
#ifdef CONFIG_SCHED_DEBUG
		if (!(sysctl_sched_features & (1UL << i)))
#ifdef HAVE_JUMP_LABEL
	if (static_key_enabled(&sched_feat_keys[i]))
	if (!static_key_enabled(&sched_feat_keys[i]))
#endif /* HAVE_JUMP_LABEL */
	if (strncmp(cmp, "NO_", 3) == 0) {
		if (strcmp(cmp, sched_feat_names[i]) == 0) {
			if (neg) {
	if (cnt > 63)
	if (copy_from_user(&buf, ubuf, cnt))
	if (i == __SCHED_FEAT_NR)
#endif /* CONFIG_SCHED_DEBUG */
		if (likely(rq == task_rq(p)))
		if (likely(rq == task_rq(p)))
#ifdef CONFIG_SCHED_HRTICK
	if (hrtimer_active(&rq->hrtick_timer))
#ifdef CONFIG_SMP
	if (rq == this_rq()) {
	} else if (!rq->hrtick_csd_pending) {
hotplug_hrtick(struct notifier_block *nfb, unsigned long action, void *hcpu)
	hotcpu_notifier(hotplug_hrtick, 0);
#endif /* CONFIG_SMP */
#ifdef CONFIG_SMP
#endif
#endif	/* CONFIG_SCHED_HRTICK */
	if (test_tsk_need_resched(p))
	if (cpu == smp_processor_id()) {
	if (!tsk_is_polling(p))
	if (!raw_spin_trylock_irqsave(&rq->lock, flags))
#ifdef CONFIG_SMP
#ifdef CONFIG_NO_HZ_COMMON
 * (as that cpu's timer base may not be uptodate wrt jiffies etc).
			if (!idle_cpu(i)) {
	if (cpu == smp_processor_id())
	if (rq->curr != rq->idle)
	if (!tsk_is_polling(rq->idle))
	if (tick_nohz_full_cpu(cpu)) {
		if (cpu != smp_processor_id() ||
	if (!wake_up_full_nohz_cpu(cpu))
	if (!test_bit(NOHZ_BALANCE_KICK, nohz_flags(cpu)))
	if (idle_cpu(cpu) && !need_resched())
#endif /* CONFIG_NO_HZ_COMMON */
#ifdef CONFIG_NO_HZ_FULL
       if (rq->nr_running > 1)
#endif /* CONFIG_NO_HZ_FULL */
#endif /* CONFIG_SMP */
#if defined(CONFIG_RT_GROUP_SCHED) || (defined(CONFIG_FAIR_GROUP_SCHED) && \
	if (ret)
	if (ret || parent == from)
	if (parent)
#endif
	if (p->policy == SCHED_IDLE) {
	if (task_contributes_to_load(p))
	if (task_contributes_to_load(p))
#if defined(CONFIG_IRQ_TIME_ACCOUNTING) || defined(CONFIG_PARAVIRT_TIME_ACCOUNTING)
#endif
#ifdef CONFIG_IRQ_TIME_ACCOUNTING
	if (irq_delta > delta)
#endif
#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING
	if (static_key_false((&paravirt_steal_rq_enabled))) {
		if (unlikely(steal > delta))
#endif
#if defined(CONFIG_IRQ_TIME_ACCOUNTING) || defined(CONFIG_PARAVIRT_TIME_ACCOUNTING)
	if ((irq_delta + steal) && sched_feat(NONTASK_POWER))
#endif
	if (stop) {
	if (old_stop) {
 * boosted by interactivity modifiers. Changes upon fork,
	if (task_has_dl_policy(p))
	else if (task_has_rt_policy(p))
 * interactivity modifiers. Will be RT if the task got
	if (!rt_prio(p->prio))
 * Return: 1 if the task is currently executing. 0 otherwise.
	if (prev_class != p->sched_class) {
		if (prev_class->switched_from)
	} else if (oldprio != p->prio || dl_task(p))
	if (p->sched_class == rq->curr->sched_class) {
			if (class == rq->curr->sched_class)
			if (class == p->sched_class) {
	if (rq->curr->on_rq && test_tsk_need_resched(rq->curr))
#ifdef CONFIG_SMP
#ifdef CONFIG_SCHED_DEBUG
#ifdef CONFIG_LOCKDEP
#endif
#endif
	if (task_cpu(p) != new_cpu) {
		if (p->sched_class->migrate_task_rq)
	if (p->on_rq) {
	if (task_cpu(arg->dst_task) != arg->dst_cpu)
	if (task_cpu(arg->src_task) != arg->src_cpu)
	if (!cpumask_test_cpu(arg->dst_cpu, tsk_cpus_allowed(arg->src_task)))
	if (!cpumask_test_cpu(arg->src_cpu, tsk_cpus_allowed(arg->dst_task)))
	if (arg.src_cpu == arg.dst_cpu)
	if (!cpu_active(arg.src_cpu) || !cpu_active(arg.dst_cpu))
	if (!cpumask_test_cpu(arg.dst_cpu, tsk_cpus_allowed(arg.src_task)))
	if (!cpumask_test_cpu(arg.src_cpu, tsk_cpus_allowed(arg.dst_task)))
 * smp_call_function() if an IPI is sent by the same process we are
		 * return false if the runqueue has changed and p
			if (match_state && unlikely(p->state != match_state))
		if (!match_state || p->state == match_state)
		if (unlikely(!ncsw))
		if (unlikely(running)) {
		 * So if it was still runnable (but just not actively
		if (unlikely(on_rq)) {
	if ((cpu != smp_processor_id()) && task_curr(p))
#endif /* CONFIG_SMP */
#ifdef CONFIG_SMP
	if (nid != -1) {
			if (!cpu_online(dest_cpu))
			if (!cpu_active(dest_cpu))
			if (cpumask_test_cpu(dest_cpu, tsk_cpus_allowed(p)))
			if (!cpu_online(dest_cpu))
			if (!cpu_active(dest_cpu))
	if (state != cpuset) {
		if (p->mm && printk_ratelimit()) {
	if (unlikely(!cpumask_test_cpu(cpu, tsk_cpus_allowed(p)) ||
	s64 diff = sample - *avg;
	*avg += diff >> 3;
#endif
#ifdef CONFIG_SCHEDSTATS
#ifdef CONFIG_SMP
	if (cpu == this_cpu) {
			if (cpumask_test_cpu(cpu, sched_domain_span(sd))) {
	if (wake_flags & WF_MIGRATED)
#endif /* CONFIG_SMP */
	if (wake_flags & WF_SYNC)
#endif /* CONFIG_SCHEDSTATS */
	/* if a worker is waking up, notify workqueue */
	if (p->flags & PF_WQ_WORKER)
#ifdef CONFIG_SMP
	if (p->sched_class->task_woken)
	if (rq->idle_stamp) {
		if (rq->avg_idle > max)
#endif
#ifdef CONFIG_SMP
	if (p->sched_contributes_to_load)
#endif
	if (p->on_rq) {
#ifdef CONFIG_SMP
	if (llist_empty(&this_rq()->wake_list)
	 * Check if someone kicked us for doing the nohz idle load balance.
	if (unlikely(got_nohz_idle_kick())) {
	if (llist_add(&p->wake_entry, &cpu_rq(cpu)->wake_list))
#endif /* CONFIG_SMP */
#if defined(CONFIG_SMP)
	if (sched_feat(TTWU_QUEUE) && !cpus_share_cache(smp_processor_id(), cpu)) {
#endif
 * @wake_flags: wake modifier flags (WF_*)
 * Put it on the run-queue if it's not already there. The "current"
 * Return: %true if @p was woken up, %false if it was already running.
	if (!(p->state & state))
	if (p->on_rq && ttwu_remote(p, wake_flags))
#ifdef CONFIG_SMP
	if (p->sched_class->task_waking)
	if (task_cpu(p) != cpu) {
#endif /* CONFIG_SMP */
 * Put @p on the run-queue if it's not already there. The caller must
	if (WARN_ON_ONCE(rq != this_rq()) ||
	if (!raw_spin_trylock(&p->pi_lock)) {
	if (!(p->state & TASK_NORMAL))
	if (!p->on_rq)
 * wake_up_process - Wake up a specific process
 * Return: 1 if the process was woken up, 0 if it was already running.
 * changing the task state if and only if any tasks are woken up.
#ifdef CONFIG_SCHEDSTATS
#endif
#ifdef CONFIG_PREEMPT_NOTIFIERS
	INIT_HLIST_HEAD(&p->preempt_notifiers);
#endif
#ifdef CONFIG_NUMA_BALANCING
	if (p->mm && atomic_read(&p->mm->mm_users) == 1) {
		p->mm->numa_next_scan = jiffies + msecs_to_jiffies(sysctl_numa_balancing_scan_delay);
	if (clone_flags & CLONE_VM)
#endif /* CONFIG_NUMA_BALANCING */
#ifdef CONFIG_NUMA_BALANCING
#ifdef CONFIG_SCHED_DEBUG
	if (enabled)
#endif /* CONFIG_SCHED_DEBUG */
#ifdef CONFIG_PROC_SYSCTL
	if (write && !capable(CAP_SYS_ADMIN))
	if (err < 0)
	if (write)
#endif
#endif
	 * Revert to default priority/policy on fork if requested.
	if (unlikely(p->sched_reset_on_fork)) {
		if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
		} else if (PRIO_TO_NICE(p->static_prio) < 0)
	if (dl_prio(p->prio)) {
	} else if (rt_prio(p->prio)) {
	if (p->sched_class->task_fork)
#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
	if (likely(sched_info_on()))
#endif
#if defined(CONFIG_SMP)
#endif
#ifdef CONFIG_SMP
#endif
	if (runtime == RUNTIME_INF)
	if (period == 0)
#ifdef CONFIG_SMP
#endif
	if (new_bw == p->dl.dl_bw)
	 * Either if a task, enters, leave, or stays -deadline but changes
	if (dl_policy(policy) && !task_has_dl_policy(p) &&
	} else if (dl_policy(policy) && task_has_dl_policy(p) &&
	} else if (!dl_policy(policy) && task_has_dl_policy(p)) {
#ifdef CONFIG_SMP
#endif
#ifdef CONFIG_SMP
	if (p->sched_class->task_woken)
#endif
#ifdef CONFIG_PREEMPT_NOTIFIERS
 * preempt_notifier_register - tell me when current is being preempted & rescheduled
 * @notifier: notifier struct to register
void preempt_notifier_register(struct preempt_notifier *notifier)
	hlist_add_head(&notifier->link, &current->preempt_notifiers);
EXPORT_SYMBOL_GPL(preempt_notifier_register);
 * preempt_notifier_unregister - no longer interested in preemption notifications
 * @notifier: notifier struct to unregister
 * This is safe to call from within a preemption notifier.
void preempt_notifier_unregister(struct preempt_notifier *notifier)
	hlist_del(&notifier->link);
EXPORT_SYMBOL_GPL(preempt_notifier_unregister);
static void fire_sched_in_preempt_notifiers(struct task_struct *curr)
	struct preempt_notifier *notifier;
	hlist_for_each_entry(notifier, &curr->preempt_notifiers, link)
		notifier->ops->sched_in(notifier, raw_smp_processor_id());
fire_sched_out_preempt_notifiers(struct task_struct *curr,
	struct preempt_notifier *notifier;
	hlist_for_each_entry(notifier, &curr->preempt_notifiers, link)
		notifier->ops->sched_out(notifier, next);
static void fire_sched_in_preempt_notifiers(struct task_struct *curr)
fire_sched_out_preempt_notifiers(struct task_struct *curr,
#endif /* CONFIG_PREEMPT_NOTIFIERS */
 * prepare_task_switch sets up locking and calls architecture specific
	fire_sched_out_preempt_notifiers(prev, next);
 * and do any other architecture-specific cleanup actions.
	 *		Manfred Spraul <manfred@colorfullife.com>
	fire_sched_in_preempt_notifiers(current);
	if (mm)
	if (unlikely(prev_state == TASK_DEAD)) {
		if (prev->sched_class->task_dead)
#ifdef CONFIG_SMP
	if (prev->sched_class->pre_schedule)
	if (rq->post_schedule) {
		if (rq->curr->sched_class->post_schedule)
#endif
#ifdef __ARCH_WANT_UNLOCKED_CTXSW
#endif
	if (current->set_child_tid)
	if (!mm) {
	if (!prev->mm) {
#ifndef __ARCH_WANT_UNLOCKED_CTXSW
#endif
#ifdef CONFIG_SMP
	if (dest_cpu == smp_processor_id())
	if (likely(cpu_active(dest_cpu))) {
#endif
	if (task_current(rq, p)) {
		if ((s64)ns < 0)
#if defined(CONFIG_64BIT) && defined(CONFIG_SMP)
	if (!p->on_cpu)
#endif
#ifdef CONFIG_SMP
#endif
#ifdef CONFIG_NO_HZ_FULL
	unsigned long next, now = ACCESS_ONCE(jiffies);
	if (time_before_eq(next, now))
	return jiffies_to_nsecs(next - now);
#endif
	if (in_lock_functions(addr)) {
		if (in_lock_functions(addr))
#if defined(CONFIG_PREEMPT) && (defined(CONFIG_DEBUG_PREEMPT) || \
#ifdef CONFIG_DEBUG_PREEMPT
	if (DEBUG_LOCKS_WARN_ON((preempt_count() < 0)))
#endif
#ifdef CONFIG_DEBUG_PREEMPT
#endif
	if (preempt_count() == val)
#ifdef CONFIG_DEBUG_PREEMPT
	if (DEBUG_LOCKS_WARN_ON(val > preempt_count()))
	if (DEBUG_LOCKS_WARN_ON((val < PREEMPT_MASK) &&
#endif
	if (preempt_count() == val)
#endif
	if (oops_in_progress)
	if (irqs_disabled())
	 * Test if we are atomic. Since do_exit() needs to call into
	 * if we are scheduling when we should not.
	if (unlikely(in_atomic_preempt_off() && prev->state != TASK_DEAD))
	if (prev->on_rq || rq->skip_clock_update < 0)
	 * Optimization: we know that if all tasks are in
	if (likely(rq->nr_running == rq->cfs.h_nr_running)) {
		if (likely(p))
		if (p)
 *      Now, if the new task added to the run-queue preempts the current
	if (sched_feat(HRTICK))
	if (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {
		if (unlikely(signal_pending_state(prev->state, prev))) {
			 * If a worker went to sleep, notify and ask workqueue
			if (prev->flags & PF_WQ_WORKER) {
				if (to_wakeup)
	if (unlikely(!rq->nr_running))
	if (likely(prev != next)) {
	if (need_resched())
	if (!tsk->state || tsk_is_pi_blocked(tsk))
	if (blk_needs_flush_plug(tsk))
#ifdef CONFIG_CONTEXT_TRACKING
#endif
#ifdef CONFIG_PREEMPT
	if (likely(!preemptible()))
#endif /* CONFIG_PREEMPT */
#ifdef CONFIG_RT_MUTEXES
	if (unlikely(p == rq->idle)) {
	if (on_rq)
	if (running)
	if (dl_prio(prio)) {
		if (!dl_prio(p->normal_prio) || (p->pi_top_task &&
	} else if (rt_prio(prio)) {
		if (dl_prio(oldprio))
		if (oldprio < prio)
		if (dl_prio(oldprio))
	if (running)
	if (on_rq)
#endif
	if (TASK_NICE(p) == nice || nice < -20 || nice > 19)
	 * We have to be careful, if called from sys_setpriority(),
	if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
	if (on_rq)
	if (on_rq) {
		if (delta < 0 || (delta > 0 && task_running(rq, p)))
 * can_nice - check if a task can reduce its nice value
#ifdef __ARCH_WANT_SYS_NICE
	if (increment < -40)
	if (increment > 40)
	if (nice < -20)
	if (nice > 19)
	if (increment < 0 && !can_nice(current, nice))
	if (retval)
#endif
 * Return: 1 if the CPU is currently idle. 0 otherwise.
	if (rq->curr != rq->idle)
	if (rq->nr_running)
#ifdef CONFIG_SMP
	if (!llist_empty(&rq->wake_list))
#endif
 * The task of @pid, if found. %NULL otherwise.
	if (policy == -1) /* setparam */
	if (dl_policy(policy))
	else if (fair_policy(policy))
	if (dl_prio(p->prio))
	else if (rt_prio(p->prio))
	if (attr->sched_deadline == 0)
	if (attr->sched_runtime < (1ULL << DL_SCALE))
	if (attr->sched_deadline & (1ULL << 63) ||
	/* runtime <= deadline <= period (if period != 0) */
	if ((attr->sched_period != 0 &&
	if (policy < 0) {
		if (policy != SCHED_DEADLINE &&
	if (attr->sched_flags & ~(SCHED_FLAG_RESET_ON_FORK))
	if ((p->mm && attr->sched_priority > MAX_USER_RT_PRIO-1) ||
	if ((dl_policy(policy) && !__checkparam_dl(attr)) ||
	if (user && !capable(CAP_SYS_NICE)) {
		if (fair_policy(policy)) {
			if (attr->sched_nice < TASK_NICE(p) &&
		if (rt_policy(policy)) {
			if (policy != p->policy && !rlim_rtprio)
			if (attr->sched_priority > p->rt_priority &&
		if (dl_policy(policy))
		 * SCHED_NORMAL if the RLIMIT_NICE would normally permit it.
		if (p->policy == SCHED_IDLE && policy != SCHED_IDLE) {
			if (!can_nice(p, TASK_NICE(p)))
		if (!check_same_owner(p))
		if (p->sched_reset_on_fork && !reset_on_fork)
	if (user) {
		if (retval)
	if (p == rq->stop) {
	if (unlikely(policy == p->policy)) {
		if (fair_policy(policy) && attr->sched_nice != TASK_NICE(p))
		if (rt_policy(policy) && attr->sched_priority != p->rt_priority)
		if (dl_policy(policy))
	if (user) {
#ifdef CONFIG_RT_GROUP_SCHED
		if (rt_bandwidth_enabled() && rt_policy(policy) &&
#endif
#ifdef CONFIG_SMP
		if (dl_bandwidth_enabled() && dl_policy(policy)) {
			 * will also fail if there's no bandwidth available.
			if (!cpumask_subset(span, &p->cpus_allowed) ||
#endif
	if (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) {
	 * of a SCHED_DEADLINE task) we need to check if enough bandwidth
	if ((dl_policy(policy) || dl_task(p)) && dl_overflow(p, policy, attr)) {
	if (on_rq)
	if (running)
	if (running)
	if (on_rq)
	if (policy & SCHED_RESET_ON_FORK) {
 * Just like sched_setscheduler, only don't bother checking if the
	if (!param || pid < 0)
	if (copy_from_user(&lparam, param, sizeof(struct sched_param)))
	if (p != NULL)
	if (!access_ok(VERIFY_WRITE, uattr, SCHED_ATTR_SIZE_VER0))
	if (ret)
	if (size > PAGE_SIZE)	/* silly large */
	if (!size)		/* abi compat */
	if (size < SCHED_ATTR_SIZE_VER0)
	if (size > sizeof(*attr)) {
			if (ret)
			if (val)
	if (ret)
	if (policy < 0)
	if (!uattr || pid < 0 || flags)
	if (retval)
	if ((int)attr.sched_policy < 0)
	if (p != NULL)
	if (pid < 0)
	if (p) {
		if (!retval)
	if (!param || pid < 0)
	if (!p)
	if (retval)
	if (task_has_rt_policy(p))
	if (!access_ok(VERIFY_WRITE, uattr, usize))
	if (usize < sizeof(*attr)) {
			if (*addr)
	if (ret)
	if (!uattr || pid < 0 || size > PAGE_SIZE ||
	if (!p)
	if (retval)
	if (p->sched_reset_on_fork)
	if (task_has_dl_policy(p))
	else if (task_has_rt_policy(p))
	if (!p) {
	if (p->flags & PF_NO_SETAFFINITY) {
	if (!alloc_cpumask_var(&cpus_allowed, GFP_KERNEL)) {
	if (!alloc_cpumask_var(&new_mask, GFP_KERNEL)) {
	if (!check_same_owner(p)) {
		if (!ns_capable(__task_cred(p)->user_ns, CAP_SYS_NICE)) {
	if (retval)
	 * if admission test is enabled, we only admit -deadline
#ifdef CONFIG_SMP
	if (task_has_dl_policy(p)) {
		if (dl_bandwidth_enabled() && !cpumask_subset(span, new_mask)) {
#endif
	if (!retval) {
		if (!cpumask_subset(new_mask, cpus_allowed)) {
	if (len < cpumask_size())
	else if (len > cpumask_size())
	if (!alloc_cpumask_var(&new_mask, GFP_KERNEL))
	if (retval == 0)
	if (!p)
	if (retval)
	if ((len * BITS_PER_BYTE) < nr_cpu_ids)
	if (len & (sizeof(unsigned long)-1))
	if (!alloc_cpumask_var(&mask, GFP_KERNEL))
	if (ret == 0) {
		if (copy_to_user(user_mask_ptr, mask, retlen))
	if (should_resched()) {
 * __cond_resched_lock() - if a reschedule is pending, drop the given lock,
	if (spin_needbreak(lock) || resched) {
		if (resched)
	if (should_resched()) {
 * eligible task to run, if removing the yield() call from your code breaks
 *	true (>0) if we indeed boosted the target task.
 *	false (0) if we failed to boost the target.
 *	-ESRCH if there's no task to yield to.
	if (rq->nr_running == 1 && p_rq->nr_running == 1) {
	if (task_rq(p) != p_rq) {
	if (!curr->sched_class->yield_to_task)
	if (curr->sched_class != p->sched_class)
	if (task_running(p_rq, p) || p->state)
	if (yielded) {
		if (preempt && rq != p_rq)
	if (yielded > 0)
	if (pid < 0)
	if (!p)
	if (retval)
	if (p->sched_class->get_rr_interval)
	jiffies_to_timespec(time_slice, &t);
#if BITS_PER_LONG == 32
	if (state == TASK_RUNNING)
	if (state == TASK_RUNNING)
#endif
#ifdef CONFIG_DEBUG_STACK_USAGE
#endif
#if BITS_PER_LONG == 32
#endif
		if (!state_filter || (p->state & state_filter))
#ifdef CONFIG_SCHED_DEBUG
#endif
	 * Only show locks if all tasks are dumped:
	if (!state_filter)
#if defined(CONFIG_SMP)
#endif
#if defined(CONFIG_SMP)
#endif
#ifdef CONFIG_SMP
	if (p->sched_class && p->sched_class->set_cpus_allowed)
 * 4) if it's in the wrong runqueue then the migration thread removes
 * proper CPU and schedule it away if the CPU it's executing on
	if (cpumask_equal(&p->cpus_allowed, new_mask))
	if (!cpumask_intersects(new_mask, cpu_active_mask)) {
	if (cpumask_test_cpu(task_cpu(p), new_mask))
	if (p->on_rq) {
 * Returns non-zero if task was successfully migrated.
	if (unlikely(!cpu_active(dest_cpu)))
	if (task_cpu(p) != src_cpu)
	if (!cpumask_test_cpu(dest_cpu, tsk_cpus_allowed(p)))
	if (p->on_rq) {
#ifdef CONFIG_NUMA_BALANCING
	if (curr_cpu == target_cpu)
	if (!cpumask_test_cpu(target_cpu, tsk_cpus_allowed(p)))
	if (on_rq)
	if (running)
	if (running)
	if (on_rq)
#endif
#ifdef CONFIG_HOTPLUG_CPU
	if (mm != &init_mm)
	if (delta)
		if (rq->nr_running == 1)
		/* Find suitable destination for @next, with force if needed. */
#endif /* CONFIG_HOTPLUG_CPU */
#if defined(CONFIG_SCHED_DEBUG) && defined(CONFIG_SYSCTL)
		if (entry->child)
		if (entry->proc_handler == NULL)
	if (load_idx) {
	if (table == NULL)
	if (table == NULL)
	if (entry == NULL)
	if (sd_sysctl_header)
	if (sd_ctl_dir[0].child)
#endif
	if (!rq->online) {
			if (class->rq_online)
	if (rq->online) {
			if (class->rq_offline)
migration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)
		if (rq->rd) {
#ifdef CONFIG_HOTPLUG_CPU
		if (rq->rd) {
#endif
 * the notifier in the perf_event subsystem, though.
static struct notifier_block migration_notifier = {
	.notifier_call = migration_call,
static int sched_cpu_active(struct notifier_block *nfb,
static int sched_cpu_inactive(struct notifier_block *nfb,
		if (!(action & CPU_TASKS_FROZEN)) {
			if (overflow)
				return notifier_from_errno(-EBUSY);
	err = migration_call(&migration_notifier, CPU_UP_PREPARE, cpu);
	migration_call(&migration_notifier, CPU_ONLINE, cpu);
	register_cpu_notifier(&migration_notifier);
	/* Register cpu active notifiers */
	cpu_notifier(sched_cpu_active, CPU_PRI_SCHED_ACTIVE);
	cpu_notifier(sched_cpu_inactive, CPU_PRI_SCHED_INACTIVE);
#endif
#ifdef CONFIG_SMP
#ifdef CONFIG_SCHED_DEBUG
	if (!(sd->flags & SD_LOAD_BALANCE)) {
		if (sd->parent)
	if (!cpumask_test_cpu(cpu, sched_domain_span(sd))) {
	if (!cpumask_test_cpu(cpu, sched_group_cpus(group))) {
		if (!group) {
		 * we leave power_orig unset. This allows us to detect if
		if (!group->sgp->power_orig) {
		if (!cpumask_weight(sched_group_cpus(group))) {
		if (!(sd->flags & SD_OVERLAP) &&
		if (group->sgp->power != SCHED_POWER_SCALE) {
	if (!cpumask_equal(sched_domain_span(sd), groupmask))
	if (sd->parent &&
	if (!sched_debug_enabled)
	if (!sd) {
		if (sched_domain_debug_one(sd, cpu, level, sched_domains_tmpmask))
		if (!sd)
#endif /* CONFIG_SCHED_DEBUG */
	if (cpumask_weight(sched_domain_span(sd)) == 1)
	if (sd->flags & (SD_LOAD_BALANCE |
		if (sd->groups != sd->groups->next)
	if (sd->flags & (SD_WAKE_AFFINE))
	if (sd_degenerate(parent))
	if (!cpumask_equal(sched_domain_span(sd), sched_domain_span(parent)))
	/* Flags needing groups don't count if only 1 group in parent */
	if (parent->groups == parent->groups->next) {
		if (nr_node_ids == 1)
	if (~cflags & pflags)
	if (rq->rd) {
		if (cpumask_test_cpu(rq->cpu, old_rd->online))
		if (!atomic_dec_and_test(&old_rd->refcount))
	if (cpumask_test_cpu(rq->cpu, cpu_active_mask))
	if (old_rd)
	if (!alloc_cpumask_var(&rd->span, GFP_KERNEL))
	if (!alloc_cpumask_var(&rd->online, GFP_KERNEL))
	if (!alloc_cpumask_var(&rd->dlo_mask, GFP_KERNEL))
	if (!alloc_cpumask_var(&rd->rto_mask, GFP_KERNEL))
	if (cpudl_init(&rd->cpudl) != 0)
	if (cpupri_init(&rd->cpupri) != 0)
	if (!rd)
	if (init_rootdomain(rd) != 0) {
	if (!sg)
		if (free_sgp && atomic_dec_and_test(&sg->sgp->ref))
	if (sd->flags & SD_OVERLAP) {
	} else if (atomic_dec_and_test(&sd->groups->ref)) {
 * the cpumask of the domain), this allows us to quickly tell if
	if (sd) {
		if (!parent)
		if (sd_parent_degenerate(tmp, parent)) {
			if (parent->parent)
			if (parent->flags & SD_PREFER_SIBLING)
	if (sd && sd_degenerate(sd)) {
		if (sd)
		if (!cpumask_test_cpu(i, sched_domain_span(sibling)))
		if (cpumask_test_cpu(i, covered))
		if (!cpumask_test_cpu(i, sched_domain_span(child)))
		if (!sg)
		if (child->child) {
		if (atomic_inc_return(&sg->sgp->ref) == 1)
		 * Initialize sgp->power such that even if we mess up the
		if ((!groups && cpumask_test_cpu(cpu, sg_span)) ||
		if (!first)
		if (last)
	if (child)
	if (sg) {
	if (cpu != cpumask_first(span))
		if (cpumask_test_cpu(i, covered))
			if (get_group(j, sdd, NULL) != group)
		if (!first)
		if (last)
 * distributing the load between different sched groups in a sched domain.
	if (cpu != group_balance_cpu(sg))
#ifdef CONFIG_SCHED_DEBUG
#endif
#ifdef CONFIG_SCHED_SMT
#endif
#ifdef CONFIG_SCHED_MC
#endif
#ifdef CONFIG_SCHED_BOOK
#endif
	if (kstrtoint(str, 0, &default_relax_domain_level))
	if (!attr || attr->relax_domain_level < 0) {
		if (default_relax_domain_level < 0)
	if (request < sd->level) {
		if (!atomic_read(&d->rd->refcount))
	if (__sdt_alloc(cpu_map))
	if (!d->sd)
	if (!d->rd)
	if (atomic_read(&(*per_cpu_ptr(sdd->sg, cpu))->ref))
	if (atomic_read(&(*per_cpu_ptr(sdd->sgp, cpu))->ref))
#ifdef CONFIG_SCHED_SMT
#endif
#ifdef CONFIG_SCHED_SMT
#endif
#ifdef CONFIG_SCHED_MC
#endif
#ifdef CONFIG_SCHED_BOOK
#endif
#ifdef CONFIG_NUMA
	if (sched_domains_numa_distance[level] > RECLAIM_DISTANCE)
		.last_balance		= jiffies,
	if (done)
	if (distance == node_distance(0, 0))
		if (sched_domains_numa_distance[i] == distance)
	if (!sched_domains_numa_distance)
				if (distance > curr_distance &&
				 * about cases where if node A is connected to B, B is not
				if (sched_debug() && node_distance(k, i) != distance)
				if (sched_debug() && i && !find_numa_distance(distance))
			if (next_distance != curr_distance) {
		 * In case of sched_debug() we verify the above assumption.
		if (!sched_debug())
	if (!sched_domains_numa_masks)
		if (!sched_domains_numa_masks[i])
			if (!mask)
				if (node_distance(j, k) > sched_domains_numa_distance[i])
	if (!tl)
			if (node_distance(j, node) <= sched_domains_numa_distance[i])
static int sched_domains_numa_masks_update(struct notifier_block *nfb,
static int sched_domains_numa_masks_update(struct notifier_block *nfb,
#endif /* CONFIG_NUMA */
		if (!sdd->sd)
		if (!sdd->sg)
		if (!sdd->sgp)
			if (!sd)
			if (!sg)
			if (!sgp)
			if (sdd->sd) {
				if (sd && (sd->flags & SD_OVERLAP))
			if (sdd->sg)
			if (sdd->sgp)
	if (!sd)
	if (child) {
	if (alloc_state != sa_rootdomain)
	/* Set up domains for cpus specified by the cpu_map. */
			if (tl == sched_domain_topology)
			if (tl->flags & SDTL_OVERLAP || sched_feat(FORCE_SD_OVERLAP))
			if (cpumask_equal(cpu_map, sched_domain_span(sd)))
			if (sd->flags & SD_OVERLAP) {
				if (build_overlap_sched_groups(sd, i))
				if (build_sched_groups(sd, i))
		if (!cpumask_test_cpu(i, cpu_map))
 * cpu core maps. It is supposed to return 1 if the topology changed
 * or 0 if it stayed the same.
	if (!doms)
		if (!alloc_cpumask_var(&doms[i], GFP_KERNEL)) {
	if (!doms_cur)
 * Detach sched domains from a group of cpus specified in cpu_map
	if (!new && !cur)
 * Partition sched domains as specified by the 'ndoms_new'
			if (cpumask_equal(doms_cur[i], doms_new[j])
	if (doms_new == NULL) {
			if (cpumask_equal(doms_new[i], doms_cur[j])
	if (doms_cur != &fallback_doms)
static int cpuset_cpu_active(struct notifier_block *nfb, unsigned long action,
		if (likely(num_cpus_frozen)) {
static int cpuset_cpu_inactive(struct notifier_block *nfb, unsigned long action,
	if (cpumask_empty(non_isolated_cpus))
	hotcpu_notifier(sched_domains_numa_masks_update, CPU_PRI_SCHED_ACTIVE);
	hotcpu_notifier(cpuset_cpu_active, CPU_PRI_CPUSET_ACTIVE);
	hotcpu_notifier(cpuset_cpu_inactive, CPU_PRI_CPUSET_INACTIVE);
	if (set_cpus_allowed_ptr(current, non_isolated_cpus) < 0)
#endif /* CONFIG_SMP */
#ifdef CONFIG_CGROUP_SCHED
#endif
#ifdef CONFIG_FAIR_GROUP_SCHED
#endif
#ifdef CONFIG_RT_GROUP_SCHED
#endif
#ifdef CONFIG_CPUMASK_OFFSTACK
#endif
	if (alloc_size) {
#ifdef CONFIG_FAIR_GROUP_SCHED
#endif /* CONFIG_FAIR_GROUP_SCHED */
#ifdef CONFIG_RT_GROUP_SCHED
#endif /* CONFIG_RT_GROUP_SCHED */
#ifdef CONFIG_CPUMASK_OFFSTACK
#endif /* CONFIG_CPUMASK_OFFSTACK */
#ifdef CONFIG_SMP
#endif
#ifdef CONFIG_RT_GROUP_SCHED
#endif /* CONFIG_RT_GROUP_SCHED */
#ifdef CONFIG_CGROUP_SCHED
#endif /* CONFIG_CGROUP_SCHED */
		rq->calc_load_update = jiffies + LOAD_FREQ;
#ifdef CONFIG_FAIR_GROUP_SCHED
		 * In other words, if root_task_group has 10 tasks of weight
#endif /* CONFIG_FAIR_GROUP_SCHED */
#ifdef CONFIG_RT_GROUP_SCHED
#endif
		rq->last_load_update_tick = jiffies;
#ifdef CONFIG_SMP
		rq->next_balance = jiffies;
#ifdef CONFIG_NO_HZ_COMMON
#endif
#ifdef CONFIG_NO_HZ_FULL
#endif
#endif
#ifdef CONFIG_PREEMPT_NOTIFIERS
	INIT_HLIST_HEAD(&init_task.preempt_notifiers);
#endif
	calc_load_update = jiffies + LOAD_FREQ;
#ifdef CONFIG_SMP
	if (cpu_isolated_map == NULL)
#endif
#ifdef CONFIG_DEBUG_ATOMIC_SLEEP
	static unsigned long prev_jiffy;	/* ratelimiting */
	if ((preempt_count_equals(preempt_offset) && !irqs_disabled()) ||
	if (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)
	prev_jiffy = jiffies;
	if (irqs_disabled())
#endif
#ifdef CONFIG_MAGIC_SYSRQ
	if (on_rq)
	if (on_rq) {
		if (!p->mm)
#ifdef CONFIG_SCHEDSTATS
#endif
		if (!dl_task(p) && !rt_task(p)) {
			if (TASK_NICE(p) < 0 && p->mm)
#endif /* CONFIG_MAGIC_SYSRQ */
#if defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB)
#endif /* defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB) */
#ifdef CONFIG_IA64
#endif
#ifdef CONFIG_CGROUP_SCHED
	if (!tg)
	if (!alloc_fair_sched_group(tg, parent))
	if (!alloc_rt_sched_group(tg, parent))
	if (on_rq)
	if (unlikely(running))
#ifdef CONFIG_FAIR_GROUP_SCHED
	if (tsk->sched_class->task_move_group)
#endif
	if (unlikely(running))
	if (on_rq)
#endif /* CONFIG_CGROUP_SCHED */
#ifdef CONFIG_RT_GROUP_SCHED
		if (rt_task(p) && task_rq(p)->rt.tg == tg)
	if (tg == d->tg) {
	if (runtime > period && runtime != RUNTIME_INF)
	if (rt_bandwidth_enabled() && !runtime && tg_has_rt_tasks(tg))
	if (total > to_ratio(global_rt_period(), global_rt_runtime()))
		if (child == d->tg) {
	if (sum > total)
	if (err)
	if (rt_runtime_us < 0)
	if (tg->rt_bandwidth.rt_runtime == RUNTIME_INF)
	if (rt_period == 0)
#endif /* CONFIG_RT_GROUP_SCHED */
#ifdef CONFIG_RT_GROUP_SCHED
	if (rt_task(tsk) && tg->rt_bandwidth.rt_runtime == 0)
#endif /* CONFIG_RT_GROUP_SCHED */
	 * cycling on root_domains... Discussion on different/better
		if (new_bw < dl_b->total_bw)
		if (ret)
	if (global_rt_runtime() != RUNTIME_INF)
	if (sysctl_sched_rt_period <= 0)
	if ((sysctl_sched_rt_runtime != RUNTIME_INF) &&
	if (!ret && write) {
		if (ret)
		if (ret)
		if (ret)
	if (0) {
	/* make sure that internally we keep jiffies */
	if (!ret && write) {
			RR_TIMESLICE : msecs_to_jiffies(sched_rr_timeslice);
#ifdef CONFIG_CGROUP_SCHED
	if (!parent) {
	if (IS_ERR(tg))
	if (parent)
#ifdef CONFIG_RT_GROUP_SCHED
		if (!sched_rt_can_attach(css_tg(css), task))
		if (task->sched_class != &fair_sched_class)
#endif
	if (!(task->flags & PF_EXITING))
#ifdef CONFIG_FAIR_GROUP_SCHED
#ifdef CONFIG_CFS_BANDWIDTH
	if (tg == &root_task_group)
	if (quota < min_cfs_quota_period || period < min_cfs_quota_period)
	if (period > max_cfs_quota_period)
	if (ret)
	if (runtime_enabled && !runtime_was_enabled)
	/* restart the period timer (if active) to handle new period expiry */
	if (runtime_enabled && cfs_b->timer_active) {
		if (cfs_rq->throttled)
	if (runtime_was_enabled && !runtime_enabled)
	if (cfs_quota_us < 0)
	if (tg->cfs_bandwidth.quota == RUNTIME_INF)
	if (tg == d->tg) {
	if (quota == RUNTIME_INF || quota == -1)
	if (!tg->parent) {
		if (quota == RUNTIME_INF)
		else if (parent_quota != RUNTIME_INF && quota > parent_quota)
	if (quota != RUNTIME_INF) {
#endif /* CONFIG_CFS_BANDWIDTH */
#endif /* CONFIG_FAIR_GROUP_SCHED */
#ifdef CONFIG_RT_GROUP_SCHED
#endif /* CONFIG_RT_GROUP_SCHED */
#ifdef CONFIG_FAIR_GROUP_SCHED
#endif
#ifdef CONFIG_CFS_BANDWIDTH
#endif
#ifdef CONFIG_RT_GROUP_SCHED
#endif
#endif	/* CONFIG_CGROUP_SCHED */
