list print :      (nil) ( 0x1126040       INIT)  0x1126060 
list print :  0x1126040 ( 0x1126060         if)  0x1126080 
list print :  0x1126060 ( 0x1126080        for)  0x11260a0 
list print :  0x1126080 ( 0x11260a0      while)      (nil) 
[ OK ] open : 3 ok... 
for (;;) { 
forward(period_timer, now, period); 
if (rq->skip_clock_update > 0) 
for (i = 0; i < __SCHED_FEAT_NR; i++) { 
if (static_key_enabled(&sched_feat_keys[i])) 
if (!static_key_enabled(&sched_feat_keys[i])) 
if (strncmp(cmp, "NO_", 3) == 0) { 
for (i = 0; i < __SCHED_FEAT_NR; i++) { 
if (neg) { 
if (cnt > 63) 
if (copy_from_user(&buf, ubuf, cnt)) 
if (i == __SCHED_FEAT_NR) 
for (;;) { 
if (likely(rq == task_rq(p))) 
for (;;) { 
if (likely(rq == task_rq(p))) 
if (hrtimer_active(&rq->hrtick_timer)) 
if (rq == this_rq()) { 
if (!rq->hrtick_csd_pending) { 
ifier_block *nfb, unsigned long action, void *hcpu) 
ifier(hotplug_hrtick, 0); 
if (test_tsk_need_resched(p)) 
if (cpu == smp_processor_id()) { 
if (!tsk_is_polling(p)) 
if (!raw_spin_trylock_irqsave(&rq->lock, flags)) 
iffies etc). 
for_each_domain(cpu, sd) { 
if (!idle_cpu(i)) { 
if (cpu == smp_processor_id()) 
if (rq->curr != rq->idle) 
if (!tsk_is_polling(rq->idle)) 
if (tick_nohz_full_cpu(cpu)) { 
if (!wake_up_full_nohz_cpu(cpu)) 
if (!test_bit(NOHZ_BALANCE_KICK, nohz_flags(cpu))) 
if (idle_cpu(cpu) && !need_resched()) 
if (rq->nr_running > 1) 
while ((s64)(rq_clock(rq) - rq->age_stamp) > period) { 
for another example of this. 
if (ret) 
for_each_entry_rcu(child, &parent->children, siblings) { 
if (ret || parent == from) 
if (parent) 
if (p->policy == SCHED_IDLE) { 
if (task_contributes_to_load(p)) 
if (task_contributes_to_load(p)) 
if defined(CONFIG_IRQ_TIME_ACCOUNTING) || defined(CONFIG_PARAVIRT_TIME_ACCOUNTING) 
if (irq_delta > delta) 
if (static_key_false((&paravirt_steal_rq_enabled))) { 
if (unlikely(steal > delta)) 
if defined(CONFIG_IRQ_TIME_ACCOUNTING) || defined(CONFIG_PARAVIRT_TIME_ACCOUNTING) 
if (stop) { 
if (old_stop) { 
if (task_has_dl_policy(p)) 
if (task_has_rt_policy(p)) 
if (!rt_prio(p->prio)) 
if (prev_class != p->sched_class) { 
if (oldprio != p->prio || dl_task(p)) 
if (p->sched_class == rq->curr->sched_class) { 
for_each_class(class) { 
if (class == p->sched_class) { 
if (rq->curr->on_rq && test_tsk_need_resched(rq->curr)) 
if (task_cpu(p) != new_cpu) { 
if (p->on_rq) { 
if (task_cpu(arg->dst_task) != arg->dst_cpu) 
if (task_cpu(arg->src_task) != arg->src_cpu) 
if (!cpumask_test_cpu(arg->dst_cpu, tsk_cpus_allowed(arg->src_task))) 
if (!cpumask_test_cpu(arg->src_cpu, tsk_cpus_allowed(arg->dst_task))) 
if (arg.src_cpu == arg.dst_cpu) 
if (!cpu_active(arg.src_cpu) || !cpu_active(arg.dst_cpu)) 
if (!cpumask_test_cpu(arg.dst_cpu, tsk_cpus_allowed(arg.src_task))) 
if (!cpumask_test_cpu(arg.src_cpu, tsk_cpus_allowed(arg.dst_task))) 
if an IPI is sent by the same process we are 
for (;;) { 
while (task_running(rq, p)) { 
if (!match_state || p->state == match_state) 
if (unlikely(!ncsw)) 
if (unlikely(running)) { 
if it was still runnable (but just not actively 
if (unlikely(on_rq)) { 
if ((cpu != smp_processor_id()) && task_curr(p)) 
if (nid != -1) { 
if (!cpu_online(dest_cpu)) 
if (!cpu_active(dest_cpu)) 
if (cpumask_test_cpu(dest_cpu, tsk_cpus_allowed(p))) 
for (;;) { 
for_each_cpu(dest_cpu, tsk_cpus_allowed(p)) { 
if (!cpu_active(dest_cpu)) 
if (state != cpuset) { 
if (p->mm && printk_ratelimit()) { 
fork, wakeup) owns p->pi_lock, ->cpus_allowed is stable. 
if (unlikely(!cpumask_test_cpu(cpu, tsk_cpus_allowed(p)) || 
if (cpu == this_cpu) { 
for_each_domain(this_cpu, sd) { 
if (wake_flags & WF_MIGRATED) 
if (wake_flags & WF_SYNC) 
if (rq->idle_stamp) { 
if (rq->avg_idle > max) 
if (p->on_rq) { 
while (llist) { 
for the first time) will also send 
if (llist_empty(&this_rq()->wake_list) 
if (unlikely(got_nohz_idle_kick())) { 
if (llist_add(&p->wake_entry, &cpu_rq(cpu)->wake_list)) 
if defined(CONFIG_SMP) 
ifier flags (WF_*) 
fore_spinlock(); 
if (!(p->state & state)) 
if (p->on_rq && ttwu_remote(p, wake_flags)) 
while (p->on_cpu) 
if (p->sched_class->task_waking) 
if (task_cpu(p) != cpu) { 
if (WARN_ON_ONCE(rq != this_rq()) || 
if (!raw_spin_trylock(&p->pi_lock)) { 
if (!(p->state & TASK_NORMAL)) 
if (!p->on_rq) 
fork() is basic setup used by init_idle() too: 
fork(unsigned long clone_flags, struct task_struct *p) 
INIT_LIST_HEAD(&p->se.group_node); 
INIT_LIST_HEAD(&p->rt.run_list); 
if (p->mm && atomic_read(&p->mm->mm_users) == 1) { 
if (clone_flags & CLONE_VM) 
INIT_LIST_HEAD(&p->numa_entry); 
if (enabled) 
if (write && !capable(CAP_SYS_ADMIN)) 
if (err < 0) 
if (write) 
fork()/clone()-time setup: 
fork(unsigned long clone_flags, struct task_struct *p) 
fork(clone_flags, p); 
if (unlikely(p->sched_reset_on_fork)) { 
if (PRIO_TO_NICE(p->static_prio) < 0) 
if (rt_prio(p->prio)) { 
if (p->sched_class->task_fork) 
fork(p); 
fork() 
if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) 
if (runtime == RUNTIME_INF) 
if (period == 0) 
for_each_cpu_and(i, rd->span, cpu_active_mask) 
if (new_bw == p->dl.dl_bw) 
if (dl_policy(policy) && !task_has_dl_policy(p) && 
if (dl_policy(policy) && task_has_dl_policy(p) && 
if (!dl_policy(policy) && task_has_dl_policy(p)) { 
ifier_register(struct preempt_notifier *notifier) 
ifier_register); 
ifier_unregister(struct preempt_notifier *notifier) 
ifier_unregister); 
for_each_entry(notifier, &curr->preempt_notifiers, link) 
ifiers(struct task_struct *curr, 
for_each_entry(notifier, &curr->preempt_notifiers, link) 
ifiers(struct task_struct *curr) 
ifiers(struct task_struct *curr, 
ifiers(prev, next); 
for 
ifiers(current); 
if (unlikely(prev_state == TASK_DEAD)) { 
if (prev->sched_class->task_dead) 
if (prev->sched_class->pre_schedule) 
if (rq->post_schedule) { 
if (rq->curr->sched_class->post_schedule) 
if (!mm) { 
if (!prev->mm) { 
for_each_online_cpu(i) 
for_each_possible_cpu(i) 
for_each_possible_cpu(i) 
if (dest_cpu == smp_processor_id()) 
if (likely(cpu_active(dest_cpu))) { 
if (task_current(rq, p)) { 
if ((s64)ns < 0) 
if defined(CONFIG_64BIT) && defined(CONFIG_SMP) 
if (!p->on_cpu) 
iffies); 
if (time_before_eq(next, now)) 
iffies_to_nsecs(next - now); 
if (in_lock_functions(addr)) { 
if (in_lock_functions(addr)) 
if defined(CONFIG_PREEMPT) && (defined(CONFIG_DEBUG_PREEMPT) || \ 
if (DEBUG_LOCKS_WARN_ON((preempt_count() < 0))) 
if (DEBUG_LOCKS_WARN_ON(val > preempt_count())) 
if (DEBUG_LOCKS_WARN_ON((val < PREEMPT_MASK) && 
if (oops_in_progress) 
while atomic: %s/%d/0x%08x\n", 
if (irqs_disabled()) 
if we are atomic. Since do_exit() needs to call into 
if (unlikely(in_atomic_preempt_off() && prev->state != TASK_DEAD)) 
if (prev->on_rq || rq->skip_clock_update < 0) 
if (likely(rq->nr_running == rq->cfs.h_nr_running)) { 
if (likely(p)) 
for_each_class(class) { 
if (p) 
if (sched_feat(HRTICK)) 
fore_spinlock(); 
if (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) { 
if (prev->flags & PF_WQ_WORKER) { 
if (to_wakeup) 
if (unlikely(!rq->nr_running)) 
if (likely(prev != next)) { 
if (need_resched()) 
if (!tsk->state || tsk_is_pi_blocked(tsk)) 
if (blk_needs_flush_plug(tsk)) 
if (likely(!preemptible())) 
while (need_resched()); 
while (need_resched()); 
form) 
if (unlikely(p == rq->idle)) { 
if (on_rq) 
if (running) 
if (dl_prio(prio)) { 
if (rt_prio(prio)) { 
if (oldprio < prio) 
if (dl_prio(oldprio)) 
if (running) 
if (on_rq) 
if (TASK_NICE(p) == nice || nice < -20 || nice > 19) 
if called from sys_setpriority(), 
if (task_has_dl_policy(p) || task_has_rt_policy(p)) { 
if (on_rq) 
if (on_rq) { 
if (delta < 0 || (delta > 0 && task_running(rq, p))) 
if (increment < -40) 
if (increment > 40) 
if (nice < -20) 
if (nice > 19) 
if (increment < 0 && !can_nice(current, nice)) 
if (retval) 
if (rq->curr != rq->idle) 
if (rq->nr_running) 
if (policy == -1) /* setparam */ 
if (dl_policy(policy)) 
if (fair_policy(policy)) 
for !rt tasks. 
if (dl_prio(p->prio)) 
if (rt_prio(p->prio)) 
if (attr->sched_deadline == 0) 
if (attr->sched_runtime < (1ULL << DL_SCALE)) 
if (attr->sched_deadline & (1ULL << 63) || 
if period != 0) */ 
if (policy < 0) { 
fork = !!(attr->sched_flags & SCHED_FLAG_RESET_ON_FORK); 
if (attr->sched_flags & ~(SCHED_FLAG_RESET_ON_FORK)) 
if ((p->mm && attr->sched_priority > MAX_USER_RT_PRIO-1) || 
if ((dl_policy(policy) && !__checkparam_dl(attr)) || 
if (user && !capable(CAP_SYS_NICE)) { 
if (attr->sched_nice < TASK_NICE(p) && 
if (rt_policy(policy)) { 
if (policy != p->policy && !rlim_rtprio) 
if (attr->sched_priority > p->rt_priority && 
if (dl_policy(policy)) 
if (p->policy == SCHED_IDLE && policy != SCHED_IDLE) { 
if (!check_same_owner(p)) 
if (user) { 
if (retval) 
while we are 
if (p == rq->stop) { 
if (unlikely(policy == p->policy)) { 
if (rt_policy(policy) && attr->sched_priority != p->rt_priority) 
if (dl_policy(policy)) 
if (user) { 
if (rt_bandwidth_enabled() && rt_policy(policy) && 
if (dl_bandwidth_enabled() && dl_policy(policy)) { 
if (!cpumask_subset(span, &p->cpus_allowed) || 
if (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) { 
if ((dl_policy(policy) || dl_task(p)) && dl_overflow(p, policy, attr)) { 
if (on_rq) 
if (running) 
if (running) 
if (on_rq) 
if (policy & SCHED_RESET_ON_FORK) { 
if (!param || pid < 0) 
if (copy_from_user(&lparam, param, sizeof(struct sched_param))) 
if (p != NULL) 
if (!access_ok(VERIFY_WRITE, uattr, SCHED_ATTR_SIZE_VER0)) 
if (ret) 
if (size > PAGE_SIZE)	/* silly large */ 
if (!size)		/* abi compat */ 
if (size < SCHED_ATTR_SIZE_VER0) 
if (size > sizeof(*attr)) { 
for (; addr < end; addr++) { 
if (ret) 
if (val) 
if (ret) 
if (!uattr || pid < 0 || flags) 
if (retval) 
if ((int)attr.sched_policy < 0) 
if (p != NULL) 
if (pid < 0) 
if (p) { 
if (!retval) 
fork ? SCHED_RESET_ON_FORK : 0); 
if (!param || pid < 0) 
if (!p) 
if (retval) 
if (task_has_rt_policy(p)) 
if (!access_ok(VERIFY_WRITE, uattr, usize)) 
if (usize < sizeof(*attr)) { 
for (; addr < end; addr++) { 
if (ret) 
for fwd/bwd comp. 
if (!uattr || pid < 0 || size > PAGE_SIZE || 
if (!p) 
if (retval) 
if (p->sched_reset_on_fork) 
if (task_has_dl_policy(p)) 
if (task_has_rt_policy(p)) 
if (!p) { 
INITY) { 
if (!alloc_cpumask_var(&cpus_allowed, GFP_KERNEL)) { 
if (!alloc_cpumask_var(&new_mask, GFP_KERNEL)) { 
if (!check_same_owner(p)) { 
if (!ns_capable(__task_cred(p)->user_ns, CAP_SYS_NICE)) { 
if (retval) 
if (dl_bandwidth_enabled() && !cpumask_subset(span, new_mask)) { 
if (!retval) { 
if (!cpumask_subset(new_mask, cpus_allowed)) { 
if (len < cpumask_size()) 
if (len > cpumask_size()) 
if (!alloc_cpumask_var(&new_mask, GFP_KERNEL)) 
if (retval == 0) 
if (!p) 
if (retval) 
if ((len * BITS_PER_BYTE) < nr_cpu_ids) 
if (len & (sizeof(unsigned long)-1)) 
if (!alloc_cpumask_var(&mask, GFP_KERNEL)) 
if (ret == 0) { 
if (copy_to_user(user_mask_ptr, mask, retlen)) 
if (should_resched()) { 
if a reschedule is pending, drop the given lock, 
if (spin_needbreak(lock) || resched) { 
if (resched) 
if (should_resched()) { 
if removing the yield() call from your code breaks 
while (!event) 
for something, use wait_event(). 
if we indeed boosted the target task. 
if (rq->nr_running == 1 && p_rq->nr_running == 1) { 
if (task_rq(p) != p_rq) { 
if (!curr->sched_class->yield_to_task) 
if (curr->sched_class != p->sched_class) 
if (task_running(p_rq, p) || p->state) 
if (yielded) { 
if (preempt && rq != p_rq) 
if (yielded > 0) 
if (pid < 0) 
if (!p) 
if (retval) 
if (p->sched_class->get_rr_interval) 
iffies_to_timespec(time_slice, &t); 
if (state == TASK_RUNNING) 
if (!state_filter || (p->state & state_filter)) 
while_each_thread(g, p); 
if (!state_filter) 
fork(0, idle); 
fork(). / Alternatively we could 
if defined(CONFIG_SMP) 
if defined(CONFIG_SMP) 
if (p->sched_class && p->sched_class->set_cpus_allowed) 
forcing the migrated thread 
if (cpumask_equal(&p->cpus_allowed, new_mask)) 
if (!cpumask_intersects(new_mask, cpu_active_mask)) { 
if (cpumask_test_cpu(task_cpu(p), new_mask)) 
if (p->on_rq) { 
if (unlikely(!cpu_active(dest_cpu))) 
if (task_cpu(p) != src_cpu) 
if (!cpumask_test_cpu(dest_cpu, tsk_cpus_allowed(p))) 
if (p->on_rq) { 
if (curr_cpu == target_cpu) 
if (!cpumask_test_cpu(target_cpu, tsk_cpus_allowed(p))) 
if (on_rq) 
if (running) 
if (running) 
if (on_rq) 
if (mm != &init_mm) 
if (delta) 
for ( ; ; ) { 
if (rq->nr_running == 1) 
for (entry = *tablep; entry->mode; entry++) { 
if (entry->proc_handler == NULL) 
if (load_idx) { 
if (table == NULL) 
forkexec_idx", &sd->forkexec_idx, 
for_each_domain(cpu, sd) 
if (table == NULL) 
for_each_domain(cpu, sd) { 
if (entry == NULL) 
for_each_possible_cpu(i) { 
if (sd_sysctl_header) 
if (sd_ctl_dir[0].child) 
if (!rq->online) { 
for_each_class(class) { 
if (rq->online) { 
for_each_class(class) { 
ifier_block *nfb, unsigned long action, void *hcpu) 
if (rq->rd) { 
if (rq->rd) { 
ifier_block *nfb, 
ifier_block *nfb, 
if (!(action & CPU_TASKS_FROZEN)) { 
if (overflow) 
ifier, CPU_ONLINE, cpu); 
ifier(sched_cpu_inactive, CPU_PRI_SCHED_INACTIVE); 
if (!(sd->flags & SD_LOAD_BALANCE)) { 
if (sd->parent) 
if (!cpumask_test_cpu(cpu, sched_domain_span(sd))) { 
if (!cpumask_test_cpu(cpu, sched_group_cpus(group))) { 
if (!group) { 
if (!group->sgp->power_orig) { 
if (!cpumask_weight(sched_group_cpus(group))) { 
if (!(sd->flags & SD_OVERLAP) && 
if (group->sgp->power != SCHED_POWER_SCALE) { 
while (group != sd->groups); 
if (!cpumask_equal(sched_domain_span(sd), groupmask)) 
if (sd->parent && 
if (!sched_debug_enabled) 
if (!sd) { 
for (;;) { 
if (!sd) 
while (0) 
if (cpumask_weight(sched_domain_span(sd)) == 1) 
if (sd->flags & (SD_LOAD_BALANCE | 
if (sd->groups != sd->groups->next) 
if (sd->flags & (SD_WAKE_AFFINE)) 
if (sd_degenerate(parent)) 
if (!cpumask_equal(sched_domain_span(sd), sched_domain_span(parent))) 
if (nr_node_ids == 1) 
if (~cflags & pflags) 
if (rq->rd) { 
if (cpumask_test_cpu(rq->cpu, old_rd->online)) 
if (!atomic_dec_and_test(&old_rd->refcount)) 
if (cpumask_test_cpu(rq->cpu, cpu_active_mask)) 
if (old_rd) 
if (!alloc_cpumask_var(&rd->span, GFP_KERNEL)) 
if (!alloc_cpumask_var(&rd->online, GFP_KERNEL)) 
if (!alloc_cpumask_var(&rd->dlo_mask, GFP_KERNEL)) 
if (!alloc_cpumask_var(&rd->rto_mask, GFP_KERNEL)) 
if (cpudl_init(&rd->cpudl) != 0) 
if (cpupri_init(&rd->cpupri) != 0) 
if (!rd) 
if (init_rootdomain(rd) != 0) { 
if (!sg) 
if (free_sgp && atomic_dec_and_test(&sg->sgp->ref)) 
while (sg != first); 
if (sd->flags & SD_OVERLAP) { 
if (atomic_dec_and_test(&sd->groups->ref)) { 
for (; sd; sd = sd->parent) 
for this 
if (sd) { 
for (tmp = sd; tmp; ) { 
if (!parent) 
if (sd_parent_degenerate(tmp, parent)) { 
if (parent->parent) 
if (parent->flags & SD_PREFER_SIBLING) 
if (sd && sd_degenerate(sd)) { 
if (sd) 
for_each_cpu(i, span) { 
if (!cpumask_test_cpu(i, sched_domain_span(sibling))) 
for_each_cpu(i, span) { 
if (cpumask_test_cpu(i, covered)) 
if (!cpumask_test_cpu(i, sched_domain_span(child))) 
if (!sg) 
if (child->child) { 
if (atomic_inc_return(&sg->sgp->ref) == 1) 
if ((!groups && cpumask_test_cpu(cpu, sg_span)) || 
if (!first) 
if (last) 
if (child) 
if (sg) { 
for claim_allocations */ 
if (cpu != cpumask_first(span)) 
for_each_cpu(i, span) { 
if (cpumask_test_cpu(i, covered)) 
for_each_cpu(j, span) { 
if (!first) 
if (last) 
while (sg != sd->groups); 
INIT_NAME(sd, type)		do { } while (0) 
INIT_FUNC(CPU) 
INIT_FUNC(MC) 
INIT_FUNC(BOOK) 
if (kstrtoint(str, 0, &default_relax_domain_level)) 
if (!attr || attr->relax_domain_level < 0) { 
if (request < sd->level) { 
if (!atomic_read(&d->rd->refcount)) 
if (__sdt_alloc(cpu_map)) 
if (!d->sd) 
if (!d->rd) 
if (atomic_read(&(*per_cpu_ptr(sdd->sg, cpu))->ref)) 
if (atomic_read(&(*per_cpu_ptr(sdd->sgp, cpu))->ref)) 
for_each_sd_topology(tl)			\ 
if (sched_domains_numa_distance[level] > RECLAIM_DISTANCE) 
INIT_NAME(sd, NUMA); 
if (done) 
for (i = 0; i < nr_node_ids; i++) { 
for (j = 0; j < nr_node_ids; j++) 
if (distance == node_distance(0, 0)) 
for (i = 0; i < sched_domains_numa_levels; i++) { 
if (!sched_domains_numa_distance) 
for (i = 0; i < nr_node_ids; i++) { 
for (k = 0; k < nr_node_ids; k++) { 
if (distance > curr_distance && 
if (sched_debug() && node_distance(k, i) != distance) 
if (sched_debug() && i && !find_numa_distance(distance)) 
if (next_distance != curr_distance) { 
ify the above assumption. 
if (!sched_debug()) 
if (!sched_domains_numa_masks) 
for (i = 0; i < level; i++) { 
if (!sched_domains_numa_masks[i]) 
for (j = 0; j < nr_node_ids; j++) { 
if (!mask) 
for (k = 0; k < nr_node_ids; k++) { 
if (!tl) 
for (i = 0; default_topology[i].init; i++) 
for (j = 0; j < level; i++, j++) { 
for (i = 0; i < sched_domains_numa_levels; i++) { 
if (node_distance(j, node) <= sched_domains_numa_distance[i]) 
for (i = 0; i < sched_domains_numa_levels; i++) { 
ifier_block *nfb, 
ifier_block *nfb, 
for_each_sd_topology(tl) { 
if (!sdd->sd) 
if (!sdd->sg) 
if (!sdd->sgp) 
for_each_cpu(j, cpu_map) { 
if (!sd) 
if (!sg) 
if (!sgp) 
for_each_sd_topology(tl) { 
for_each_cpu(j, cpu_map) { 
if (sdd->sd) { 
if (sd && (sd->flags & SD_OVERLAP)) 
if (sdd->sg) 
if (sdd->sgp) 
if (!sd) 
if (child) { 
if (alloc_state != sa_rootdomain) 
for_each_cpu(i, cpu_map) { 
for_each_sd_topology(tl) { 
if (tl == sched_domain_topology) 
if (tl->flags & SDTL_OVERLAP || sched_feat(FORCE_SD_OVERLAP)) 
if (cpumask_equal(cpu_map, sched_domain_span(sd))) 
for (sd = *per_cpu_ptr(d.sd, i); sd; sd = sd->parent) { 
if (sd->flags & SD_OVERLAP) { 
if (build_sched_groups(sd, i)) 
if (!cpumask_test_cpu(i, cpu_map)) 
for (sd = *per_cpu_ptr(d.sd, i); sd; sd = sd->parent) { 
for_each_cpu(i, cpu_map) { 
if (!doms) 
for (i = 0; i < ndoms; i++) { 
for (i = 0; i < ndoms; i++) 
if (!doms_cur) 
for_each_cpu(i, cpu_map) 
if (!new && !cur) 
for (i = 0; i < ndoms_cur; i++) { 
if (cpumask_equal(doms_cur[i], doms_new[j]) 
if (doms_new == NULL) { 
for (i = 0; i < ndoms_new; i++) { 
if (cpumask_equal(doms_new[i], doms_cur[j]) 
if (doms_cur != &fallback_doms) 
ifier_block *nfb, unsigned long action, 
if (likely(num_cpus_frozen)) { 
ifier_block *nfb, unsigned long action, 
if (cpumask_empty(non_isolated_cpus)) 
ifier(sched_domains_numa_masks_update, CPU_PRI_SCHED_ACTIVE); 
ifier(cpuset_cpu_inactive, CPU_PRI_CPUSET_INACTIVE); 
if (set_cpus_allowed_ptr(current, non_isolated_cpus) < 0) 
for_each_possible_cpu(i) { 
INIT_LIST_HEAD(&root_task_group.children); 
for_each_possible_cpu(i) { 
INIT_LIST_HEAD(&rq->leaf_cfs_rq_list); 
for (j = 0; j < CPU_LOAD_IDX_MAX; j++) 
INIT_LIST_HEAD(&rq->cfs_tasks); 
if (cpu_isolated_map == NULL) 
if ((preempt_count_equals(preempt_offset) && !irqs_disabled()) || 
if (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy) 
if (irqs_disabled()) 
if (on_rq) 
if (on_rq) { 
if (!p->mm) 
if (TASK_NICE(p) < 0 && p->mm) 
while_each_thread(g, p); 
if /* defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB) */ 
fore reenabling interrupts and 
if (!tg) 
if (!alloc_fair_sched_group(tg, parent)) 
if (!alloc_rt_sched_group(tg, parent)) 
INIT_LIST_HEAD(&tg->children); 
for_each_possible_cpu(i) 
if (on_rq) 
if (unlikely(running)) 
if (unlikely(running)) 
if (on_rq) 
if (rt_task(p) && task_rq(p)->rt.tg == tg) 
while_each_thread(g, p); 
if (tg == d->tg) { 
if (runtime > period && runtime != RUNTIME_INF) 
if (rt_bandwidth_enabled() && !runtime && tg_has_rt_tasks(tg)) 
if (total > to_ratio(global_rt_period(), global_rt_runtime())) 
for_each_entry_rcu(child, &tg->children, siblings) { 
if (child == d->tg) { 
if (sum > total) 
if (err) 
for_each_possible_cpu(i) { 
if (rt_runtime_us < 0) 
if (tg->rt_bandwidth.rt_runtime == RUNTIME_INF) 
if (rt_period == 0) 
for_each_possible_cpu(i) { 
for_each_possible_cpu(cpu) { 
if (new_bw < dl_b->total_bw) 
if (ret) 
if (global_rt_runtime() != RUNTIME_INF) 
for_each_possible_cpu(cpu) { 
if (sysctl_sched_rt_period <= 0) 
if ((sysctl_sched_rt_runtime != RUNTIME_INF) && 
if (!ret && write) { 
if (ret) 
if (ret) 
if (ret) 
if (0) { 
if (!ret && write) { 
iffies(sched_rr_timeslice); 
if (!parent) { 
if (IS_ERR(tg)) 
if (parent) 
for_each(task, css, tset) { 
if (!sched_rt_can_attach(css_tg(css), task)) 
if (task->sched_class != &fair_sched_class) 
for_each(task, css, tset) 
if (!(task->flags & PF_EXITING)) 
if (tg == &root_task_group) 
if (quota < min_cfs_quota_period || period < min_cfs_quota_period) 
if (period > max_cfs_quota_period) 
if (ret) 
if (runtime_enabled && !runtime_was_enabled) 
if active) to handle new period expiry */ 
for_each_possible_cpu(i) { 
if (cfs_rq->throttled) 
if (runtime_was_enabled && !runtime_enabled) 
if (cfs_quota_us < 0) 
if (tg->cfs_bandwidth.quota == RUNTIME_INF) 
if (tg == d->tg) { 
if (quota == RUNTIME_INF || quota == -1) 
if (!tg->parent) { 
if (quota == RUNTIME_INF) 
if (parent_quota != RUNTIME_INF && quota > parent_quota) 
if (quota != RUNTIME_INF) { 
for CPU %d:\n", cpu); 
