list print :      (nil) (  0xef0040  DKDK_HEAD)   0xef0060 
list print :   0xef0040 (  0xef0060         if)   0xef0080 
list print :   0xef0060 (  0xef0080        for)   0xef00a0 
list print :   0xef0080 (  0xef00a0      while)      (nil) 
[ OK ] open : 3 ok... 
file : ./test/kernel/fs/ntfs/file.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for now without overflowing the page 
if (sizeof(unsigned long) < 8) { 
ifdef NTFS_RW 
if relevant complete pages are already uptodate in the page cache then 
if the attribute is resident, we do not need to touch the page 
if the page is uptodate, the 
ifies that the behaviour of resizing a file whilst it is mmap()ped 
if 
for i_ino 0x%lx, attribute type 0x%x, " 
if (!NInoAttr(ni)) 
if (NInoNonResident(ni)) 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (new_init_size > old_i_size) { 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
for us. 
if (IS_ERR(page)) { 
if (unlikely(PageError(page))) { 
if (ni->initialized_size > new_init_size) 
ifying the 
for sparse pages and here we would 
for us to only 
while (++index < end_index); 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (ctx) 
if (m) 
if (ctx) 
if (m) 
ifficult to understand, then think of the while loop being 
while (!ret && uaddr < end); 
while (!__get_user(c, uaddr) && (uaddr += PAGE_SIZE, uaddr < end)) 
if (len > bytes) 
while (bytes); 
if (!pages[nr]) { 
if (unlikely(!*cached_page)) { 
if (unlikely(err)) { 
while (nr < nr_pages); 
while (nr > 0) { 
for_read(struct buffer_head *bh) 
for_non_resident_write - prepare pages for receiving data 
for non-resident attributes from ntfs_file_buffered_write() 
if necessary, 
ified yet. 
for_non_resident_write(struct page **pages, 
for inode 0x%lx, attribute type 0x%x, start page " 
if 
if (!page_has_buffers(page)) { 
if (unlikely(!page_has_buffers(page))) 
while (++u < nr_pages); 
for each page over each buffer.  Use goto to 
if (buffer_new(bh)) 
if (buffer_mapped(bh)) { 
if (buffer_uptodate(bh)) 
if (PageUptodate(page)) { 
fore the write, i.e. now. 
if ((bh_pos < pos && bh_end > pos) || 
if (bh_pos < initialized_size) { 
for_read(bh); 
if (likely(!cdelta || (cdelta > 0 && cdelta < vcn_len))) { 
if 
if we allocated it.  On the other 
if (PageUptodate(page)) { 
if (unlikely(was_hole)) { 
if (bh_end <= pos || bh_pos >= end) 
if (likely(!was_hole)) { 
fore the write, 
if (!buffer_uptodate(bh) && bh_pos < end && 
if (bh_pos < initialized_size) { 
for_read(bh); 
if the 
if (bh_end <= pos || bh_pos >= end) { 
if (!buffer_uptodate(bh) && 
if (bh_pos < pos) { 
if (bh_end > end) { 
if (bh_pos > initialized_size) { 
if (!buffer_uptodate(bh)) 
if (!buffer_uptodate(bh)) { 
if (!rl) { 
if (likely(rl != NULL)) { 
while (rl->length && rl[1].vcn <= bh_cpos) 
if (likely(lcn >= 0)) { 
if (likely(vcn + vcn_len >= cend)) { 
if (unlikely(lcn != LCN_HOLE && lcn != LCN_ENOENT)) { 
if (!rl_write_locked) { 
for 
if (likely(!err)) { 
if (err == -ENOENT) { 
if (unlikely(vol->cluster_size < PAGE_CACHE_SIZE)) { 
if ((bh_cend <= cpos || bh_cpos >= cend)) { 
while is 
if (PageUptodate(page)) { 
if (!buffer_uptodate(bh)) { 
if it was not really out of 
if it is locked 
for reading relock it now and retry in case it changed 
if (!rl_write_locked) { 
while (--rl2 >= ni->runlist.rl) { 
if (IS_ERR(rl2)) { 
if (IS_ERR(rl)) { 
if (err != -ENOMEM) 
if (ntfs_cluster_free_from_rl(vol, rl2)) { 
if (!NInoAttr(ni)) 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (!highest_vcn) 
for the new 
for_mapping_pairs(vol, rl2, vcn, 
if (unlikely(mp_size <= 0)) { 
for mapping pairs " 
if (unlikely(err)) { 
if when we reach the end we have not 
if even that fails, add a new attribute 
for the extended attribute " 
if (unlikely(err)) { 
if it was not set. */ 
if (likely(NInoSparse(ni) || NInoCompressed(ni))) { 
if (a->data.non_resident.lowest_vcn) { 
if (unlikely(err)) { 
if (likely(vcn + vcn_len >= cend)) { 
while (bh_pos += blocksize, (bh = bh->b_this_page) != head); 
if (likely(!err && ++u < nr_pages)) 
if we took it. */ 
if (unlikely(rl_write_locked)) { 
if (unlikely(rl)) 
while (wait_bh > wait) { 
if (likely(buffer_uptodate(bh))) { 
if (unlikely(bh_pos + blocksize > initialized_size)) { 
if (likely(bh_pos < initialized_size)) 
if (unlikely(!buffer_uptodate(bh))) */ 
if (likely(!err)) { 
if (buffer_new(bh)) 
while ((bh = bh->b_this_page) != head); 
if (status.attr_switched) { 
if (ntfs_attr_lookup(ni->type, ni->name, ni->name_len, 
ified, need to restore it by punching a 
ify the runlist if we are able to generate a 
if (status.runlist_merged && !status.attr_switched) { 
if (ntfs_rl_punch_nolock(vol, &ni->runlist, bh_cpos, 1)) { 
if (success) */ { 
if we succeeded in punching its vcn out of the 
if (ntfs_bitmap_clear_bit(vol->lcnbmp_ino, lcn)) { 
if the runlist has been 
if (status.mp_rebuilt && !status.runlist_merged) { 
if (success) */ { 
if (status.mft_attr_mapped) { 
if (rl_write_locked) 
if (rl) 
if (u == nr_pages && 
if (!buffer_new(bh)) 
if (!buffer_uptodate(bh)) { 
while ((bh = bh->b_this_page) != head); 
if (len > bytes) 
if (unlikely(left)) { 
if (unlikely(left)) 
if (!bytes) 
while (++pages < last_page); 
while (++pages < last_page) { 
if (!bytes) 
if (len > bytes) 
while (1) { 
if (len > bytes) 
if (unlikely(left)) { 
if (!bytes) 
while (bytes) { 
if (len > bytes) 
if (iov->iov_len == iov_ofs) { 
ifference is that on a fault we need to memset the remainder of the 
ifference between __copy_from_user_inatomic() and 
former 
ifference at all on those architectures. 
if (len > bytes) 
if (unlikely(copied != len)) { 
if (unlikely(copied != len)) 
if (!bytes) 
while (++pages < last_page); 
while (++pages < last_page) { 
if (!bytes) 
if (len > bytes) 
while (nr_pages > 0); 
if (bh_end <= pos || bh_pos >= end) { 
while (bh_pos += blocksize, (bh = bh->b_this_page) != head); 
if (!partial && !PageUptodate(page)) 
while (++u < nr_pages); 
if we do not need to update initialized_size or i_size we 
if (end <= initialized_size) { 
if (!NInoAttr(ni)) 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (end > i_size_read(vi)) { 
if (ctx) 
if (m) 
if (err != -ENOMEM) 
for_non_resident_write() has been called before 
if all buffers in the 
for_non_resident_write(), we do not need to do any page 
for inode 0x%lx, attribute type 0x%x, start page " 
if (NInoNonResident(ni)) 
if (!NInoAttr(ni)) 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if necessary. */ 
if (!PageUptodate(page)) { 
if (end < attr_len) 
if necessary. */ 
if (end > initialized_size) { 
if (err == -ENOMEM) { 
if (PageUptodate(page)) { 
if (ctx) 
if (m) 
if (to > inode->i_size) { 
for i_ino 0x%lx, attribute type 0x%x, " 
if (unlikely(!count)) 
for 
if (ni->type != AT_INDEX_ALLOCATION) { 
if (NInoEncrypted(ni)) { 
for later: Encrypted files are _always_ 
if (NInoCompressed(ni)) { 
for later: If resident, the data is not 
if it 
if (unlikely(NInoTruncateFailed(ni))) { 
if (err || NInoTruncateFailed(ni)) { 
form write to inode " 
if (end > ll) { 
if (likely(ll >= 0)) { 
if (end > ll) { 
if possible or fail. */ 
form write to " 
if (pos > ll) { 
if (err < 0) { 
form write to inode " 
for non-resident 
if (vol->cluster_size > PAGE_CACHE_SIZE && NInoNonResident(ni)) 
form the actual write. */ 
if (likely(nr_segs == 1)) 
if (nr_pages > 1) { 
if (vcn != last_vcn) { 
if (unlikely(lcn < LCN_HOLE)) { 
if (lcn == LCN_ENOMEM) 
form write to " 
if (lcn == LCN_HOLE) { 
if (bytes > count) 
if (likely(nr_segs == 1)) 
if (unlikely(status)) 
if (NInoNonResident(ni)) { 
for_non_resident_write( 
if (unlikely(status)) { 
while (do_pages); 
if (pos + bytes > i_size) { 
if (likely(nr_segs == 1)) { 
if (likely(!status)) { 
if (unlikely(copied != bytes)) 
while (do_pages); 
while (count); 
if (cached_page) 
if (err) 
if (!count) 
if (err) 
if (err) 
if (ret > 0) { 
if (err < 0) 
if non-zero only flush user data and not metadata 
for fsync, fdatasync, and msync 
if @datasync is true, we do not wait on the inode to be written out 
for now. 
for inode 0x%lx.", vi->i_ino); 
if (err) 
if (!datasync || !NInoNonResident(NTFS_I(vi))) 
for dirty blocks then we could optimize the below to be 
if (unlikely(err && !ret)) 
if (likely(!ret)) 
if /* NTFS_RW */ 
ifdef NTFS_RW 
for 
for 
if /* NTFS_RW */ 
form function on the 
ifdef NTFS_RW 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/index.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
if allocation failed. 
if (ictx) 
if (ictx->entry) { 
if (ictx->actx) 
if (ictx->base_ni) 
if (page) { 
for which to search in the index 
fore calling ntfs_index_lookup(), @ictx must have been obtained from a 
ified by the index lookup context @ictx. 
for the @key. 
ified, call flush_dcache_index_entry_page() 
fore the call to ntfs_index_ctx_put() to 
fore writing out and then 
if (!ntfs_is_collation_rule_supported( 
for the index inode. */ 
if (IS_ERR(m)) { 
if (unlikely(!actx)) { 
if (unlikely(err)) { 
ified in read_inode). */ 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if ((u8*)ie < (u8*)actx->mrec || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
if ((u32)sizeof(INDEX_ENTRY_HEADER) + 
if ((key_len == le16_to_cpu(ie->key_length)) && !memcmp(key, 
fore the key of the current entry, there 
if (rc == -1) 
if (!rc) 
for the 
if (!(ie->flags & INDEX_ENTRY_NODE)) { 
ify that an index allocation exists. */ 
if necessary. 
if (IS_ERR(page)) { 
if ((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_CACHE_SIZE) { 
if (unlikely(!ntfs_is_indx_record(ia->magic))) { 
if (sle64_to_cpu(ia->index_block_vcn) != vcn) { 
ifferent from expected VCN (0x%llx).  Inode " 
if (le32_to_cpu(ia->index.allocated_size) + 0x18 != 
iffering from the index " 
if (index_end > kaddr + PAGE_CACHE_SIZE) { 
if (index_end > (u8*)ia + idx_ni->itype.index.block_size) { 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if ((u8*)ie < (u8*)ia || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
if ((u32)sizeof(INDEX_ENTRY_HEADER) + 
if ((key_len == le16_to_cpu(ie->key_length)) && !memcmp(key, 
fore the key of the current entry, there 
if (rc == -1) 
if (!rc) 
for 
if (!(ie->flags & INDEX_ENTRY_NODE)) { 
if ((ia->index.flags & NODE_MASK) == LEAF_NODE) { 
if (vcn >= 0) { 
if (old_vcn << vol->cluster_size_bits >> 
if (!err) 
if (actx) 
if (m) 
file : ./test/kernel/fs/ntfs/compress.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for the decompression engine 
if the allocations failed. 
if (!ntfs_compression_buffer) 
if (((s64)page->index << PAGE_CACHE_SHIFT) >= initialized_size) { 
for now there is no problem. 
for&handle out of bounds compressed page 
if ((page->index >= (initialized_size >> PAGE_CACHE_SHIFT)) && 
if none) (IN) 
ified. 
if success or -EOVERFLOW on error in the compressed stream. 
for uncompressed data / destination. */ 
for tag and token parsing. */ 
for the eight tokens in tag. */ 
if the current 
fore its end so the 
if (cb == cb_end || !le16_to_cpup((le16*)cb) || 
if (nr_completed_pages > 0) { 
for (i = 0; i < nr_completed_pages; i++) { 
if (di == xpage) 
for the current sub-block destination. */ 
if (*dest_index == dest_max_index && do_sb_end > dest_max_ofs) 
if (cb + 6 > cb_end) 
if (cb_sb_end > cb_end) 
if (!dp) { 
if (!*dest_ofs && (++*dest_index > dest_max_index)) 
if (!(le16_to_cpup((le16*)cb) & NTFS_SB_IS_COMPRESSED)) { 
if (cb_sb_end - cb != NTFS_SB_SIZE) 
if (!(*dest_ofs &= ~PAGE_CACHE_MASK)) { 
if (++*dest_index > dest_max_index) 
if (cb == cb_sb_end) { 
if (dp_addr < dp_sb_end) { 
if (!(*dest_ofs &= ~PAGE_CACHE_MASK)) 
if (cb > cb_sb_end || dp_addr > dp_sb_end) 
for (token = 0; token < 8; token++, tag >>= 1) { 
if we are done / still in range. */ 
if ((tag & NTFS_TOKEN_MASK) == NTFS_SYMBOL_TOKEN) { 
if (dp_addr == dp_sb_start) 
for (i = *dest_ofs - do_sb_start - 1; i >= 0x10; i >>= 1) 
if (dp_back_addr < dp_sb_start) 
ify it is in range. */ 
if (*dest_ofs > do_sb_end) 
if (length <= max_non_overlap) { 
for the 
while (length--) 
ified to be locked and the 
for the next compression 
if we were to just overwrite 
if pages are above 8kiB and the NTFS volume only uses 512 byte 
for PAGE_CACHE_SIZE > cb_size we are screwing up both in 
if the two sizes are not equal for a 
if we get here for anything that is not an 
if (unlikely(!pages || !bhs)) { 
if (xpage >= max_page) { 
if (nr_pages < max_page) 
for (i = 0; i < max_page; i++, offset++) { 
if (page) { 
if it isn't already read 
if (!PageDirty(page) && (!PageUptodate(page) || 
for (vcn = start_vcn, start_vcn += cb_clusters; vcn < start_vcn; 
if (!rl) { 
if (likely(rl != NULL)) { 
while (rl->length && rl[1].vcn <= vcn) 
if (lcn < 0) { 
if (lcn == LCN_HOLE) 
if (is_retry || lcn != LCN_RL_NOT_MAPPED) 
for the 
if (!ntfs_map_runlist(ni, vcn)) 
if (unlikely(!(bhs[nr_bhs] = sb_getblk(sb, block)))) 
while (++block < max_block); 
if (rl) 
for (i = 0; i < nr_bhs; i++) { 
if (!trylock_buffer(tbh)) 
if (unlikely(buffer_uptodate(tbh))) { 
for io completion on all buffer heads. */ 
if (buffer_uptodate(tbh)) 
if (unlikely(!buffer_uptodate(tbh))) { 
if (unlikely(!buffer_uptodate(tbh))) 
for (i = 0; i < nr_bhs; i++) { 
if (cb_pos + 2 <= cb + cb_size) 
if present) and destination. */ 
for the current cb. */ 
if (cb_max_page > max_page) 
if (vcn == start_vcn - cb_clusters) { 
if (cb_max_ofs) 
for (; cur_page < cb_max_page; cur_page++) { 
if (page) { 
for now there is no problem. 
if (likely(!cur_ofs)) 
if (cur_page == xpage) 
if (cb_pos >= cb_end) 
if (cb_max_ofs && cb_pos < cb_end) { 
if (page) 
if (vcn == start_vcn) { 
fore we read all the pages and use block_read_full_page() 
for the majority of pages. 
if (cb_max_ofs) 
for (; cur_page < cb_max_page; cur_page++) { 
if (page) 
if (cb_pos >= cb_end) 
if (cb_max_ofs && cb_pos < cb_end) { 
if (page) 
for (; cur2_page < cb_max_page; cur2_page++) { 
if (page) { 
if (cur2_page == xpage) 
if (cb_pos2 >= cb_end) 
if (err) { 
for (; prev_cur_page < cur_page; prev_cur_page++) { 
if (page) { 
if (prev_cur_page != xpage) 
for (i = 0; i < nr_bhs; i++) 
if (nr_cbs) 
if we have any pages left. Should never happen. */ 
for (cur_page = 0; cur_page < max_page; cur_page++) { 
if (page) { 
if (cur_page != xpage) 
if (likely(xpage_done)) 
while reading compressed data."); 
for (i = 0; i < nr_bhs; i++) 
for (i = cur_page; i < max_page; i++) { 
if (page) { 
if (i != xpage) 
file : ./test/kernel/fs/ntfs/dir.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for the name 
if necessary (see below) 
for an inode with name @uname in the directory with inode @dir_ni. 
format, i.e. it 
if the inode is not found -ENOENT is returned. Note that you 
for being negative, you have to check the 
for a case sensitive match first but we also look for a case 
for the case that we don't find an exact match, where we return 
for how quickly one can access them. This also fixes 
fore writing out and then 
for the directory. */ 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
ified in read_inode). */ 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if ((u8*)ie < (u8*)ctx->mrec || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
if that matches 
for 
if (ntfs_are_names_equal(uname, uname_len, 
fore, so we can 
if the perfect match is a short file name, 
if (ie->key.file_name.file_name_type == FILE_NAME_DOS) { 
if (!name) { 
form a case 
for simplicity). 
if (!NVolCaseSensitive(vol) && 
if (name) { 
if that doesn't find any " 
forge.net."); 
if (type != FILE_NAME_DOS) 
if (!name) { 
if (type != FILE_NAME_DOS) { 
fore the name of the current entry, there 
if (rc == -1) 
if (rc) 
for proper 
if (rc == -1) 
if (rc) 
for the 
if (!(ie->flags & INDEX_ENTRY_NODE)) { 
ify that an index allocation exists. */ 
if necessary. 
if (IS_ERR(page)) { 
if ((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_CACHE_SIZE) { 
if (unlikely(!ntfs_is_indx_record(ia->magic))) { 
if (sle64_to_cpu(ia->index_block_vcn) != vcn) { 
ifferent from expected VCN (0x%llx). " 
if (le32_to_cpu(ia->index.allocated_size) + 0x18 != 
iffering from the " 
if (index_end > kaddr + PAGE_CACHE_SIZE) { 
if (index_end > (u8*)ia + dir_ni->itype.index.block_size) { 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if ((u8*)ie < (u8*)ia || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
if that matches 
for 
if (ntfs_are_names_equal(uname, uname_len, 
fore, so we can 
if the perfect match is a short file name, 
if (ie->key.file_name.file_name_type == FILE_NAME_DOS) { 
if (!name) { 
form a case 
for simplicity). 
if (!NVolCaseSensitive(vol) && 
if (name) { 
if that doesn't find any " 
forge.net."); 
if (type != FILE_NAME_DOS) 
if (!name) { 
if (type != FILE_NAME_DOS) { 
fore the name of the current entry, there 
if (rc == -1) 
if (rc) 
for proper 
if (rc == -1) 
if (rc) 
for 
if (ie->flags & INDEX_ENTRY_NODE) { 
if (vcn >= 0) { 
if (old_vcn << vol->cluster_size_bits >> 
if (name) { 
if (!err) 
if (ctx) 
if (m) 
if (name) { 
if 0 
for the time when we 
for the name 
for an inode with name @uname in the directory with inode @dir_ni. 
format, i.e. it 
if the inode is not found -ENOENT is returned. Note that you 
for being negative, you have to check the 
for the directory. */ 
if (IS_ERR(m)) { 
if (!ctx) { 
if (unlikely(err)) { 
ified in read_inode). */ 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if ((u8*)ie < (u8*)ctx->mrec || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
iffer in case, but 
for consistency checking. We 
if (ntfs_are_names_equal(uname, uname_len, 
fore the name of the current entry, there 
if (rc == -1) 
if (rc) 
for proper 
if (rc == -1) 
if (rc) 
for the 
if (!(ie->flags & INDEX_ENTRY_NODE)) { 
ify that an index allocation exists. */ 
if necessary. 
if (IS_ERR(page)) { 
if ((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_CACHE_SIZE) { 
if (unlikely(!ntfs_is_indx_record(ia->magic))) { 
if (sle64_to_cpu(ia->index_block_vcn) != vcn) { 
ifferent from expected VCN (0x%llx). " 
if (le32_to_cpu(ia->index.allocated_size) + 0x18 != 
iffering from the " 
if (index_end > kaddr + PAGE_CACHE_SIZE) { 
if (index_end > (u8*)ia + dir_ni->itype.index.block_size) { 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if ((u8*)ie < (u8*)ia || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
iffer in case, but 
for consistency checking. We 
if (ntfs_are_names_equal(uname, uname_len, 
fore the name of the current entry, there 
if (rc == -1) 
if (rc) 
for proper 
if (rc == -1) 
if (rc) 
for 
if (ie->flags & INDEX_ENTRY_NODE) { 
if (vcn >= 0) { 
if (old_vcn << vol->cluster_size_bits >> 
if (!err) 
if (ctx) 
if (m) 
if 
ific filldir method 
for the converted name 
if we are returning a non-zero value as ntfs_readdir() 
if (name_type == FILE_NAME_DOS) { 
if (MREF_LE(ie->data.dir.indexed_file) == FILE_root) { 
if (MREF_LE(ie->data.dir.indexed_file) < FILE_first_user && 
if (name_len <= 0) { 
if (ie->key.file_name.file_attributes & 
if (ia_page) 
for %s with len %i, fpos 0x%llx, inode " 
if (!dir_emit(actor, name, name_len, mref, dt_type)) 
if we are aborting ->readdir. */ 
for 
ifications). 
fore writing out and then 
for inode 0x%lx, fpos 0x%llx.", 
if (actor->pos >= i_size + vol->mft_record_size) 
for all directories. */ 
format determined by current NLS. 
if (unlikely(!name)) { 
if (actor->pos >= vol->mft_record_size) 
for the directory. */ 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
fore calling the 
for us to 
if (unlikely(!ir)) { 
ified in read_inode). */ 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if (unlikely((u8*)ie < (u8*)ir || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
if continuing previous readdir. */ 
if going to skip the entry. */ 
if (rc) { 
if (!NInoIndexAllocPresent(ndir)) 
if (IS_ERR(bmp_vi)) { 
if (unlikely(bmp_pos >> 3 >= i_size_read(bmp_vi))) { 
if (IS_ERR(bmp_page)) { 
while (!(bmp[cur_bmp_pos >> 3] & (1 << (cur_bmp_pos & 7)))) { 
if (unlikely((cur_bmp_pos >> 3) >= PAGE_CACHE_SIZE)) { 
if (unlikely(((bmp_pos + cur_bmp_pos) >> 3) >= i_size)) 
if ((prev_ia_pos & (s64)PAGE_CACHE_MASK) != 
if (likely(ia_page != NULL)) { 
if necessary. 
if (IS_ERR(ia_page)) { 
if (unlikely((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_CACHE_SIZE)) { 
if (unlikely(!ntfs_is_indx_record(ia->magic))) { 
if (unlikely(sle64_to_cpu(ia->index_block_vcn) != (ia_pos & 
ifferent from expected VCN (0x%llx). " 
if (unlikely(le32_to_cpu(ia->index.allocated_size) + 0x18 != 
iffering from the " 
if (unlikely(index_end > kaddr + PAGE_CACHE_SIZE)) { 
if (unlikely(index_end > (u8*)ia + ndir->itype.index.block_size)) { 
for (;; ie = (INDEX_ENTRY*)((u8*)ie + le16_to_cpu(ie->length))) { 
if (unlikely((u8*)ie < (u8*)ia || (u8*)ie + 
if (ie->flags & INDEX_ENTRY_END) 
if continuing previous readdir. */ 
if going to skip the entry. */ 
fore returning, unless a non-zero value is returned in 
if (rc) { 
if (ia_page) { 
if (bmp_page) { 
if (ia_page) { 
if (ctx) 
if (m) 
if (!err) 
for now without overflowing the 
if (sizeof(unsigned long) < 8) { 
ifdef NTFS_RW 
if non-zero only flush user data and not metadata 
for fsync, fdatasync, and 
if it is present 
for a directory so things are not too bad. 
for inode 0x%lx.", vi->i_ino); 
if (err) 
if (bmp_vi) { 
if (unlikely(err && !ret)) 
if (likely(!ret)) 
if /* NTFS_RW */ 
ifdef NTFS_RW 
if /* NTFS_RW */ 
form function on the 
file : ./test/kernel/fs/ntfs/inode.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for equality 
ific part of the vfs 
for equality with the ntfs attribute @na. 
if the attributes match and 0 if not. 
if (vi->i_ino != na->mft_no) 
if (likely(!NInoAttr(ni))) { 
for a normal inode this is a mismatch. */ 
if (ni->type != na->type) 
if (ni->name_len != na->name_len) 
if (na->name_len && memcmp(ni->name, na->name, 
if (na->type == AT_INDEX_ALLOCATION) 
if (likely(na->type == AT_UNUSED)) { 
if (na->name_len && na->name != I30) { 
if (!ni->name) 
ific normal inode 
ific normal inode (i.e. a 
if true, the function failed and the error code is obtained from PTR_ERR(). 
if (unlikely(!vi)) 
if (vi->i_state & I_NEW) { 
if the failure was 
if (unlikely(err == -ENOMEM)) { 
if unnamed) 
ified by 
ified by the vfs inode @base_vi. 
for index allocation attributes, you need to use ntfs_index_iget() 
if true, the function failed and the error code is 
for indices. */ 
if (unlikely(!vi)) 
if (vi->i_state & I_NEW) { 
ifies things in that we never need to check for bad attribute 
if (unlikely(err)) { 
ified by @name 
if true, the function failed and the error code is 
if (unlikely(!vi)) 
if (vi->i_state & I_NEW) { 
ifies things in that we never need to check for bad index 
if (unlikely(err)) { 
if (likely(ni != NULL)) { 
if (!atomic_dec_and_test(&ni->count)) 
if (likely(ni != NULL)) { 
if (!atomic_dec_and_test(&ni->count)) 
ific part of an inode 
while the base inode 
for nested inode's mrec_lock's: 
if (likely(ni != NULL)) { 
if a file is in the $Extend directory 
if any of the names are in the $Extend system 
if the file is in the $Extend directory 
while (!(err = ntfs_attr_lookup(AT_FILE_NAME, NULL, 0, 0, 0, NULL, 0, 
if (p < (u8*)ctx->mrec || (u8*)p > (u8*)ctx->mrec + 
if (attr->non_resident) { 
if (attr->flags) { 
if (!(attr->data.resident.flags & RESIDENT_ATTR_IS_INDEXED)) { 
if (p2 < (u8*)attr || p2 > p) 
if (MREF_LE(file_name_attr->parent_directory) == FILE_Extend) 
if (unlikely(err != -ENOENT)) 
if (unlikely(nr_links)) { 
for reading and sets up the necessary @vi fields as well as initializing 
for i_ino 0x%lx.", vi->i_ino); 
for checking whether an inode has changed w.r.t. a file so 
ific part of @vi special casing 
if (vi->i_ino != FILE_MFT) 
if (IS_ERR(m)) { 
if (!ctx) { 
if (!(m->flags & MFT_RECORD_IN_USE)) { 
if (m->base_mft_record) { 
formation from mft record into vfs and ntfs inodes. */ 
for files which have both 
for the short file names by subtracting them or we need 
for now. 
for now. 
if (IS_RDONLY(vi)) 
if (m->flags & MFT_RECORD_IS_DIRECTORY) { 
if (vi->i_nlink > 1) 
formation attribute in the mft record. At this 
if the standard information is in an extent record, but 
if (unlikely(err)) { 
if the 
formation attribute value. */ 
formation from the standard information into vi. */ 
for example but changed whenever the file is written to. 
if present. */ 
if (err) { 
if (!err) */ { 
if (a->flags & ATTR_COMPRESSION_MASK) { 
if (a->flags & ATTR_IS_ENCRYPTED || 
if (a->non_resident) { 
for the attribute list. */ 
if (!ni->attr_list) { 
for attribute list."); 
if (a->non_resident) { 
if (a->data.non_resident.lowest_vcn) { 
for locking as we have 
if (IS_ERR(ni->attr_list_rl.rl)) { 
if ((err = load_attribute_list(vol, &ni->attr_list_rl, 
if (!a->non_resident) */ { 
if (S_ISDIR(vi->i_mode)) { 
if (unlikely(err)) { 
if recovery option is 
if (unlikely(a->non_resident)) { 
fore the value. */ 
if (a->flags & ATTR_COMPRESSION_MASK) 
if (a->flags & ATTR_IS_ENCRYPTED) { 
if (a->flags & ATTR_IS_SPARSE) 
if (ir_end > (u8*)ctx->mrec + vol->mft_record_size) { 
if (index_end > ir_end) { 
if (ir->type != AT_FILE_NAME) { 
if (ir->collation_rule != COLLATION_FILE_NAME) { 
if (ni->itype.index.block_size & 
if (ni->itype.index.block_size > PAGE_CACHE_SIZE) { 
if (ni->itype.index.block_size < NTFS_BLOCK_SIZE) { 
if (vol->cluster_size <= ni->itype.index.block_size) { 
if not present. */ 
if (!(ir->index.flags & LARGE_INDEX)) { 
if (unlikely(err)) { 
if (!a->non_resident) { 
fore the mapping pairs 
if (unlikely(a->name_length && (le16_to_cpu(a->name_offset) >= 
if (a->flags & ATTR_IS_ENCRYPTED) { 
if (a->flags & ATTR_IS_SPARSE) { 
if (a->flags & ATTR_COMPRESSION_MASK) { 
if (a->data.non_resident.lowest_vcn) { 
if (IS_ERR(bvi)) { 
if (NInoCompressed(bni) || NInoEncrypted(bni) || 
if ((bvi_size << 3) < (vi->i_size >> 
for index allocation (0x%llx).", 
for this inode. */ 
if not present. */ 
if (unlikely(err)) { 
if (err != -ENOENT) { 
if (vi->i_ino == FILE_Secure) 
if not all the system files in the $Extend 
if the parent 
if (ntfs_is_extended_system_file(ctx) > 0) 
if recovery option is set. 
if (a->flags & (ATTR_COMPRESSION_MASK | ATTR_IS_SPARSE)) { 
if (vol->cluster_size > 4096) { 
if ((a->flags & ATTR_COMPRESSION_MASK) 
if (a->flags & ATTR_IS_SPARSE) 
if (a->flags & ATTR_IS_ENCRYPTED) { 
if (a->non_resident) { 
if (NInoCompressed(ni) || NInoSparse(ni)) { 
if (a->data.non_resident.compression_unit) { 
if (a->data.non_resident.lowest_vcn) { 
if (vi->i_size > ni->allocated_size) { 
for this inode. */ 
if (NInoMstProtected(ni)) 
for stat). This is in so 
if not entirely 
ificant slowdown as it would involve iterating over all 
if (S_ISREG(vi->i_mode) && (NInoCompressed(ni) || NInoSparse(ni))) 
if (!err) 
if (ctx) 
if (m) 
if (err != -EOPNOTSUPP && err != -ENOMEM) 
for 
for AT_INDEX_ALLOCATION. 
for i_ino 0x%lx.", vi->i_ino); 
if (IS_ERR(m)) { 
if (!ctx) { 
if (unlikely(err)) 
if (a->flags & (ATTR_COMPRESSION_MASK | ATTR_IS_SPARSE)) { 
if ((ni->type != AT_DATA) || (ni->type == AT_DATA && 
forge.net"); 
if (vol->cluster_size > 4096) { 
if ((a->flags & ATTR_COMPRESSION_MASK) != 
if (NInoMstProtected(ni) && ni->type != AT_INDEX_ROOT) { 
forge.net", 
if (a->flags & ATTR_IS_SPARSE) 
if (a->flags & ATTR_IS_ENCRYPTED) { 
if (NInoMstProtected(ni) && ni->type != AT_INDEX_ROOT) { 
forge." 
if (ni->type != AT_DATA) { 
if (!a->non_resident) { 
fore the value. */ 
if (NInoMstProtected(ni)) { 
forge.net"); 
if (vi->i_size > ni->allocated_size) { 
fore the mapping pairs 
if (unlikely(a->name_length && (le16_to_cpu(a->name_offset) >= 
if (NInoCompressed(ni) || NInoSparse(ni)) { 
if (a->data.non_resident.compression_unit) { 
if (a->data.non_resident.lowest_vcn) { 
if (NInoMstProtected(ni)) 
if ((NInoCompressed(ni) || NInoSparse(ni)) && ni->type != AT_INDEX_ROOT) 
if (!err) 
if (ctx) 
while reading attribute " 
if (err != -ENOMEM) 
for 
fore setting up the necessary fields in @vi as well as initializing the 
for small indices the index allocation attribute might not actually exist. 
for directories, we need to have an attribute inode for 
for 
for i_ino 0x%lx.", vi->i_ino); 
for the base inode. */ 
if (IS_ERR(m)) { 
if (!ctx) { 
if (unlikely(err)) { 
if (unlikely(a->non_resident)) { 
fore the value. */ 
for 
if (a->flags & (ATTR_COMPRESSION_MASK | ATTR_IS_ENCRYPTED | 
if (ir_end > (u8*)ctx->mrec + vol->mft_record_size) { 
if (index_end > ir_end) { 
if (ir->type) { 
if (!is_power_of_2(ni->itype.index.block_size)) { 
if (ni->itype.index.block_size > PAGE_CACHE_SIZE) { 
if (ni->itype.index.block_size < NTFS_BLOCK_SIZE) { 
if (vol->cluster_size <= ni->itype.index.block_size) { 
for presence of index allocation attribute. */ 
if (unlikely(err)) { 
if (!a->non_resident) { 
fore the mapping pairs array. 
if (unlikely(a->name_length && (le16_to_cpu(a->name_offset) >= 
if (a->flags & ATTR_IS_ENCRYPTED) { 
if (a->flags & ATTR_IS_SPARSE) { 
if (a->flags & ATTR_COMPRESSION_MASK) { 
if (a->data.non_resident.lowest_vcn) { 
if (IS_ERR(bvi)) { 
if (NInoCompressed(bni) || NInoEncrypted(bni) || 
if ((bvi_size << 3) < (vi->i_size >> ni->itype.index.block_size_bits)) { 
for " 
for this index inode. */ 
if (!err) 
if (ctx) 
if (m) 
while reading index " 
if (err != -EOPNOTSUPP && err != -ENOMEM) 
for mount time use only 
for $MFT/$DATA 
fore anything 
formation for the next step to complete. 
if they are real pits or just smoke... 
ific part of @vi. */ 
for directories. 
if (vol->mft_record_size > 64 * 1024) { 
if (i < sb->s_blocksize) 
if (!m) { 
for $MFT record 0."); 
if (!nr_blocks) 
for (i = 0; i < nr_blocks; i++) { 
if (!bh) { 
if (post_read_mst_fixup((NTFS_RECORD*)m, vol->mft_record_size)) { 
for map_mft_record(). */ 
if (!ctx) { 
if present. */ 
if (err) { 
if (!err) */ { 
if (a->flags & ATTR_COMPRESSION_MASK) { 
if (a->flags & ATTR_IS_ENCRYPTED || 
if (a->non_resident) { 
for the attribute list. */ 
if (!ni->attr_list) { 
for attribute list."); 
if (a->non_resident) { 
if (a->data.non_resident.lowest_vcn) { 
if (IS_ERR(ni->attr_list_rl.rl)) { 
if ((err = load_attribute_list(vol, &ni->attr_list_rl, 
if (!ctx.attr->non_resident) */ { 
if this case is actually possible. 
form a manual search and make sure the first $MFT/$DATA 
if we ever see a report of this error we will need 
for (;; al_entry = next_al_entry) { 
if ((u8*)al_entry < ni->attr_list || 
if ((u8*)al_entry == al_end) 
if (!al_entry->length) 
if ((u8*)al_entry + 6 > al_end || (u8*)al_entry + 
if (le32_to_cpu(al_entry->type) > le32_to_cpu(AT_DATA)) 
if (AT_DATA != al_entry->type) 
if (al_entry->name_length) 
if (al_entry->lowest_vcn) 
if (MREF_LE(al_entry->mft_reference) != vi->i_ino) { 
forge.net"); 
if (MSEQNO_LE(al_entry->mft_reference) != 
while (!(err = ntfs_attr_lookup(AT_DATA, NULL, 0, 0, next_vcn, NULL, 0, 
if (!a->non_resident) { 
if (a->flags & ATTR_COMPRESSION_MASK || 
for locking 
if (IS_ERR(nrl)) { 
if (!next_vcn) { 
ify the number of mft records does not exceed 
if ((vi->i_size >> vol->mft_record_size_bits) >= 
for 
for $MFT, this time entering 
if 
if (is_bad_inode(vi)) { 
if no errors " 
forge.net"); 
ifics about $MFT's inode as 
for anyone. */ 
for $MFT. */ 
for the next extent. */ 
if (next_vcn <= 0) 
if (next_vcn < sle64_to_cpu( 
if (err != -ENOENT) { 
if (!a) { 
if (highest_vcn && highest_vcn != last_vcn - 1) { 
for " 
if (ni->runlist.rl) { 
if (ni->attr_list) { 
if (ni->attr_list_rl.rl) { 
if (ni->name_len && ni->name != I30) { 
for inode 0x%lx.", ni->mft_no); 
ifdef NTFS_RW 
if (!is_bad_inode(VFS_I(ni->ext.base_ntfs_ino))) 
if /* NTFS_RW */ 
ific part of an inode 
ific part 
fore doing anything else. 
ifdef NTFS_RW 
if (!was_bad && (is_bad_inode(vi) || NInoDirty(ni))) { 
if /* NTFS_RW */ 
if (ni->nr_extents > 0) { 
for (i = 0; i < ni->nr_extents; i++) 
if (NInoAttr(ni)) { 
if (ni->nr_extents == -1) { 
for each mounted ntfs volume when someone reads 
ified by @root are written to the seq file 
if (vol->fmask == vol->dmask) 
if (NVolCaseSensitive(vol)) 
if (NVolShowSystemFiles(vol)) 
if (!NVolSparseEnabled(vol)) 
for (i = 0; on_errors_arr[i].val; i++) { 
ifdef NTFS_RW 
for which the i_size was changed 
forced in ntfs_setattr(), see 
for us that @vi is a file inode rather than a directory, index, 
for inode 0x%lx.", vi->i_ino); 
for writing and map the mft record to ensure it is 
if (!NInoAttr(ni)) 
if (IS_ERR(m)) { 
for inode 0x%lx " 
if (unlikely(!ctx)) { 
for " 
if (unlikely(err)) { 
for the attribute value. 
if (NInoNonResident(ni)) 
if no change, >0 if the 
if (new_size - old_size >= 0) { 
if (new_size == old_size) 
for the allocated size. */ 
if (new_alloc_size - old_alloc_size >= 0) { 
if (new_alloc_size == old_alloc_size) 
if (!size_change && !alloc_change) 
if new size is allowed in $AttrDef. */ 
if (unlikely(err)) { 
for its attribute type " 
if (NInoCompressed(ni) || NInoEncrypted(ni)) { 
for %s files, ignoring.", 
if (a->non_resident) 
if (new_size < vol->mft_record_size && 
ifies that the behaviour is unspecified thus we do not 
ify that 
for us as we cannot lock the 
if that fails dropping 
for any given file. 
if successful restart the truncation process. 
if (likely(!err)) 
for this attribute type or there not being enough space, 
if (unlikely(err != -EPERM && err != -ENOSPC)) { 
if (err != -ENOMEM) 
if (err == -ENOSPC) 
for the non-resident attribute value.  " 
if (err == -EPERM) */ 
if 0 
if (!err) 
formation 
if (ni->type == AT_ATTRIBUTE_LIST || 
if (!err) 
if it is not already the only attribute in an mft record in 
if (!err) 
if 
if (alloc_change < 0) { 
if (highest_vcn > 0 && 
fore reducing the allocation. 
if (size_change < 0) { 
if (new_size < ni->initialized_size) { 
if (!alloc_change) 
for the 
if (size_change >= 0) */ { 
if (alloc_change > 0) { 
if the new 
ified as explained above for the resident 
if (!alloc_change) 
if (unlikely(nr_freed < 0)) { 
if (unlikely(err || IS_ERR(m))) { 
for the shrunk mapping pairs array for the runlist. */ 
if (unlikely(mp_size <= 0)) { 
for the mapping pairs failed with error " 
for the new mapping pairs array.  Note, 
if (unlikely(err)) { 
if (NInoSparse(ni) || NInoCompressed(ni)) { 
fore the allocation change). 
if (size_change > 0) 
ified mft record is written out. */ 
fore so it got a copy & paste version 
for real. 
if (!IS_NOCMTIME(VFS_I(base_ni)) && !IS_RDONLY(VFS_I(base_ni))) { 
if (!timespec_equal(&VFS_I(base_ni)->i_mtime, &now) || 
if (sync_it) 
if (likely(!err)) { 
if (err != -ENOMEM && err != -EOPNOTSUPP) 
if (err != -EOPNOTSUPP) 
if (old_size >= 0) 
if (ctx) 
if (m) 
if (err != -ENOMEM && err != -EOPNOTSUPP) 
if (err != -EOPNOTSUPP) 
for ntfs_truncate() that has no return value 
for ntfs_truncate() that has no return value. 
ifdef NTFS_RW 
if 
ify_change() when an attribute is being changed 
if (err) 
if (ia_valid & (ATTR_UID | ATTR_GID | ATTR_MODE)) { 
if (ia_valid & ATTR_SIZE) { 
if (NInoCompressed(ni) || NInoEncrypted(ni)) { 
for " 
if (err || ia_valid == ATTR_SIZE) 
if (ia_valid & ATTR_ATIME) 
if (ia_valid & ATTR_MTIME) 
if (ia_valid & ATTR_CTIME) 
if true, write out synchronously 
for io completion.  This 
for i/o 
forms synchronous writes. 
ified = false; 
for %sinode 0x%lx.", NInoAttr(ni) ? "attr " : "", 
if (NInoAttr(ni)) { 
if (IS_ERR(m)) { 
formation attribute. */ 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if they have changed. */ 
if (si->last_data_change_time != nt) { 
for inode 0x%lx: old = 0x%llx, " 
ified = true; 
if (si->last_mft_change_time != nt) { 
for inode 0x%lx: old = 0x%llx, " 
ified = true; 
if (si->last_access_time != nt) { 
for inode 0x%lx: old = 0x%llx, " 
ified = true; 
ified the standard information attribute we need to 
ified so it 
for $MFT itself is being 
fore 
if (modified) { 
if (!NInoTestSetDirty(ctx->ntfs_ino)) 
if (NInoDirty(ni)) 
if (ni->nr_extents > 0) { 
for (i = 0; i < ni->nr_extents; i++) { 
if (NInoDirty(tni)) { 
if (IS_ERR(tm)) { 
if (unlikely(ret)) { 
if (unlikely(err)) 
if (err == -ENOMEM) { 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/namei.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for the inode 
for the inode represented by the dentry @dent 
ifies which inode to look for by 
for the converted Unicode name. If the name is found in the 
if an actual error occurs, do we return an error via ERR_PTR(). 
iffer in case in ->ntfs_lookup() while maintaining 
for any other case (or for the short file 
for a fully matching file name 
ifferent case and if that has non-POSIX semantics we return 
ify matters for us, we do not treat the short vs long filenames as 
for the corresponding long filename instead. 
if a dentry with this name already exists 
for a dentry with this name, etc, as in case 2), above. 
if (uname_len < 0) { 
if (!IS_ERR_MREF(mref)) { 
if (likely(!IS_ERR(dent_inode))) { 
if (is_bad_inode(dent_inode) || MSEQNO(mref) == 
if (!name) { 
if (MREF_ERR(mref) == -ENOENT) { 
if (name->type != FILE_NAME_DOS) {			/* Case 2. */ 
if (name->type == FILE_NAME_DOS) */ {		/* Case 3. */ 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (err == -ENOENT) 
if (a->non_resident || a->flags) 
if (le16_to_cpu(a->data.resident.value_offset) + 
if ((u32)(fn->file_name_length * sizeof(ntfschar) + 
while (fn->file_name_type != FILE_NAME_WIN32); 
if a conversion error occurred. */ 
if (ctx) 
if (m) 
for directories. 
ified by the 
for inode 0x%lx.", vi->i_ino); 
if (IS_ERR(mrec)) 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (err == -ENOENT) 
if (unlikely(attr->non_resident)) 
if (unlikely((u8 *)fn + le32_to_cpu(attr->data.resident.value_length) > 
if (!IS_ERR(inode)) { 
for now.  Note that they 
for now we will ignore the 
for the system file $MFT 
file : ./test/kernel/fs/ntfs/logfile.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifdef NTFS_RW 
for consistency 
if it is 
if (logfile_system_page_size < NTFS_BLOCK_SIZE || 
if (pos && pos != logfile_system_page_size) { 
if (sle16_to_cpu(rp->major_ver) != 1 || 
if (ntfs_is_chkd_record(rp->magic) && !le16_to_cpu(rp->usa_count)) { 
ify the size of the update sequence array. */ 
if (usa_count != le16_to_cpu(rp->usa_count)) { 
ify the position of the update sequence array. */ 
if (usa_ofs < sizeof(RESTART_PAGE_HEADER) || 
ifies " 
ify the position of the restart area.  It must be: 
if (ra_ofs & 7 || (have_usa ? ra_ofs < usa_end : 
ifies " 
ified by chkdsk are allowed to have chkdsk_lsn 
if (!ntfs_is_chkd_record(rp->magic) && sle64_to_cpu(rp->chkdsk_lsn)) { 
ified."); 
for consistency 
for consistency and return 
fore ra->file_size must be before the first word 
if (ra_ofs + offsetof(RESTART_AREA, file_size) > 
ifies " 
fore the first word protected by an 
if (((ca_ofs + 7) & ~7) != ca_ofs || 
ifies " 
ified by ra->restart_area_length. 
if (ra_ofs + ra_len > le32_to_cpu(rp->system_page_size) || 
ified by the " 
if ((ra->client_free_list != LOGFILE_NO_CLIENT && 
ifies " 
for consistency. 
while (file_size) { 
if (le32_to_cpu(ra->seq_number_bits) != 67 - fs_bits) { 
if (((le16_to_cpu(ra->log_record_header_length) + 7) & ~7) != 
ifies " 
for the log page data offset. */ 
ifies " 
for consistency 
for consistency and 
for (idx_is_first = true; idx != LOGFILE_NO_CLIENT_CPU; nr_clients--, 
if (!nr_clients || idx >= le16_to_cpu(ra->log_clients)) 
if (idx_is_first) { 
if we just did the free list. */ 
for consistency 
if it is consistent 
if @lsn is not NULL, on success *@lsn will be set to the current 
for consistency. */ 
for consistency. */ 
if (!trp) { 
for $LogFile " 
if (size >= le32_to_cpu(rp->system_page_size)) { 
if (IS_ERR(page)) { 
if (err != -EIO && err != -ENOMEM) 
while (to_read > 0); 
if the 
if ((!ntfs_is_chkd_record(trp->magic) || le16_to_cpu(trp->usa_count)) 
if the restart page contents exceed the multi sector 
if (le16_to_cpu(rp->restart_area_offset) + 
ified by chkdsk or there are no active 
for consistency, too. 
if (ntfs_is_rstr_record(rp->magic) && 
if (!ntfs_check_log_client_array(vi, trp)) { 
if (lsn) { 
if (ntfs_is_chkd_record(rp->magic)) */ 
if (wrp) 
for consistency 
if it is 
if the $LogFile was created on a system with a different page size to ours 
fore it got emptied. */ 
if (size > MaxLogFileSize) 
if the page cache size is between the default log page 
if (PAGE_CACHE_SIZE >= DefaultLogPageSize && PAGE_CACHE_SIZE <= 
if (size < log_page_size * 2 || (size - log_page_size * 2) >> 
for a restart page.  Since the restart 
for each page size) rather 
for (pos = 0; pos < size; pos <<= 1) { 
if (!page || page->index != idx) { 
if (IS_ERR(page)) { 
while an 
if (!ntfs_is_empty_recordp((le32*)kaddr)) 
if (!logfile_is_empty) 
if (ntfs_is_rcrd_recordp((le32*)kaddr)) 
ified by chkdsk) restart page, continue. */ 
if (!pos) 
ified by chkdsk) restart page for consistency 
if (!err) { 
ified by chkdsk) 
for the second one. 
if (!pos) { 
ified by chkdsk) 
if the restart page was invalid as we might still 
if (err != -EINVAL) { 
if (!pos) 
if (page) 
if (logfile_is_empty) { 
if (!rstr1_ph) { 
if (rstr2_ph) { 
if (rstr2_lsn > rstr1_lsn) { 
if (rp) 
if (rstr1_ph) 
if the volume is clean 
if it indicates the volume was 
for the five seconds preceding the unclean shutdown. 
if the $LogFile 
fore it got emptied. */ 
if (!ntfs_is_rstr_record(rp->magic) && 
fore calling " 
if (ra->client_in_use_list != LOGFILE_NO_CLIENT && 
if (NVolLogFileEmpty(vol)) { 
for the $LogFile/$DATA attribute and 
if (unlikely(!rl || vcn < rl->vcn || !rl->length)) { 
if (err) { 
while (rl->length && vcn >= rl[1].vcn) 
if (unlikely(lcn == LCN_RL_NOT_MAPPED)) { 
if (unlikely(!rl->length || lcn < LCN_HOLE)) 
if (lcn == LCN_HOLE) 
if (rl[1].vcn > end_vcn) 
if (!buffer_uptodate(bh)) 
if (buffer_dirty(bh)) 
for i/o to complete but 
if one buffer worked all of them will work. 
if (should_wait) { 
if (unlikely(!buffer_uptodate(bh))) 
while (++block < end_block); 
if emptying should fail. 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/debug.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifications 
for the mounted ntfs filesystem described 
format string containing 
format vaf; 
ifndef DEBUG 
if 
if (sb) 
ifications 
for the mounted ntfs filesystem described 
format string containing 
format vaf; 
ifndef DEBUG 
if 
if (sb) 
ifdef DEBUG 
format vaf; 
if (!debug_msgs) 
if (function) 
for @rl. */ 
if (!debug_msgs) 
if (!rl) { 
for (i = 0; ; i++) { 
if (lcn < (LCN)0) { 
if (index > -LCN_ENOENT - 1) 
if (!(rl + i)->length) 
if 
file : ./test/kernel/fs/ntfs/usnjrnl.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifdef NTFS_RW 
if (likely(!NVolUsnJrnlStamped(vol))) { 
if (IS_ERR(page)) { 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/lcnalloc.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifdef NTFS_RW 
for writing on entry and is 
if (!rl) 
for (; rl->length; rl++) { 
if (rl->lcn < 0) 
if (unlikely(err && (!ret || ret == -ENOMEM) && ret != err)) 
for the first allocated cluster 
if none) 
if 'true', this is an attribute extension 
if @start_lcn is -1, on the mounted ntfs volume 
for allocation of normal clusters or 
ifies the vcn of the first allocated cluster.  This makes 
if it is 'false', the caller is allocating clusters to fill a 
if 
ified/standard NTFS 1.x 
for the 
for caller supplied hints as to the location 
while, because this allocator should: 1) be a full implementation of 
for speed, but the algorithm is, so further speed improvements are probably 
for the future.  We will just cause 
fort. (AIA) 
if we have only one of 
for now, I am leaving the double logic - 
for writing and 
for start_vcn 0x%llx, count 0x%llx, start_lcn " 
if @count is zero. */ 
for writing. */ 
ific @start_lcn was requested, use the current data zone 
ified position.  If the latter is out of bounds then we start 
for mft zone, 2 for data zone 1 (end of mft zone till end of 
if (zone_start < 0) { 
if (!zone_start) { 
if (zone == DATA_ZONE && zone_start >= vol->mft_zone_start && 
if (zone == MFT_ZONE && (zone_start < vol->mft_zone_start || 
if (!vol->mft_zone_end) 
if (zone == MFT_ZONE) { 
if (zone == DATA_ZONE) */ { 
if (zone_start >= vol->mft_zone_end) { 
while (1) { 
if (last_read_pos > i_size) { 
if (likely(page)) { 
if (IS_ERR(page)) { 
if (unlikely(last_read_pos + buf_size > i_size)) 
fore inner while loop: buf_size %i, lcn 0x%llx, " 
while (lcn < buf_size && lcn + bmp_pos < zone_end) { 
while loop: buf_size %i, " 
if (*byte == 0xff) { 
while loop 1."); 
if (*byte & bit) { 
while loop 2."); 
if needed, including space for 
if ((rlpos + 2) * sizeof(*rl) > rlsize) { 
if (!rl) 
if (unlikely(!rl2)) { 
if adjacent LCNs. 
if (prev_lcn == lcn + bmp_pos - prev_run_len && rlpos) { 
if (likely(rlpos)) { 
if (!--clusters) { 
fore checks, " 
if (tc >= vol->mft_zone_end) { 
if (!vol->mft_zone_end) 
if ((bmp_initial_pos >= 
fore checks, " 
if (tc >= vol->nr_clusters) 
if ((bmp_initial_pos >= 
fore checks, " 
if (tc >= vol->mft_zone_start) 
if (bmp_initial_pos >= 
while loop: buf_size 0x%x, lcn " 
if (bmp_pos < zone_end) { 
while loop, " 
if (pass == 1) { 
if (zone_end < zone_start) 
while loop, pass 2, " 
fore 0x%x, done_zones after 0x%x.", 
if (done_zones < 7) { 
if (rlpos) { 
fore checks, " 
if (tc >= vol->mft_zone_end) { 
if (!vol->mft_zone_end) 
if ((bmp_initial_pos >= 
if (zone_start == vol->mft_zone_end) 
if (zone_start >= zone_end) { 
if (rlpos) { 
fore checks, " 
if (tc >= vol->nr_clusters) 
if ((bmp_initial_pos >= 
if (!zone_start) 
if (zone_start >= zone_end) { 
if (rlpos) { 
fore checks, " 
if (tc >= vol->mft_zone_start) 
if (bmp_initial_pos >= 
if (zone_start == zone_end) { 
while loop."); 
if (zone == MFT_ZONE || mft_zone_size <= 0) { 
if (mft_zone_size > 0) 
if (vol->mft_zone_pos >= vol->mft_zone_end) { 
if (!vol->mft_zone_end) 
while loop.", 
while loop."); 
if (likely(rl)) { 
if (likely(page && !IS_ERR(page))) { 
if (likely(!err)) { 
if (rl) { 
if (err == -ENOSPC) 
if (err2) { 
if (err == -ENOSPC) 
for all clusters 
if this is a rollback operation 
ified, it is an active search context of @ni and its base mft 
ify @ctx as NULL and __ntfs_cluster_free() will 
form the necessary mapping and unmapping. 
fore returning.  Thus, @ctx will be left pointing to the same attribute on 
ifferent memory locations on return, so you must remember to reset any 
for internal use to rollback 
ify the runlist, so you have to 
if 'true' the @ctx 
for 
for writing on entry 
for writing and 
for i_ino 0x%lx, start_vcn 0x%llx, count " 
if not rolling back.  We 
for another use. 
if (likely(!is_rollback)) 
if (IS_ERR(rl)) { 
if (unlikely(rl->lcn < LCN_HOLE)) { 
if (count >= 0 && to_free > count) 
if (likely(rl->lcn >= 0)) { 
if (unlikely(err)) { 
if (count >= 0) 
for (; rl->length && count != 0; ++rl) { 
if (IS_ERR(rl)) { 
if (!is_rollback) 
if (unlikely(rl->lcn < LCN_HOLE)) { 
if (count >= 0 && to_free > count) 
if (likely(rl->lcn >= 0)) { 
if (unlikely(err)) { 
if (count >= 0) 
if (likely(!is_rollback)) 
if (is_rollback) 
if (!real_freed) { 
if that succeeds just return the error code. 
if (delta < 0) { 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/bitmap.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifdef NTFS_RW 
if 'true' this is a rollback operation 
for internal use to rollback 
for i_ino 0x%lx, start_bit 0x%llx, count 0x%llx, " 
for the pages containing the first and last 
if (IS_ERR(page)) { 
ify the appropriate bits in it. */ 
while ((bit & 7) && cnt) { 
if (value) 
if (!cnt) 
ify all remaining whole bytes in the page up 
if (cnt < 8) 
while (index < end_index) { 
if (IS_ERR(page)) 
ify all remaining whole bytes in the 
ify the appropriate bits in it.  Note, @len is the 
if (cnt) { 
while (bit--) { 
ified 
if (is_rollback) 
if (count != cnt) 
if (!pos) { 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/mft.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ific mft record resides 
if that is true PTR_ERR() 
for 
if the volume was that big... 
for $MFT's data. */ 
if (unlikely(index >= end_index)) { 
if (likely(!IS_ERR(page))) { 
if (likely(ntfs_is_mft_recordp((le32*)(page_address(page) + 
while waiting 
fore being 
if 
formed, the page gets PG_uptodate set and PG_locked cleared (this is done 
if 
while we are accessing it. 
fore I/O can proceed. In that case we 
if that is true, PTR_ERR() will return 
for setting the mft record dirty before calling 
ified the mft record... 
for mft_no 0x%lx.", ni->mft_no); 
if (likely(!IS_ERR(m))) 
ific mft record resides 
if highmem is not configured. 
for others to get hold of. We also release the ntfs 
ified the mft record, it is imperative to set the mft 
for mft_no 0x%lx.", ni->mft_no); 
if IS_ERR(result) is false.  Otherwise 
if this extent inode has already been added to the base inode, 
fore returning it. 
if (base_ni->nr_extents > 0) { 
for (i = 0; i < base_ni->nr_extents; i++) { 
if (likely(ni != NULL)) { 
if (likely(!IS_ERR(m))) { 
if (likely(le16_to_cpu(m->sequence_number) == seq_no)) { 
if (unlikely(!ni)) { 
if (IS_ERR(m)) { 
ify the sequence number if it is present. */ 
if needed. */ 
if (unlikely(!tmp)) { 
if (base_ni->nr_extents) { 
if (destroy_ni) 
ifdef NTFS_RW 
ified, 
fore, a 
for inode 0x%lx.", ni->mft_no); 
if (likely(ni->nr_extents >= 0)) 
forge.net and say that you saw " 
for use at umount time when the mft mirror inode has 
while the mft mirror 
if true, wait for i/o completion 
form synchronous i/o and ignore the @sync parameter. 
for inode 0x%lx.", mft_no); 
if (unlikely(!vol->mftmirr_ino)) { 
if (likely(!err)) 
if (IS_ERR(page)) { 
if not present. */ 
while (bh); 
if (block_end <= m_start) 
if (unlikely(block_start >= m_end)) 
if it is not mapped already. */ 
if (!rl) { 
while (rl->length && rl[1].vcn <= vcn) 
if (likely(lcn >= 0)) { 
while (block_start = block_end, (bh = bh->b_this_page) != head); 
if (likely(!err)) { 
for (i_bhs = 0; i_bhs < nr_bhs; i_bhs++) { 
if (!trylock_buffer(tbh)) 
for (i_bhs = 0; i_bhs < nr_bhs; i_bhs++) { 
if (unlikely(!buffer_uptodate(tbh))) { 
if (unlikely(err)) */ { 
for (i_bhs = 0; i_bhs < nr_bhs; i_bhs++) 
if (likely(!err)) { 
while writing mft mirror " 
if true, wait for i/o completion 
if the ntfs inode @ni is dirty and the first 
form synchronous i/o and ignore the @sync parameter. 
for i/o completion, and only then write 
for completion 
if you asked 
for asynchronous writing you probably do not care about that anyway. 
for inode 0x%lx.", ni->mft_no); 
if needed. 
for the mft record @m and the page it is in. 
if (!NInoTestClearDirty(ni)) 
if (block_end <= m_start) 
if (unlikely(block_start >= m_end)) 
if (block_start == m_start) { 
if (!buffer_dirty(bh)) { 
if it is not mapped already. */ 
if (!rl) { 
while (rl->length && rl[1].vcn <= vcn) 
if (likely(lcn >= 0)) { 
while (block_start = block_end, (bh = bh->b_this_page) != head); 
if (!nr_bhs) 
if (unlikely(err)) 
if (err) { 
for (i_bhs = 0; i_bhs < nr_bhs; i_bhs++) { 
if (!trylock_buffer(tbh)) 
if not @sync. */ 
for (i_bhs = 0; i_bhs < nr_bhs; i_bhs++) { 
if (unlikely(!buffer_uptodate(tbh))) { 
if (PageUptodate(page)) 
if (sync && ni->mft_no < vol->mftmirr_size) 
if (unlikely(err)) { 
while writing mft record " 
for (i_bhs = 0; i_bhs < nr_bhs; i_bhs++) 
if (err == -ENOMEM) { 
if an mft record may be written out 
if one is returned 
for unlocking the ntfs inode and unpinning the base 
if the mft record may be written out and 'false' if not. 
form: 
ify the base mft record because Windows 
formation attribute is not in the base 
if we find the lock was already taken, it is not safe to write the mft 
for the lock 
fore the page is locked but we already have the page locked here 
form further checks. 
if the inode 
if it is.  If it is not, we can safely write it and return 'true'. 
for the extent mft record.  We check if it has an 
for the extent mft record is attached to the base inode so we 
for actually writing dirty mft records here and not just 
ified without them ever having actual inodes in memory.  Also we can have 
if we only 
format().  The clean inode can then 
for inode 0x%lx.", mft_no); 
if the inode corresponding to this mft record is in the VFS 
for inode 0x%lx in icache.", mft_no); 
for it rather often. 
if (!mft_no) { 
for the 
if (vi) { 
if (NInoDirty(ni)) { 
if (unlikely(!mutex_trylock(&ni->mrec_lock))) { 
while we hold the mft record lock so 
if it is not a mft record (type "FILE"). */ 
if it is a base inode. */ 
if the inode corresponding to 
for base " 
if (!na.mft_no) { 
if (!vi) { 
if it has the extent inode 
if (ni->nr_extents <= 0) { 
for (eni = NULL, i = 0; i < ni->nr_extents; ++i) { 
if (!eni) { 
if (unlikely(!mutex_trylock(&eni->mrec_lock))) { 
if (NInoTestClearDirty(eni)) 
while we hold the mft record lock so return 
for a free mft record 
for a free mft record in the mft bitmap attribute on the ntfs volume 
for writing. 
for free mft record in the currently " 
if (pass_end > ll) 
if (!base_ni) 
if (data_pos < 24) 
if (data_pos >= pass_end) { 
formatted volume. */ 
for (; pass <= 2;) { 
if (size > ll) 
for a zero bit. 
if (size) { 
if (IS_ERR(page)) { 
fore inner for loop: size 0x%x, " 
for (; bit < size && data_pos + bit < pass_end; 
if (*byte == 0xff) 
if (b < 8 && b >= (bit & 7)) { 
if (unlikely(ll > (1ll << 32))) { 
for loop: size 0x%x, " 
for a zero bit. 
if (data_pos < pass_end) 
if (++pass == 2) { 
if (data_pos >= pass_end) 
for writing. 
fore returning. 
fore returning. 
if (unlikely(IS_ERR(rl) || !rl->length || rl->lcn < 0)) { 
if (!IS_ERR(rl)) 
if (IS_ERR(page)) { 
if (*b != 0xff && !(*b & tb)) { 
if (IS_ERR(rl2)) { 
for " 
if (IS_ERR(rl)) { 
for mft " 
if (ntfs_cluster_free_from_rl(vol, rl2)) { 
for (; rl[1].length; rl++) 
if (IS_ERR(mrec)) { 
if (unlikely(!ctx)) { 
if (unlikely(ret)) { 
if (ret == -ENOENT) 
for the previous last allocated cluster of mft bitmap. */ 
if (ll >= rl2->vcn) 
for the new mapping pairs array for this extent. */ 
if (unlikely(mp_size <= 0)) { 
for mapping pairs failed for " 
if (!ret) 
if necessary. */ 
if (unlikely(ret)) { 
for mft bitmap attribute."); 
if none of 
if (unlikely(ret)) { 
for " 
if (a->data.non_resident.lowest_vcn) { 
if (unlikely(ret)) { 
if (ntfs_attr_lookup(mftbmp_ni->type, mftbmp_ni->name, 
if (status.added_cluster) { 
if (status.added_run) { 
if (ntfs_bitmap_clear_bit(vol->lcnbmp_ino, lcn)) { 
if (status.mp_rebuilt) { 
if (ntfs_attr_record_resize(ctx->mrec, a, old_alen)) { 
if (ctx) 
if (!IS_ERR(mrec)) 
for writing. 
if (IS_ERR(mrec)) { 
if (unlikely(!ctx)) { 
if (unlikely(ret)) { 
if (ret == -ENOENT) 
fore filling the space 
if (mftbmp_ni->initialized_size > old_data_size) { 
if (likely(!ret)) { 
if (IS_ERR(mrec)) { 
if (unlikely(!ctx)) { 
if (ntfs_attr_lookup(mftbmp_ni->type, mftbmp_ni->name, 
if (i_size_read(mftbmp_vi) != old_data_size) { 
ifdef DEBUG 
if /* DEBUG */ 
if not enough space for this by one mft record worth 
for writing. 
fore returning. 
fore returning. 
if (unlikely(IS_ERR(rl) || !rl->length || rl->lcn < 0)) { 
if (!IS_ERR(rl)) 
if (!min_nr) 
if (!nr) 
if (unlikely((ll + (nr << vol->cluster_size_bits)) >> 
if (unlikely((ll + (nr << vol->cluster_size_bits)) >> 
if (likely(!IS_ERR(rl2))) 
if (PTR_ERR(rl2) != -ENOSPC || nr == min_nr) { 
for the " 
fore failing. 
while (1); 
if (IS_ERR(rl)) { 
for mft data " 
if (ntfs_cluster_free_from_rl(vol, rl2)) { 
for (; rl[1].length; rl++) 
if (IS_ERR(mrec)) { 
if (unlikely(!ctx)) { 
if (unlikely(ret)) { 
if (ret == -ENOENT) 
for the previous last allocated cluster of mft bitmap. */ 
if (ll >= rl2->vcn) 
for the new mapping pairs array for this extent. */ 
if (unlikely(mp_size <= 0)) { 
for mapping pairs failed for " 
if (!ret) 
if necessary. */ 
if (unlikely(ret)) { 
for mft data attribute."); 
if (unlikely(ret)) { 
if (a->data.non_resident.lowest_vcn) { 
if (unlikely(ret)) { 
if (ntfs_attr_lookup(mft_ni->type, mft_ni->name, mft_ni->name_len, 
if (ntfs_cluster_free(mft_ni, old_last_vcn, -1, ctx) < 0) { 
if (ntfs_rl_truncate_nolock(vol, &mft_ni->runlist, old_last_vcn)) { 
if (mp_rebuilt && !IS_ERR(ctx->mrec)) { 
if (ntfs_attr_record_resize(ctx->mrec, a, old_alen)) { 
if (IS_ERR(ctx->mrec)) { 
if (ctx) 
if (!IS_ERR(mrec)) 
ifying the mft record number 
ified in NTFS 3.1 so we need to know which volume version this mft 
for mft record 0x%llx.", (long long)mft_no); 
if (vol->major_ver < 3 || (vol->major_ver == 3 && !vol->minor_ver)) 
ific fields while we know that the 
if (vol->mft_record_size >= NTFS_BLOCK_SIZE) 
forge.net stating " 
ified filesystem created was corrupt.  " 
for the termination attribute). 
format - format an mft record on an ntfs volume 
format 
format(const ntfs_volume *vol, const s64 mft_no) 
for mft record 0x%llx.", (long long)mft_no); 
for $MFT's data. */ 
if (unlikely(index >= end_index)) { 
format non-existing mft " 
if (IS_ERR(page)) { 
format 0x%llx.", (long long)mft_no); 
if (unlikely(err)) { 
if an inode is in icache and so on but this is 
if want a file or directory, i.e. base inode or 0 
for extent inodes. 
for a zero bit.  To 
form wrap around 
for storing extension mft records 
for 
while Windows can still create up to 8 small files.  We can start 
if cluster size is above 16kiB.  If there 
if cluster size is above the mft record size. 
for it and return it to the caller, unless 
for use by normal files. 
if required.  The bitmap data size has to be at least equal to the 
if necessary, 
for the allocated mft record, and we will have 
for example for attribute resizing, etc, because when the run list overflows 
formation contained inside them, as 
for finding the mft records, but on 
formatted = false; 
for " 
if (mode) { 
if (!S_ISREG(mode) && !S_ISDIR(mode)) 
if (bit >= 0) { 
if (bit != -ENOSPC) { 
if (old_data_initialized << 3 > ll && old_data_initialized > 3) { 
if (bit < 24) 
if (unlikely(bit >= (1ll << 32))) 
if (unlikely(bit >= (1ll << 32))) 
fore extension: allocated_size 0x%llx, " 
if (old_data_initialized + 8 > old_data_size) { 
if (unlikely(err)) { 
ifdef DEBUG 
if /* DEBUG */ 
if necessary and fill the new space with 
if (unlikely(err)) { 
ifdef DEBUG 
if /* DEBUG */ 
if (unlikely(err)) { 
for writing until all 
for mft bitmap and mft record allocation done 
if (ll <= old_data_initialized) { 
formatted volume is 
fore extension: " 
while (ll > mft_ni->allocated_size) { 
if (unlikely(err)) { 
formatting the mft records allong the way. 
format().  We will update the attribute 
while (ll > mft_ni->initialized_size) { 
if (new_initialized_size > i_size_read(vol->mft_ino)) 
format(vol, mft_no); 
format mft record."); 
formatted = true; 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
if (IS_ERR(page)) { 
formatted the mft record no need to do it again. */ 
if (ntfs_is_file_record(m->magic) && 
format the mft record, preserving the 
if it is not zero or -1 (0xffff).  This 
if (unlikely(err)) { 
if (seq_no) 
if (usn && le16_to_cpu(usn) != 0xffff) 
if (S_ISDIR(mode)) 
if (base_ni) { 
for the new mft record, 
if (IS_ERR(m_tmp)) { 
if (unlikely(!vi)) { 
for checking whether an inode has changed w.r.t. a 
ific part of @vi. */ 
if (S_ISDIR(mode)) { 
if (vol->cluster_size <= ni->itype.index.block_size) { 
if (IS_RDONLY(vi)) 
formation attribute yet.  Also, there is no need 
for the superblock. */ 
if (ntfs_bitmap_clear_bit(vol->mftbmp_ino, bit)) { 
if something is wrong with it as long as it is properly detached 
for extent inode 0x%lx, base inode 0x%lx.\n", 
if (atomic_read(&ni->count) > 2) { 
for (i = 0; i < base_ni->nr_extents; i++) { 
if (unlikely(err)) { 
if it is not zero. */ 
if (seq_no == 0xffff) 
if (seq_no) 
if (unlikely(err)) { 
if (unlikely(err)) { 
if (!(base_ni->nr_extents & 3)) { 
if (unlikely(!extent_nis)) { 
if (base_ni->nr_extents) { 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/runlist.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
if (likely((dst != src) && (size > 0))) 
if (likely(size > 0)) 
for runlists 
for 
ifferent number of pages in 
if (old_size == new_size) 
if (unlikely(!new_rl)) 
if (likely(rl != NULL)) { 
for runlists 
for 
for as long as it takes to complete the allocation. 
ifferent number of pages in 
if (old_size == new_size) 
if (likely(rl != NULL)) { 
if two runlists can be joined together 
for mergeability with @dst 
if they are misaligned. */ 
if ((dst->vcn + dst->length) != src->vcn) 
if ((dst->lcn >= 0) && (src->lcn >= 0) && 
if ((dst->lcn == LCN_HOLE) && (src->lcn == LCN_HOLE)) 
if they can be merged 
if necessary. Adjust the size of the hole before the 
fore returning so you cannot use 
ified. The following 
if the right hand end needs merging. */ 
if we merged. */ 
if (IS_ERR(dst)) 
ifying the 
if necessary. */ 
if (dst[marker].lcn == LCN_ENOENT) 
fore this element in @dst 
if necessary. Adjust the size of the hole 
fore returning so you cannot use 
ified. The following 
if (loc == 0) 
if (left) 
if we merged, plus 
if (IS_ERR(dst)) 
ifying the 
if (left) 
if @left, then the first run in @src has 
if (dst[marker].lcn == LCN_HOLE || dst[marker].lcn == LCN_RL_NOT_MAPPED) 
if (disc) { 
if necessary. 
fore returning so you cannot use 
ified. The following 
if the left and right ends need merging. */ 
if (loc > 0) 
if the left, right, or both 
for the run being replaced. 
if (delta > 0) { 
if (IS_ERR(dst)) 
ifying the 
if necessary. */ 
if (left) 
for the runs to be copied from @src, i.e. the first 
if @right, then one of @dst's runs is 
if @left, then the first run in @src has 
if (dsize - tail > 0 && dst[marker].lcn == LCN_ENOENT) 
fore returning so you cannot use 
ified. The following 
if (IS_ERR(dst)) 
ifying the 
fore returning so you cannot use 
ified. The following 
ifdef DEBUG 
if 
for silly calling... */ 
if (IS_ERR(srl) || IS_ERR(drl)) 
for the case where the first mapping is being done now. */ 
if necessary. */ 
for (dend = 0; likely(drl[dend].length); dend++) 
if (IS_ERR(drl)) 
while (srl[si].length && srl[si].lcn < LCN_HOLE) 
forward in @drl until we reach the position where @srl needs to 
for (; drl[di].length; di++) { 
for illegal overlaps. */ 
for (send = si; srl[send].length; send++) 
for (dend = di; drl[dend].length; dend++) 
if (srl[send].lcn == LCN_ENOENT) 
for (sfinal = send; sfinal >= 0 && srl[sfinal].lcn < LCN_HOLE; sfinal--) 
for (dfinal = dend; dfinal >= 0 && drl[dfinal].lcn < LCN_HOLE; dfinal--) 
if (finish && !drl[dins].length) 
if (marker && (drl[dins].vcn + drl[dins].length > srl[send - 1].vcn)) 
if 0 
if 
if (finish) 
if (finish) 
if (IS_ERR(drl)) { 
if (marker) { 
for (ds = dend; drl[ds].length; ds++) 
if @srl ended after @drl. */ 
if (drl[ds].vcn == marker_vcn) { 
fore adding the 
if (drl[ds].lcn == LCN_ENOENT) { 
if (drl[ds].lcn != LCN_RL_NOT_MAPPED) { 
if (!slots) { 
if it isn't set already. */ 
if (!slots) 
ified in that case. 
if that is possible (we check for overlap and discard the new 
fore returning ERR_PTR(-ERANGE)). 
ifdef DEBUG 
if (!attr || !attr->non_resident || sle64_to_cpu( 
if 
if (unlikely(buf < (u8*)attr || buf > attr_end)) { 
if (!vcn && !*buf) 
if (unlikely(!rl)) 
if necessary. */ 
while (buf < attr_end && *buf) { 
if needed, including space for the 
if (((rlpos + 3) * sizeof(*old_rl)) > rlsize) { 
if (unlikely(!rl2)) { 
if (b) { 
for (deltaxcn = (s8)buf[b--]; b; b--) 
if (unlikely(deltaxcn < 0)) { 
for 
if (!(*buf & 0xf0)) 
if (buf + b > attr_end) 
for (deltaxcn = (s8)buf[b--]; b > b2; b--) 
ifdef DEBUG 
ified ourselves 
if either is found give us a message so we 
if (vol->major_ver < 3) { 
if (unlikely(lcn == (LCN)-1)) 
if 
if (unlikely(lcn < (LCN)-1)) { 
if (unlikely(buf >= attr_end)) 
ified, it must be equal to the final 
if (unlikely(deltaxcn && vcn - 1 != deltaxcn)) { 
if this is the base extent. */ 
if (deltaxcn) { 
ifference between the highest_vcn and 
if (deltaxcn < max_cluster) { 
if (unlikely(deltaxcn > max_cluster)) { 
ified, we are done. */ 
for overlaps. */ 
if (likely(!IS_ERR(old_rl))) 
for conversion 
for reading or writing). 
if 
if (unlikely(!rl)) 
if (unlikely(vcn < rl[0].vcn)) 
for (i = 0; likely(rl[i].length); i++) { 
if (likely(rl[i].lcn >= (LCN)0)) 
if (likely(rl[i].lcn < (LCN)0)) 
ifdef NTFS_RW 
if @rl is NULL or @vcn is in an unmapped part/out of bounds of 
if (unlikely(!rl || vcn < rl[0].vcn)) 
while (likely(rl->length)) { 
if (likely(rl->lcn >= LCN_HOLE)) 
if (likely(rl->lcn == LCN_ENOENT)) 
ificant_bytes - get number of bytes needed to store a number 
for which to get the number of bytes for 
ific run length. 
ificant_bytes(const s64 n) 
while (l != 0 && l != -1); 
if ((n < 0 && j >= 0) || (n > 0 && j < 0)) 
for_mapping_pairs - get bytes needed for mapping pairs array 
for example allows us to allocate a buffer of the right size when 
for the single terminator byte). 
for reading or writing), it 
for_mapping_pairs(const ntfs_volume *vol, 
if (!rl) { 
while (rl->length && first_vcn >= rl[1].vcn) 
if (unlikely((!rl->length && first_vcn > rl->vcn) || 
if present. */ 
if (unlikely(length < 0 || rl->lcn < LCN_HOLE)) 
if (unlikely(last_vcn >= 0 && rl[1].vcn > last_vcn)) { 
if (unlikely(rl[1].vcn > s1)) 
ificant_bytes(length - delta); 
if (likely(rl->lcn >= 0 || vol->major_ver < 3)) { 
if (likely(rl->lcn >= 0)) 
ificant_bytes(prev_lcn); 
for (; rl->length && !the_end; rl++) { 
if (unlikely(length < 0 || rl->lcn < LCN_HOLE)) 
if (unlikely(last_vcn >= 0 && rl[1].vcn > last_vcn)) { 
if (unlikely(rl[1].vcn > s1)) 
ificant_bytes(length); 
if (likely(rl->lcn >= 0 || vol->major_ver < 3)) { 
ificant_bytes(rl->lcn - 
if (rl->lcn == LCN_RL_NOT_MAPPED) 
ificant_bytes - write the significant bytes of a number 
for bounds checking 
ify @n unambiguously as a signed number, taking care not to exceed 
ific run length to the minimum 
ificant_bytes(s8 *dst, const s8 *dst_max, 
if (unlikely(dst > dst_max)) 
while (l != 0 && l != -1); 
if (n < 0 && j >= 0) { 
if (n > 0 && j < 0) { 
for the ntfs version) 
for which to build the mapping pairs array 
for_mapping_pairs(). 
if @stop_vcn is not NULL, *@stop_vcn is set to 
for reading or writing), it 
if (!rl) { 
if (stop_vcn) 
while (rl->length && first_vcn >= rl[1].vcn) 
if (unlikely((!rl->length && first_vcn > rl->vcn) || 
for bounds checking in 
if present. */ 
if (unlikely(length < 0 || rl->lcn < LCN_HOLE)) 
if (unlikely(last_vcn >= 0 && rl[1].vcn > last_vcn)) { 
if (unlikely(rl[1].vcn > s1)) 
ificant_bytes(dst + 1, dst_max, 
if (unlikely(len_len < 0)) 
if (likely(rl->lcn >= 0 || vol->major_ver < 3)) { 
if (likely(rl->lcn >= 0)) 
ificant_bytes(dst + 1 + 
if (unlikely(lcn_len < 0)) 
if (unlikely(dst_next > dst_max)) 
for (; rl->length && !the_end; rl++) { 
if (unlikely(length < 0 || rl->lcn < LCN_HOLE)) 
if (unlikely(last_vcn >= 0 && rl[1].vcn > last_vcn)) { 
if (unlikely(rl[1].vcn > s1)) 
ificant_bytes(dst + 1, dst_max, 
if (unlikely(len_len < 0)) 
if (likely(rl->lcn >= 0 || vol->major_ver < 3)) { 
ificant_bytes(dst + 1 + 
if (unlikely(lcn_len < 0)) 
if (unlikely(dst_next > dst_max)) 
if (stop_vcn) 
if (rl->lcn == LCN_RL_NOT_MAPPED) 
ified vcn 
for error output) 
if @new_length is 
if the last runlist element is a sparse 
for unmapped runlist elements.  It is assumed that 
for writing. 
for new_length 0x%llx.", (long long)new_length); 
if (!new_length) { 
if (rl) 
if (unlikely(!rl)) { 
if (unlikely(!rl)) { 
while (likely(rl->length && new_length >= rl[1].vcn)) 
if (rl->length) { 
while (likely(trl->length)) 
if (rl->length) { 
if (!rl->length) 
if necessary. */ 
if (IS_ERR(rl)) 
if (likely(/* !rl->length && */ new_length > rl->vcn)) { 
if ((rl > runlist->rl) && ((rl - 1)->lcn == LCN_HOLE)) 
if necessary. */ 
if (IS_ERR(rl)) { 
fore in the old runlist. 
if (unlikely(!rl->length && new_length == rl->vcn)) */ { 
for error output) 
ified. 
for writing. 
for start 0x%llx, length 0x%llx.", 
if (unlikely(!rl)) { 
while (likely(rl->length && start >= rl[1].vcn)) 
while (likely(rl_end->length && end >= rl_end[1].vcn)) { 
if (unlikely(rl_end->lcn < LCN_HOLE)) 
if (unlikely(rl_end->length && rl_end->lcn < LCN_HOLE)) 
if (!rl_end->length && end > rl_end->vcn) 
if (!length) 
if (!rl->length) 
while (likely(rl_real_end->length)) 
if (rl->lcn == LCN_HOLE) { 
if (end <= rl[1].vcn) { 
if (rl_end->lcn == LCN_HOLE) { 
if (rl < rl_end) 
if necessary. */ 
if it is real. */ 
if the allocation changed. */ 
if (IS_ERR(rl)) 
if (start == rl->vcn) { 
if (rl > runlist->rl && (rl - 1)->lcn == LCN_HOLE) { 
if (end >= rl[1].vcn) { 
for the 
for the remaining non-sparse 
if (IS_ERR(trl)) 
if (runlist->rl != trl) { 
ift all the runs up by one. */ 
if it is real. */ 
if (rl_end->lcn == LCN_HOLE) { 
if (rl < rl_end) 
if @end is in the next run need to split the run into a sparse 
if (end >= rl[1].vcn) { 
if (rl[1].length && end >= rl[2].vcn) { 
if (IS_ERR(trl)) 
if (runlist->rl != trl) { 
if (rl->lcn >= 0) { 
for the non-sparse 
for the remaining 
if (IS_ERR(trl)) 
if (runlist->rl != trl) { 
ift all the runs up by two. */ 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/aops.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for reading attributes 
for reading pages belonging to the 
form the post read mst fixups when all IO on the 
if (likely(uptodate)) { 
if (unlikely(init_size > i_size)) { 
for the current buffer head overflowing. */ 
if (file_ofs < init_size) 
if (!buffer_uptodate(tmp)) 
if (buffer_async_read(tmp)) { 
while (tmp != bh); 
if the 
if (!NInoMstProtected(ni)) { 
ified before we got here... */ 
for (i = 0; i < recs; i++) 
if (likely(page_uptodate && !PageError(page))) 
if required, automatically 
fore finally marking it uptodate and 
force allocated_size limit because i_size is checked for in 
if (!page_has_buffers(page)) { 
if (unlikely(!page_has_buffers(page))) { 
for the whole 
if the page is being 
if (unlikely(init_size > i_size)) { 
if (unlikely(buffer_uptodate(bh))) 
if (unlikely(buffer_mapped(bh))) { 
if (iblock < lblock) { 
if (!rl) { 
if (likely(rl != NULL)) { 
while (rl->length && rl[1].vcn <= vcn) 
if (lcn >= 0) { 
if (iblock < zblock) { 
if (lcn == LCN_HOLE) 
if (!is_retry && lcn == LCN_RL_NOT_MAPPED) { 
for 
if (likely(!err)) 
if (!rl) 
for example. 
if (err == -ENOENT || lcn == LCN_ENOENT) { 
if (!err) 
if (likely(!err)) 
while (i++, iblock++, (bh = bh->b_this_page) != head); 
if (rl) 
for i/o. */ 
for (i = 0; i < nr; i++) { 
for (i = 0; i < nr; i++) { 
if (likely(!buffer_uptodate(tbh))) 
if (likely(!PageError(page))) 
if the mft record is not cached at this point in time, we need to wait 
for it to be read in before we can do the copy. 
if (unlikely(page->index >= (i_size + PAGE_CACHE_SIZE - 1) >> 
if (PageUptodate(page)) { 
for 
if (ni->type != AT_INDEX_ALLOCATION) { 
if (NInoEncrypted(ni)) { 
if (NInoNonResident(ni) && NInoCompressed(ni)) { 
if (NInoNonResident(ni)) { 
if it is resident the actual data is not compressed so we are 
if (unlikely(page->index > 0)) { 
if (!NInoAttr(ni)) 
if (IS_ERR(mrec)) { 
if (unlikely(NInoNonResident(ni))) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) 
if (unlikely(attr_len > ni->initialized_size)) 
if (unlikely(attr_len > i_size)) { 
ifdef NTFS_RW 
for writing pages belonging to non-resident, non-mst 
for the 
for example.) 
for inode 0x%lx, attribute type 0x%x, page index " 
if (!page_has_buffers(page)) { 
if (unlikely(!page_has_buffers(page))) { 
for_writepage(wbc, page); 
ifferent naming scheme to ntfs_read_block()! */ 
for the data size. */ 
if (unlikely(block >= dblock)) { 
fore we get here, 
if (!buffer_dirty(bh)) 
if (unlikely((block >= iblock) && 
if (block > iblock) { 
for each page do: 
for each page do: 
if the page is uptodate. 
fore. 
if (!PageUptodate(page)) { 
if (buffer_mapped(bh)) 
if (!rl) { 
if (likely(rl != NULL)) { 
while (rl->length && rl[1].vcn <= vcn) 
if (lcn >= 0) { 
if (lcn == LCN_HOLE) { 
if the buffer is zero. */ 
if (unlikely(*bpos)) 
while (likely(++bpos < bend)); 
if (bpos == bend) { 
if (!is_retry && lcn == LCN_RL_NOT_MAPPED) { 
for 
if (likely(!err)) 
if (!rl) 
if (err == -ENOENT || lcn == LCN_ENOENT) { 
if (!err) 
while (block++, (bh = bh->b_this_page) != head); 
if (rl) 
if (unlikely(!PageUptodate(page))) { 
if (!buffer_uptodate(bh)) { 
while ((bh = bh->b_this_page) != head); 
for async write i/o. */ 
if (buffer_mapped(bh) && buffer_dirty(bh)) { 
if (test_clear_buffer_dirty(bh)) { 
if (unlikely(err)) { 
if (err != -ENOMEM) 
while ((bh = bh->b_this_page) != head); 
if (unlikely(err == -EOPNOTSUPP)) 
if (err == -ENOMEM) { 
for_writepage(wbc, page); 
for i/o. */ 
if (buffer_async_write(bh)) { 
while (bh != head); 
if (unlikely(need_end_writeback)) 
for writing pages belonging to non-resident, mst protected 
for the index allocation case. 
if we were to unlock the 
fore undoing the fixups, any other user of the page will see the 
for the duration of the function to ensure 
for inode 0x%lx, attribute type 0x%x, page index " 
if a page 
for now. 
for sync purposes? */ 
for the data size. */ 
if (likely(block < rec_block)) { 
if (!rec_is_dirty) 
if (unlikely(err2)) { 
if (block == rec_block) */ { 
if (unlikely(block >= dblock)) { 
if (!buffer_dirty(bh)) { 
if it is not mapped already. */ 
if (!rl) { 
if (likely(rl != NULL)) { 
while (rl->length && rl[1].vcn <= vcn) 
if (likely(lcn >= 0)) { 
if (!is_mft && !is_retry && 
for the duration. 
if (likely(!err2)) 
if (err2 == -ENOMEM) 
if (!rl) 
if (!err || err == -ENOMEM) 
if not error -ENOMEM. 
if (rec_start_bh != bh) { 
while (bhs[--nr_bhs] != rec_start_bh) 
if (err2 != -ENOMEM) { 
while ((rec_start_bh = 
while (block++, (bh = bh->b_this_page) != head); 
if (!nr_bhs) 
for (i = 0; i < nr_bhs; i++) { 
if (i % bhs_per_rec) 
if (is_mft) { 
if (!ntfs_may_write_mft_record(vol, mft_no, 
fore 
while (++i % bhs_per_rec); 
if (tni) 
if (unlikely(err2)) { 
while (++i % bhs_per_rec); 
if (!nr_recs) 
for (i = 0; i < nr_bhs; i++) { 
if (!tbh) 
if (!trylock_buffer(tbh)) 
if not @sync. */ 
for (i = 0; i < nr_bhs; i++) { 
if (!tbh) 
if (unlikely(!buffer_uptodate(tbh))) { 
while writing ntfs " 
if (!err || err == -ENOMEM) 
if (is_mft && sync) { 
for (i = 0; i < nr_bhs; i++) { 
if (i % bhs_per_rec) 
if (!tbh) 
if (mft_no < vol->mftmirr_size) 
if (!sync) 
for (i = 0; i < nr_bhs; i++) { 
if (!tbh) 
while (nr_locked_nis-- > 0) { 
if (tni->nr_extents >= 0) 
if (unlikely(err && err != -ENOMEM)) { 
if there is only one ntfs record in the page. 
if (ni->itype.index.block_size == PAGE_CACHE_SIZE) 
if (page_is_dirty) { 
for_writepage(wbc, page); 
if (likely(!err)) 
if necessary creates and writes the 
for the inode the mft record belongs to or via the 
if (unlikely(page->index >= (i_size + PAGE_CACHE_SIZE - 1) >> 
for 
if (ni->type != AT_INDEX_ALLOCATION) { 
if (NInoEncrypted(ni)) { 
if (NInoNonResident(ni) && NInoCompressed(ni)) { 
if (NInoNonResident(ni) && NInoSparse(ni)) { 
if (NInoNonResident(ni)) { 
if (page->index >= (i_size >> PAGE_CACHE_SHIFT)) { 
if (NInoMstProtected(ni)) 
if it is resident the actual data is not 
if (unlikely(page->index > 0)) { 
if (!NInoAttr(ni)) 
if (IS_ERR(m)) { 
if (unlikely(NInoNonResident(ni))) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) 
if (unlikely(attr_len > i_size)) { 
if (err == -ENOMEM) { 
for_writepage(wbc, page); 
if (ctx) 
if (m) 
if	/* NTFS_RW */ 
for inodes and attributes 
ifdef NTFS_RW 
if /* NTFS_RW */ 
for mst protecteed inodes 
ifdef NTFS_RW 
if /* NTFS_RW */ 
ifdef NTFS_RW 
if (unlikely(!page_has_buffers(page))) { 
if (likely(!page_has_buffers(page))) { 
while (bh); 
if (bh_ofs + bh_size <= ofs) 
if (unlikely(bh_ofs >= end)) 
while ((bh = bh->b_this_page) != head); 
if (unlikely(buffers_to_free)) { 
while (buffers_to_free); 
file : ./test/kernel/fs/ntfs/super.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
if (s) { 
if (!strcmp(s, "0") || !strcmp(s, "no") || 
for the ntfs volume described by @vol. 
if (!strcmp(p, option)) {					\ 
if (*v)						\ 
if (!strcmp(p, option)) {					\ 
if (*v)							\ 
if (!strcmp(p, option)) {					\ 
if (!v || !*v)						\ 
if (*v)							\ 
if (!uid_valid(variable))				\ 
if (!strcmp(p, option)) {					\ 
if (!v || !*v)						\ 
if (*v)							\ 
if (!gid_valid(variable))				\ 
if (!strcmp(p, option)) {					\ 
if (*v)							\ 
if (!strcmp(p, option)) {					\ 
if (!simple_getbool(v, &val))				\ 
if (!strcmp(p, option)) {					\ 
if (!v || !*v)						\ 
if (variable == -1)					\ 
for (_i = 0; opt_array[_i].str && *opt_array[_i].str; _i++) \ 
if (!opt_array[_i].str || !*opt_array[_i].str)		\ 
if (!opt || !*opt) 
while ((p = strsep(&opt, ","))) { 
if (!strcmp(p, "posix") || !strcmp(p, "show_inodes")) 
if (!strcmp(p, "nls") || !strcmp(p, "iocharset")) { 
if (!v || !*v) 
if (!nls_map) { 
if (!strcmp(p, "utf8")) { 
if (!v || !*v) 
if (!simple_getbool(v, &val)) 
if (val) { 
if (errors < INT_MAX) 
if (errors && !sloppy) 
if (sloppy) 
if (on_errors != -1) { 
if (nls_map) { 
if (!vol->nls_map) { 
if (!vol->nls_map) { 
if (mft_zone_multiplier != -1) { 
if (mft_zone_multiplier < 1 || mft_zone_multiplier > 4) { 
if (!vol->mft_zone_multiplier) 
if (on_errors != -1) 
if (!vol->on_errors || vol->on_errors == ON_ERRORS_RECOVER) 
if (uid_valid(uid)) 
if (gid_valid(gid)) 
if (fmask != (umode_t)-1) 
if (dmask != (umode_t)-1) 
if (show_sys_files != -1) { 
if (case_sensitive != -1) { 
if (disable_sparse != -1) { 
if (!NVolSparseEnabled(vol) && 
ifdef NTFS_RW 
formation flags 
for the volume information flags 
formation flags on the volume @vol with the value 
ify with the old flags and use 
if (vol->vol_flags == flags) 
if (IS_ERR(m)) { 
if (!ctx) { 
if (err) 
if (ctx) 
formation flags 
formation flags on the volume @vol. 
formation flags 
formation flags on the volume @vol. 
if /* NTFS_RW */ 
ifndef NTFS_RW 
force read-only flag. */ 
if we are remounting read-write, 
if the volume is not umounted 
if no volume errors 
if ((sb->s_flags & MS_RDONLY) && !(*flags & MS_RDONLY)) { 
if (NVolErrors(vol)) { 
if (vol->vol_flags & VOLUME_IS_DIRTY) { 
if (vol->vol_flags & VOLUME_MODIFIED_BY_CHKDSK) { 
if (vol->vol_flags & VOLUME_MUST_MOUNT_RO_MASK) { 
if (ntfs_set_volume_flags(vol, VOLUME_IS_DIRTY)) { 
formation flags%s", es); 
if 0 
ifferent between NTFS 1.2 and 3.x... 
if ((vol->major_ver > 1)) { 
if 
if (!ntfs_mark_quotas_out_of_date(vol)) { 
if (!ntfs_stamp_usnjrnl(vol)) { 
if (!(sb->s_flags & MS_RDONLY) && (*flags & MS_RDONLY)) { 
if (!NVolErrors(vol)) { 
formation " 
if /* NTFS_RW */ 
if (!parse_options(vol, opt)) 
if it is valid and 'false' if not. 
for warning/error output, i.e. it can be NULL when silent 
if this is the case. 
if ((void*)b < (void*)&b->checksum && b->checksum && !silent) { 
for (i = 0, u = (le32*)b; u < (le32*)(&b->checksum); ++u) 
if (le32_to_cpu(b->checksum) != i) 
ifier is "NTFS    " */ 
if (le16_to_cpu(b->bpb.bytes_per_sector) < 0x100 || 
if ((u32)le16_to_cpu(b->bpb.bytes_per_sector) * 
if (le16_to_cpu(b->bpb.reserved_sectors) || 
if ((u8)b->clusters_per_mft_record < 0xe1 || 
if ((u8)b->clusters_per_index_record < 0xe1 || 
for valid end of sector marker. We will work without it, but 
if (!silent && b->end_of_sector_marker != cpu_to_le16(0xaa55)) 
if true, suppress all output 
fore. 
if ((bh_primary = sb_bread(sb, 0))) { 
if (!silent) 
if (!silent) 
if (!(NTFS_SB(sb)->on_errors & ON_ERRORS_RECOVER)) { 
if (!silent) 
if ((bh_backup = sb_bread(sb, nr_blocks - 1))) { 
if (!silent) 
if ((bh_backup = sb_bread(sb, nr_blocks >> 1))) { 
if (!silent) 
if (!silent) 
if (bh_primary) 
if (bh_primary) { 
if the backup boot sector 
if (!(sb->s_flags & MS_RDONLY)) { 
if (buffer_uptodate(bh_primary)) { 
while " 
formation therein in 
if (vol->sector_size < vol->sb->s_blocksize) { 
if (vol->cluster_size < vol->sector_size) { 
if (clusters_per_mft_record > 0) 
if (vol->mft_record_size > PAGE_CACHE_SIZE) { 
if (vol->mft_record_size < vol->sector_size) { 
if (clusters_per_index_record > 0) 
if (vol->index_record_size < vol->sector_size) { 
for 64-bit-ness. 
if ((u64)ll >= 1ULL << 32) { 
if (sizeof(unsigned long) < 8) { 
for this architecture.  " 
if (ll >= vol->nr_clusters) { 
if (ll >= vol->nr_clusters) { 
ifdef NTFS_RW 
if (vol->cluster_size <= (4 << vol->mft_record_size_bits)) 
if /* NTFS_RW */ 
for which to setup the allocators 
ifdef NTFS_RW 
if /* NTFS_RW */ 
ifdef NTFS_RW 
ified NTFS volume (see mkntfs 
for the mft bitmap as well within the mft_zone itself. 
if (mft_lcn * vol->cluster_size < 16 * 1024) 
if (vol->mft_zone_start <= mft_lcn) 
while (vol->mft_zone_end >= vol->nr_clusters) { 
if /* NTFS_RW */ 
for a volume 
if (IS_ERR(tmp_ino) || is_bad_inode(tmp_ino)) { 
ifics about $MFTMirr's inode as 
for anyone. */ 
for $MFTMirr. */ 
for directories. 
if necessary. */ 
if (index) { 
if (IS_ERR(mft_page)) { 
if (IS_ERR(mirr_page)) { 
if it is not in use. */ 
if (ntfs_is_baad_recordp((le32*)kmft)) { 
if it is not in use. */ 
if (ntfs_is_baad_recordp((le32*)kmirr)) { 
if (bytes < sizeof(MFT_RECORD_OLD) || 
if (bytes < sizeof(MFT_RECORD_OLD) || 
if (memcmp(kmft, kmirr, bytes)) { 
while (++i < vol->mftmirr_size); 
for it. 
if (rl2[i].vcn != rl[i].vcn || rl2[i].lcn != rl[i].lcn || 
while (rl2[i++].length); 
for a volume 
if (IS_ERR(tmp_ino) || is_bad_inode(tmp_ino)) { 
if (!ntfs_check_logfile(tmp_ino, rp)) { 
if Windows is suspended on a volume 
if Windows is hibernated on the ntfs volume @vol.  This is done by 
for the file hiberfil.sys in the root directory of the volume.  If 
for now this should do fine. 
if 
for the above mentioned caveat of a 
if Windows is not hibernated on the volume, >0 if Windows is 
for the hibernation file by looking up the 
if (IS_ERR_MREF(mref)) { 
if (ret == -ENOENT) { 
for " 
for the type of match that was found. */ 
if (IS_ERR(vi) || is_bad_inode(vi)) { 
if (unlikely(i_size_read(vi) < NTFS_HIBERFIL_HEADER_SIZE)) { 
if (IS_ERR(page)) { 
if (*(le32*)kaddr == cpu_to_le32(0x72626968)/*'hibr'*/) { 
if (unlikely(*kaddr)) { 
while (++kaddr < kend); 
if present 
for the quota file by looking up the filename 
if (IS_ERR_MREF(mref)) { 
if (MREF_ERR(mref) == -ENOENT) { 
if they are 
for $Quota."); 
for the type of match that was found. */ 
if (IS_ERR(tmp_ino) || is_bad_inode(tmp_ino)) { 
if (IS_ERR(tmp_ino)) { 
if present 
for the transaction log file by looking up the 
if (IS_ERR_MREF(mref)) { 
if (MREF_ERR(mref) == -ENOENT) { 
if 
for " 
for the type of match that was found. */ 
if (unlikely(IS_ERR(tmp_ino) || is_bad_inode(tmp_ino))) { 
if (unlikely(vol->vol_flags & VOLUME_DELETE_USN_UNDERWAY)) { 
if (IS_ERR(tmp_ino)) { 
if (unlikely(i_size_read(tmp_ino) < sizeof(USN_HEADER))) { 
if (IS_ERR(tmp_ino)) { 
ify $J is non-resident and sparse. */ 
if (unlikely(!NInoNonResident(tmp_ni) || !NInoSparse(tmp_ni))) { 
if (IS_ERR(page)) { 
if (unlikely(sle64_to_cpu(uh->allocation_delta) > 
if (unlikely(sle64_to_cpu(uh->lowest_valid_usn) >= 
if (likely(sle64_to_cpu(uh->lowest_valid_usn) == 
if the volume does " 
for a volume 
if (IS_ERR(ino) || is_bad_inode(ino)) { 
if (i_size <= 0 || i_size > 0x7fffffff) 
if (!vol->attrdef) 
while (index < max_index) { 
if (IS_ERR(page)) 
if (size == PAGE_CACHE_SIZE) { 
if (size) 
if /* NTFS_RW */ 
for an ntfs volume 
if (IS_ERR(ino) || is_bad_inode(ino)) { 
if (!i_size || i_size & (sizeof(ntfschar) - 1) || 
if (!vol->upcase) 
while (index < max_index) { 
if (IS_ERR(page)) 
if (size == PAGE_CACHE_SIZE) { 
if (size) 
if (!default_upcase) { 
if (max > vol->upcase_len) 
for (i = 0; i < max; i++) 
if (i == max) { 
ified $UpCase matches default. Using " 
ified $UpCase since it does not match " 
if (default_upcase) { 
ifdef NTFS_RW 
if /* NTFS_RW */ 
ifdef NTFS_RW 
if (!load_and_init_mft_mirror(vol) || !check_mft_mirror(vol)) { 
if (!(sb->s_flags & MS_RDONLY)) { 
ified%s", 
if /* NTFS_RW */ 
if (IS_ERR(vol->mftbmp_ino)) { 
if (!load_and_init_upcase(vol)) 
ifdef NTFS_RW 
if (!load_and_init_attrdef(vol)) 
if /* NTFS_RW */ 
ify the size, no 
for any locking at this stage as we are already running 
if (IS_ERR(vol->lcnbmp_ino) || is_bad_inode(vol->lcnbmp_ino)) { 
if ((vol->nr_clusters + 7) >> 3 > i_size_read(vol->lcnbmp_ino)) { 
if (IS_ERR(vol->vol_ino) || is_bad_inode(vol->vol_ino)) { 
if (IS_ERR(m)) { 
if (!(ctx = ntfs_attr_get_search_ctx(NTFS_I(vol->vol_ino), m))) { 
if (ntfs_attr_lookup(AT_VOLUME_INFORMATION, NULL, 0, 0, 0, NULL, 0, 
if ((u8*)vi < (u8*)ctx->attr || (u8*)vi + 
if (vol->major_ver < 3 && NVolSparseEnabled(vol)) { 
ifdef NTFS_RW 
if (vol->vol_flags & VOLUME_MUST_MOUNT_RO_MASK) { 
ified by chkdsk"; 
if (vol->vol_flags & VOLUME_IS_DIRTY) 
if (vol->vol_flags & VOLUME_MODIFIED_BY_CHKDSK) { 
if (!(sb->s_flags & MS_RDONLY)) { 
ified%s", 
if the volume 
if (!load_and_check_logfile(vol, &rp) || 
if (!(sb->s_flags & MS_RDONLY)) { 
ified%s", 
if (vol->logfile_ino) { 
if /* NTFS_RW */ 
if (IS_ERR(vol->root_ino) || is_bad_inode(vol->root_ino)) { 
ifdef NTFS_RW 
if Windows is suspended to disk on the target volume.  If it 
if (unlikely(err)) { 
if (!(sb->s_flags & MS_RDONLY)) { 
ified%s", 
if (!(sb->s_flags & MS_RDONLY) && 
formation flags"; 
if (!(vol->on_errors & (ON_ERRORS_REMOUNT_RO | 
ified%s", 
if 0 
ifferent between NTFS 1.2 and 3.x... 
if (!(sb->s_flags & MS_RDONLY) && (vol->major_ver > 1) && 
if (!(vol->on_errors & (ON_ERRORS_REMOUNT_RO | 
ified%s", 
if 
if (!(sb->s_flags & MS_RDONLY) && 
if (!(vol->on_errors & (ON_ERRORS_REMOUNT_RO | 
ified%s", 
if /* NTFS_RW */ 
fore 3.0, we are done. */ 
ific initialization. */ 
if (IS_ERR(vol->secure_ino) || is_bad_inode(vol->secure_ino)) { 
if (IS_ERR(vol->extend_ino) || is_bad_inode(vol->extend_ino)) { 
ifdef NTFS_RW 
if (!load_and_init_quota(vol)) { 
if (!(sb->s_flags & MS_RDONLY)) { 
ified%s", 
if (!(sb->s_flags & MS_RDONLY) && 
if (!(vol->on_errors & (ON_ERRORS_REMOUNT_RO | 
ified%s", 
if present, check 
if (!load_and_init_usnjrnl(vol)) { 
if (!(sb->s_flags & MS_RDONLY)) { 
ified%s", 
if (!(sb->s_flags & MS_RDONLY) && !ntfs_stamp_usnjrnl(vol)) { 
if (!(vol->on_errors & (ON_ERRORS_REMOUNT_RO | 
ified%s", 
if /* NTFS_RW */ 
ifdef NTFS_RW 
if (vol->usnjrnl_j_ino) 
if (vol->usnjrnl_max_ino) 
if (vol->usnjrnl_ino) 
if (vol->quota_q_ino) 
if (vol->quota_ino) 
if /* NTFS_RW */ 
ifdef NTFS_RW 
if /* NTFS_RW */ 
if (vol->attrdef) { 
ifdef NTFS_RW 
if /* NTFS_RW */ 
if (vol->upcase == default_upcase) { 
if (vol->upcase) { 
ifdef NTFS_RW 
if /* NTFS_RW */ 
ific part of the 
ifdef NTFS_RW 
while they are still open in case some of them 
ific. */ 
if (vol->usnjrnl_j_ino) 
if (vol->usnjrnl_max_ino) 
if (vol->usnjrnl_ino) 
if (vol->quota_q_ino) 
if (vol->quota_ino) 
if (vol->extend_ino) 
if (vol->secure_ino) 
if (vol->logfile_ino) 
if (vol->mftmirr_ino) 
if (!(sb->s_flags & MS_RDONLY)) { 
if (ntfs_clear_volume_flags(vol, VOLUME_IS_DIRTY)) 
formation " 
if (vol->mftmirr_ino) 
if /* NTFS_RW */ 
ific clean up. */ 
ifdef NTFS_RW 
if (vol->usnjrnl_max_ino) { 
if (vol->usnjrnl_ino) { 
if (vol->quota_q_ino) { 
if (vol->quota_ino) { 
if /* NTFS_RW */ 
if (vol->secure_ino) { 
ifdef NTFS_RW 
if (vol->mftmirr_ino) { 
if /* NTFS_RW */ 
if (vol->attrdef) { 
if necessary.  Also decrease 
if (vol->upcase == default_upcase) { 
if (!ntfs_nr_upcase_users && default_upcase) { 
if (vol->cluster_size <= 4096 && !--ntfs_nr_compression_users) 
if (vol->upcase) { 
for which to obtain free cluster count 
if we have one 
for (index = 0; index < max_index; index++) { 
if necessary, and increment the use count. 
if (IS_ERR(page)) { 
for eventual bits outside logical ntfs volume (see function 
if (vol->nr_clusters & 63) 
if (nr_free < 0) 
for which to obtain free inode count 
for reading or writing. 
for (index = 0; index < max_index; index++) { 
if necessary, and increment the use count. 
if (IS_ERR(page)) { 
if (nr_free < 0) 
formation about mounted NTFS volume 
formation 
fore ntfs_statfs is 
if we run out and we can keep doing this until 
if (size < 0LL) 
if we 
ificant 32-bits in f_fsid[0] and the most significant 
ifdef NTFS_RW 
if 
ifdef NTFS_RW 
if /* NTFS_RW */ 
if errors are detected. This is used 
ifficult piece of bootstrap by reading the 
for this context 
while mounting NTFS. [The validator is still active 
ifndef NTFS_RW 
if /* ! NTFS_RW */ 
if (!vol) { 
while owner has full access. Further, files by 
if (!parse_options(vol, (char*)opt)) 
if (bdev_logical_block_size(sb->s_bdev) > PAGE_CACHE_SIZE) { 
if (blocksize < NTFS_BLOCK_SIZE) { 
if (!i_size_read(sb->s_bdev->bd_inode)) { 
if (!(bh = read_ntfs_boot_sector(sb, silent))) { 
if (!result) { 
for each sector 
for sure that it works. 
if (vol->sector_size > blocksize) { 
if (blocksize != vol->sector_size) { 
for the file size, i.e. correct would be: 
for the page cache and our address 
for $MFT which is sufficient to allow our normal inode 
if (!tmp_ino) { 
if (ntfs_read_inode_mount(tmp_ino) < 0) { 
if the cluster size is 
if (vol->cluster_size <= 4096 && !ntfs_nr_compression_users++) { 
if (result) { 
for compression engine."); 
if necessary.  Also 
if (!default_upcase) 
if (!load_system_files(vol)) { 
if ((sb->s_root = d_make_root(vol->root_ino))) { 
if it has no users. */ 
if (!--ntfs_nr_upcase_users && default_upcase) { 
ific clean up. */ 
ifdef NTFS_RW 
if (vol->usnjrnl_max_ino) { 
if (vol->usnjrnl_ino) { 
if (vol->quota_q_ino) { 
if (vol->quota_ino) { 
if /* NTFS_RW */ 
if (vol->secure_ino) { 
ifdef NTFS_RW 
if (vol->mftmirr_ino) { 
if /* NTFS_RW */ 
if (vol->attrdef) { 
if (vol->upcase == default_upcase) { 
if (vol->upcase) { 
if (vol->nls_map) { 
if necessary. 
if (!--ntfs_nr_upcase_users && default_upcase) { 
if (vol->cluster_size <= 4096 && !--ntfs_nr_compression_users) 
if (vol->mft_ino && vol->mft_ino != tmp_ino) 
for efficient allocation/deallocation of inodes. */ 
for the inode slab cache. */ 
for the slab caches. */ 
ifdef NTFS_RW 
if 
if 
if 
if (!ntfs_index_ctx_cache) { 
if (!ntfs_attr_ctx_cache) { 
if (!ntfs_name_cache) { 
if (!ntfs_inode_cache) { 
if (!ntfs_big_inode_cache) { 
if (err) { 
if (!err) { 
if (!err) { 
fore we 
ifdef DEBUG 
if 
file : ./test/kernel/fs/ntfs/collate.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
if (!rc && (data1_len != data2_len)) { 
if (d1 < d2) 
if (d1 == d2) 
ified collation rule 
if @data1 is found, respectively, to collate before, 
for everything else for now. 
if (i <= 0x02) 
if (likely(i <= 3)) 
file : ./test/kernel/fs/ntfs/attrib.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for which to map (part of) a runlist 
if present or NULL if not 
ified, it is an active search context of @ni and its base mft 
ify @ctx as NULL and ntfs_map_runlist_nolock() 
form the necessary mapping and unmapping. 
fore returning.  Thus, @ctx will be left pointing to the same 
ifferent memory locations on return, so you must remember to reset 
if @vcn is zero and 
if 'true' the @ctx 
for 
for writing on entry 
if (!NInoAttr(ni)) 
if (!ctx) { 
if (IS_ERR(m)) 
if (unlikely(!ctx)) { 
if (!a->data.non_resident.lowest_vcn && end_vcn <= 0) 
if vcn exceeds the allocated size, we will refuse to 
if (vcn >= allocated_size_vcn || (a->type == ni->type && 
if (old_ctx.base_ntfs_ino && old_ctx.ntfs_ino != 
if (ctx_needs_reset) { 
if (unlikely(err)) { 
if @vcn is inside it.  Otherwise 
if (unlikely(vcn && vcn >= end_vcn)) { 
if (IS_ERR(rl)) 
if (ctx_is_temporary) { 
if (ctx_needs_reset) { 
if (NInoAttrList(base_ni)) { 
fore, we need to unmap it and map the 
if (ctx->ntfs_ino != old_ctx.ntfs_ino) { 
if (ctx->base_ntfs_ino && ctx->ntfs_ino != 
if (old_ctx.base_ntfs_ino && 
if (IS_ERR(ctx->mrec)) { 
if (ctx->mrec != old_ctx.mrec) { 
for chkdsk to pick up 
if (put_this_page) 
for which to map (part of) a runlist 
ify 
while we were sleeping. */ 
if the runlist is locked for writing 
for writing and 
for reading, 
for i_ino 0x%lx, vcn 0x%llx, %s_locked.", 
if (!ni->runlist.rl) { 
if (!ni->allocated_size) { 
if (likely(lcn >= LCN_HOLE)) { 
if (lcn != LCN_RL_NOT_MAPPED) { 
if (!is_retry) { 
if (!write_locked) { 
if (unlikely(ntfs_rl_vcn_to_lcn(ni->runlist.rl, vcn) != 
if (!write_locked) { 
if (likely(!err)) { 
if (err == -ENOENT) 
if (err == -ENOMEM) 
if (lcn != LCN_ENOENT) 
if present or NULL if not 
ified, it is an active search context of @ni and its base mft 
ify @ctx as NULL and ntfs_attr_find_vcn_nolock() 
form the necessary mapping and unmapping. 
fore returning.  Thus, @ctx will be left pointing to the same 
ifferent memory locations on return, so you must remember to reset 
if the return is success or failure and PTR_ERR() to get to the 
if 'true' the @ctx 
for 
for writing on entry 
for i_ino 0x%lx, vcn 0x%llx, with%s ctx.", 
if (!ni->runlist.rl) { 
if (!ni->allocated_size) { 
if (likely(rl && vcn >= rl[0].vcn)) { 
while (likely(rl->length)) { 
if (likely(rl->lcn >= LCN_HOLE)) { 
if (likely(rl->lcn != LCN_RL_NOT_MAPPED)) { 
if (!err && !is_retry) { 
if (IS_ERR(ctx->mrec)) 
if (likely(!err)) { 
if (err == -EINVAL) 
if (!err) 
if (err != -ENOENT) 
if @name present) 
ified by @ctx->mrec, beginning at @ctx->attr, for an 
fore which the attribute being 
for. 
ified mft record and it ignores the 
for, obviously).  If you need to take attribute lists into consideration, 
for extent records of non-resident 
while the last extent is in the base 
for attribute types which can be 
if @ctx->is_first is 'true'. 
if (ctx->is_first) { 
for (;;	a = (ATTR_RECORD*)((u8*)a + le32_to_cpu(a->length))) { 
if (unlikely(le32_to_cpu(a->type) > le32_to_cpu(type) || 
if (unlikely(!a->length)) 
if (a->type != type) 
if (!name) { 
if (a->name_length) 
if (!ntfs_are_names_equal(name, name_len, 
fore a->name, there is no 
if (rc == -1) 
if (rc) 
if (rc == -1) 
if (rc) 
ified, we have found the attribute 
if (!val) 
fore the current attribute's 
if (!rc) { 
if (val_len == avl) 
if (val_len < avl) 
if (rc < 0) 
if (!vol || !runlist || !al || size <= 0 || initialized_size < 0 || 
if (!initialized_size) { 
if (!rl) { 
ified by the runlist one run at a time. */ 
while (rl->length) { 
if (lcn < 0) { 
if (!bh) { 
if (al + block_size >= al_end) 
while (++block < max_block); 
if (initialized_size < size) { 
if (al < al_end) { 
if (initialized_size < size) 
if @name present) 
for the corresponding 
ifferent mft record/inode, ntfs_attr_find() the attribute 
for the attribute. 
fore which 
fore which the attribute being searched for would 
if there is not enough space, the 
for the inserted attribute should be inserted in the 
for inode 0x%lx, type 0x%x.", ni->mft_no, type); 
if (ni == base_ni) 
if (type == AT_END) 
if (!ctx->al_entry) 
if @ctx->is_first is 'true'. 
if (ctx->is_first) { 
for (;; al_entry = next_al_entry) { 
if ((u8*)al_entry < base_ni->attr_list || 
if ((u8*)al_entry == al_end) 
if (!al_entry->length) 
if ((u8*)al_entry + 6 > al_end || (u8*)al_entry + 
if (le32_to_cpu(al_entry->type) > le32_to_cpu(type)) 
if (type != al_entry->type) 
if (!name) { 
if (!ntfs_are_names_equal(al_name, al_name_len, name, 
fore al_name, there is no 
if (rc == -1) 
if (rc) 
ifferent.  Perhaps I 
if (rc == -1) 
if (rc) 
if the 
if (lowest_vcn && (u8*)next_al_entry >= al_start	    && 
if (MREF_LE(al_entry->mft_reference) == ni->mft_no) { 
if (ni != base_ni) 
if (MREF_LE(al_entry->mft_reference) == 
if (IS_ERR(ctx->mrec)) { 
if (err == -ENOENT) 
for example which become 
ified ntfs_attr_find() here. 
if ((u8*)a < (u8*)ctx->mrec || (u8*)a > (u8*)ctx->mrec + 
if (a->type == AT_END) 
if (!a->length) 
if (al_entry->instance != a->instance) 
if (al_entry->type != a->type) 
if (!ntfs_are_names_equal((ntfschar*)((u8*)a + 
ified or @val specified and it matches, we 
if (!val || (!a->non_resident && le32_to_cpu( 
if (!err) { 
if (ni != base_ni) { 
if (err != -ENOMEM) 
for AT_END, we reset the search context @ctx and 
if (type == AT_END) { 
fore we return, we want to ensure 
if (ni != base_ni) 
for enumeration would 
while (!err); 
if @name present) 
if the search was successful and -errno if not. 
if one wants to add the attribute to the 
if one wants to add the attribute to the mft record this is the 
if (ctx->base_ntfs_ino) 
for debugging really. */ 
if (!NInoAttrList(base_ni) || type == AT_ATTRIBUTE_LIST) 
formed elsewhere. */ 
if present, and initialize the search context again. 
for a new attribute is being started to reset 
if (likely(!ctx->base_ntfs_ino)) { 
formed elsewhere. */ 
if (ctx->ntfs_ino != ctx->base_ntfs_ino) 
if allocation failed. 
if (ctx) 
if present. 
if (ctx->base_ntfs_ino && ctx->ntfs_ino != ctx->base_ntfs_ino) 
ifdef NTFS_RW 
for the attribute definition record corresponding to the attribute 
if found and NULL if not found. 
for (ad = vol->attrdef; (u8*)ad - (u8*)vol->attrdef < 
if (likely(le32_to_cpu(ad->type) < le32_to_cpu(type))) 
if (likely(ad->type == type)) 
for validity 
for an attribute of @type on the 
if valid, -ERANGE if not valid, or -ENOENT if the attribute is not 
if (unlikely(type == AT_ATTRIBUTE_LIST && size > 256 * 1024)) 
for the attribute @type. */ 
if (unlikely(!ad)) 
if (((sle64_to_cpu(ad->min_size) > 0) && 
if an attribute can be non-resident 
formation is obtained from $AttrDef system file. 
if the attribute is not listed in $AttrDef. 
if (unlikely(!ad)) 
if (ad->flags & ATTR_DEF_RESIDENT) 
if an attribute can be resident 
formation is derived from our ntfs knowledge and may 
for index 
if the attribute is allowed to be non-resident and -EPERM if not. 
for this here as we do not know which inode's $Bitmap is 
if (type == AT_INDEX_ALLOCATION) 
form the resize. 
for new_size %u.", new_size); 
if (new_size & 7) 
if (new_size != le32_to_cpu(a->length)) { 
if (new_muse > le32_to_cpu(m->bytes_allocated)) 
if (new_size >= offsetof(ATTR_REC, length) + sizeof(a->length)) 
form the resize. 
if (ntfs_attr_record_resize(m, a, 
if (new_size > old_size) 
fore we can map the mft record and our callers 
for trying to make 
for this kind of -ENOSPC or is it always worth trying 
if (unlikely(err)) { 
for them. 
for allocation 
if (new_size > 0) { 
if (unlikely(!page)) 
if (IS_ERR(rl)) { 
for_mapping_pairs(vol, rl, 0, -1); 
for mapping pairs array, error " 
if (!NInoAttr(ni)) 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if (unlikely(err)) { 
for the name and the mapping pairs array. 
if (NInoSparse(ni) || NInoCompressed(ni)) 
if (page && !PageUptodate(page)) { 
if (unlikely(err)) 
if it exists and update the offset. */ 
ific to non-resident attributes. */ 
if (NInoSparse(ni) || NInoCompressed(ni)) { 
if (NInoCompressed(ni) || vol->major_ver < 3) 
if (unlikely(err)) { 
if (NInoSparse(ni) || NInoCompressed(ni)) { 
if (a->data.non_resident.compression_unit) { 
if (page) { 
if it exists and update the offset. */ 
if (a->name_length) 
if (unlikely(err2)) { 
if memory corruption is at work it 
if ((mp_ofs + attr_size) > arec_size) { 
ific to resident attributes. */ 
if (page) { 
if (ctx) 
if (m) 
if (rl) { 
if (err == -EINVAL) 
if this is required. 
for @new_data_size to be smaller than the old data 
if 
for regions that are being made sparse) and 
if necessary moving it and/or other 
if present and in some of the 
if not already present. 
if present) is possible, the allocation is partially extended 
if the extension was partial.  If @data_start is -1 then partial 
formed. 
for writing as well as 
for example be converted 
for anything other 
ifdef DEBUG 
for i_ino 0x%lx, attribute type 0x%x, " 
if 
for allocation purposes. 
if (NInoNonResident(ni)) { 
if new size is allowed in $AttrDef. */ 
if (unlikely(err)) { 
if (start < 0 || start >= allocated_size) { 
for " 
formant for write(2). */ 
if (!NInoAttr(ni)) 
ifying both the runlist (if non-resident) and the mft 
if (IS_ERR(m)) { 
if (unlikely(!ctx)) { 
if someone did the work whilst we waited for the locks.  If we 
if we need to update the data size. 
if (unlikely(new_alloc_size <= allocated_size)) { 
if (new_data_size < 0) 
if (unlikely(err)) { 
if (a->non_resident) 
if the size is 
if (new_alloc_size < vol->mft_record_size && 
if (new_data_size >= 0) { 
if that fails dropping 
if successful restart the extension process. 
if (likely(!err)) 
for this attribute type or there not being enough space, 
if (unlikely(err != -EPERM && err != -ENOSPC)) { 
if (start < 0 || start >= allocated_size) 
if (err != -ENOMEM) 
if (start < 0 || start >= allocated_size) { 
for the non-resident " 
if (err == -EPERM) */ 
if 0 
if (!err) 
formation 
if (ni->type == AT_ATTRIBUTE_LIST || 
if (!err) 
if it is not already the only attribute in an mft record in 
if (!err) 
if 
if (new_alloc_size == allocated_size) { 
for this inode, then create a sparse region between the old 
if ((start >= 0 && start <= allocated_size) || ni->type != AT_DATA || 
for now... 
if (likely(rl)) { 
while (rl->length) 
if (unlikely(!rl || rl->lcn == LCN_RL_NOT_MAPPED || 
if (!rl && !allocated_size) 
if (IS_ERR(rl)) { 
if (start < 0 || start >= allocated_size) 
if (err != -ENOMEM) 
while (rl->length) 
while (rl->lcn < 0 && rl > ni->runlist.rl) 
formed when start >= 0.  (Needed for POSIX write(2) 
if (IS_ERR(rl2)) { 
if (start < 0 || start >= allocated_size) 
if (err != -ENOMEM && err != -ENOSPC) 
if (IS_ERR(rl)) { 
if (start < 0 || start >= allocated_size) 
if (err != -ENOMEM) 
if (ntfs_cluster_free_from_rl(vol, rl2)) { 
for the new mapping pairs array for this extent. */ 
if (unlikely(mp_size <= 0)) { 
if (start < 0 || start >= allocated_size) 
for the " 
if (unlikely(err)) { 
for the remainder, or by making 
if (start < 0 || start >= allocated_size) 
for the extended attribute " 
if (unlikely(err)) { 
if (a->data.non_resident.lowest_vcn) { 
if (unlikely(err)) 
if @ni is a directory, $MFT, or an index, 
for 
if we created a hole above.  For now 
if (NInoSparse(ni) || NInoCompressed(ni)) { 
if (new_data_size >= 0) { 
if (start < 0 || start >= allocated_size) 
if (err == -ENOENT) 
if (ntfs_attr_lookup(ni->type, ni->name, ni->name_len, CASE_SENSITIVE, 
if @ni is a directory...  See above. 
if (NInoSparse(ni) || NInoCompressed(ni)) { 
if (ntfs_cluster_free(ni, ll, -1, ctx) < 0) { 
if (ntfs_rl_truncate_nolock(vol, &ni->runlist, ll) || IS_ERR(m)) { 
if (mp_rebuilt) { 
if (success) */ { 
if (ctx) 
if (m) 
ified 
formed. 
for ofs 0x%llx, cnt 0x%llx, val 0x%hx.", 
if (!cnt) 
for them. 
if (unlikely(end > i_size_read(VFS_I(ni)))) { 
if (start_ofs) { 
if (IS_ERR(page)) { 
if (idx == end) 
if (idx == end) 
for (; idx < end; idx++) { 
if (unlikely(!page)) { 
if (page_has_buffers(page)) { 
while ((bh = bh->b_this_page) != head); 
if (end_ofs) { 
if (IS_ERR(page)) { 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ntfs/upcase.c 
[ OK ] open : 4 ok... 
ify it 
for more details. 
if not, write to the Free Software Foundation, 
if (!uc) 
for (i = 0; i < default_upcase_len; i++) 
for (r = 0; uc_run_table[r][0]; r++) 
for (r = 0; uc_dup_table[r][0]; r++) 
for (r = 0; uc_word_table[r][0]; r++) 
file : ./test/kernel/fs/ntfs/sysctl.c 
[ OK ] open : 4 ok... 
for sysctl handling in NTFS Linux kernel driver. Part of 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifdef DEBUG 
ifdef CONFIG_SYSCTL 
for the sysctls header. */ 
if (add) { 
if (!sysctls_root_table) 
if /* CONFIG_SYSCTL */ 
file : ./test/kernel/fs/ntfs/unistr.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
for equality 
if @ic == IGNORE_CASE) 
if the names are 
forma a case insensitive comparison. 
if (s1_len != s2_len) 
if (ic == CASE_SENSITIVE) 
if @name1 contains an invalid character return this value 
if @ic is CASE_SENSITIVE) 
if the first name collates before the second one, 
if the second name collates before the first one, or 
if (name1_len > name2_len) 
for (cnt = 0; cnt < min_len; ++cnt) { 
if (ic) { 
if (c2 < upcase_len) 
if (c1 < 64 && legal_ansi_char_array[c1] & 8) 
if (c1 < c2) 
if (c1 > c2) 
if (name1_len < name2_len) 
if (name1_len == name2_len) 
if (c1 < 64 && legal_ansi_char_array[c1] & 8) 
format and appropriate le16_to_cpu() 
if @s1 (or the first @n Unicode characters thereof) is found, respectively, 
for (i = 0; i < n; ++i) { 
if (c1 < c2) 
if (c1 > c2) 
if (!c1) 
format and appropriate 
fore the comparison. 
if @s1 (or the first @n Unicode characters thereof) is found, respectively, 
for (i = 0; i < n; ++i) { 
if ((c2 = le16_to_cpu(s2[i])) < upcase_size) 
if (c1 < c2) 
if (c1 > c2) 
if (!c1) 
for (i = 0; i < name_len; i++) 
format the loaded NLS 
for 
if (likely(ins)) { 
if (likely(ucs)) { 
for (i = o = 0; i < ins_len; i += wc_len) { 
if (likely(wc_len >= 0 && 
if (likely(wc)) { 
if (!wc) */ 
if (wc_len < 0 || 
if (!ucs) */ 
for converted " 
if (!ins) */ 
if (wc_len < 0) { 
if (o >= NTFS_MAX_NAME_LEN) */ { 
for a " 
format dictated by the loaded NLS. 
for calling kfree(*@outs); when finished with it. In this case 
if (ins) { 
if (ns && !ns_len) { 
if (!ns) { 
if (!ns) 
for (i = o = 0; i < ins_len; i++) { 
if (wc > 0) { 
if (!wc) 
if (wc == -ENAMETOOLONG && ns != *outs) { 
if (tc) { 
if (ns != *outs) 
if (wc != -ENAMETOOLONG) 
file : ./test/kernel/fs/ntfs/mst.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
form the necessary post read multi sector transfer fixup and detect the 
if ( size & (NTFS_BLOCK_SIZE - 1)	|| 
fore they are fixed up. Note no need to care for 
ifference. 
for incomplete multi sector transfer(s). 
while (usa_count--) { 
while (usa_count--) { 
form the necessary pre write multi sector transfer fixup on the data 
if fixup applied (success) or -EINVAL if no fixup was performed 
fore calling this function, otherwise it 
fore calling this function 
if it makes sense. */ 
if ( size & (NTFS_BLOCK_SIZE - 1)	|| 
if (usn == 0xffff || !usn) 
while (usa_count--) { 
form the necessary post write multi sector transfer fixup, not checking 
while (usa_count--) { 
file : ./test/kernel/fs/ntfs/quota.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License as published 
for more details. 
if not, write to the Free Software 
ifdef NTFS_RW 
if (NVolQuotaOutOfDate(vol)) 
if (!vol->quota_ino || !vol->quota_q_ino) { 
if (!ictx) { 
if (err) { 
if (ictx->data_len < offsetof(QUOTA_CONTROL_ENTRY, sid)) { 
if (le32_to_cpu(qce->version) != QUOTA_VERSION) { 
if (qce->flags & QUOTA_FLAG_OUT_OF_DATE) 
if (!(qce->flags & (QUOTA_FLAG_TRACKING_ENABLED | 
ified on WinXP to be sufficient to cause windows to 
ified flags are written to disk. */ 
if (ictx) 
if /* NTFS_RW */ 
file : ./test/kernel/fs/ext4/xattr_user.c 
[ OK ] open : 4 ok... 
for extended user attributes. 
if (!test_opt(dentry->d_sb, XATTR_USER)) 
if (list && total_len <= list_size) { 
if (strcmp(name, "") == 0) 
if (!test_opt(dentry->d_sb, XATTR_USER)) 
if (strcmp(name, "") == 0) 
if (!test_opt(dentry->d_sb, XATTR_USER)) 
file : ./test/kernel/fs/ext4/extents_status.c 
[ OK ] open : 4 ok... 
ified by 
ify the implementation of fiemap and bigalloc, and introduce 
for better understand 
fore the implementation of fiemap and bigalloc 
fore 
fore, the extent status tree may not 
while we define a shrinker 
for ext4. 
ifies a delayed extent by looking 
if a 
ify delayed allocations from holes. 
if a block is 
for the cluster. 
if a buffer is 
if a block or a range of 
ifferent status.  The extent in the 
formance analysis 
if writes are 
if (ext4_es_cachep == NULL) 
if (ext4_es_cachep) 
ifdef ES_DEBUG__ 
for inode %lu:", inode->i_ino); 
while (node) { 
if 
for an delayed extent with a given offset.  If 
while (node) { 
if (lblk < es->es_lblk) 
if (lblk > ext4_es_end(es)) 
if (es && lblk < es->es_lblk) 
if (es && lblk > ext4_es_end(es)) { 
if it exists, otherwise, the next extent after @es->lblk. 
if (tree->cache_es) { 
if (in_range(lblk, es1->es_lblk, es1->es_len)) { 
if (es1 && !ext4_es_is_delayed(es1)) { 
while ((node = rb_next(&es1->rb_node)) != NULL) { 
if (es1->es_lblk > end) { 
if (ext4_es_is_delayed(es1)) 
if (es1 && ext4_es_is_delayed(es1)) { 
if (es == NULL) 
if (!ext4_es_is_delayed(es)) { 
if (!ext4_es_is_delayed(es)) { 
if (ext4_es_status(es1) != ext4_es_status(es2)) 
if (((__u64) es1->es_len) + es2->es_len > EXT_MAX_BLOCKS) { 
if (((__u64) es1->es_lblk) + es1->es_len != es2->es_lblk) 
if ((ext4_es_is_written(es1) || ext4_es_is_unwritten(es1)) && 
if (ext4_es_is_hole(es1)) 
if (ext4_es_is_delayed(es1) && !ext4_es_is_unwritten(es1)) 
if (!node) 
if (ext4_es_can_be_merged(es1, es)) { 
if (!node) 
if (ext4_es_can_be_merged(es, es1)) { 
ifdef ES_AGGRESSIVE_TEST 
if (IS_ERR(path)) 
if (ex) { 
if (!ext4_es_is_written(es) && !ext4_es_is_unwritten(es)) { 
for " 
if (es->es_lblk < ee_block || 
for inode: %lu " 
if (ee_status ^ es_status) { 
for inode: %lu " 
if (!ext4_es_is_delayed(es) && !ext4_es_is_hole(es)) { 
for inode: %lu " 
if (path) { 
if (retval > 0) { 
for inode: %lu " 
if (ext4_es_is_written(es)) { 
for " 
if (map.m_pblk != ext4_es_pblock(es)) { 
for " 
if (retval == 0) { 
for inode: %lu " 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if 
while (*p) { 
if (newes->es_lblk < es->es_lblk) { 
ify es_lblk directly 
if (ext4_es_is_written(es) || 
if (newes->es_lblk > ext4_es_end(es)) { 
if (!es) 
formation to an inode's extent 
if (!len) 
if (err != 0) 
if (err == -ENOMEM && __ext4_es_shrink(EXT4_SB(inode->i_sb), 1, 
if (err == -ENOMEM && !ext4_es_is_delayed(&newes)) 
formation into the extent status 
if (!len) 
if (!es || es->es_lblk > end) 
if (tree->cache_es) { 
if (in_range(lblk, es1->es_lblk, es1->es_len)) { 
while (node) { 
if (lblk < es1->es_lblk) 
if (lblk > ext4_es_end(es1)) 
if (found) { 
if (!es) 
if (es->es_lblk > end) 
if (len1 > 0) 
if (len2 > 0) { 
if (ext4_es_is_written(&orig_es) || 
if (err) { 
if ((err == -ENOMEM) && 
if (ext4_es_is_written(es) || 
if (len1 > 0) { 
if (node) 
while (es && ext4_es_end(es) <= end) { 
if (!node) { 
if (es && es->es_lblk < end + 1) { 
if (ext4_es_is_written(es) || ext4_es_is_unwritten(es)) { 
if (!len) 
if (ext4_test_inode_state(&eia->vfs_inode, EXT4_STATE_EXT_PRECACHED) && 
if (!ext4_test_inode_state(&eia->vfs_inode, EXT4_STATE_EXT_PRECACHED) && 
if (eia->i_touch_when == eib->i_touch_when) 
if (time_after(eia->i_touch_when, eib->i_touch_when)) 
for_each_safe(cur, tmp, &sbi->s_es_lru) { 
if (percpu_counter_read_positive(&sbi->s_extent_cache_cnt) == 0) 
if ((sbi->s_es_last_sorted < ei->i_touch_when) || 
if (ei->i_es_lru_nr == 0 || ei == locked_ei) 
if (ei->i_es_lru_nr == 0) 
if (nr_to_scan == 0) 
forward progress, sort the list and try again. 
if ((nr_shrunk == 0) && nr_skipped && !retried) { 
iffies; 
if (ext4_test_inode_state(&ei->vfs_inode, 
if (locked_ei && nr_shrunk == 0) 
if (!nr_to_scan) 
iffies; 
if (list_empty(&ei->i_es_lru)) 
if (!list_empty(&ei->i_es_lru)) 
if (ei->i_es_lru_nr == 0) 
if (ext4_test_inode_state(inode, EXT4_STATE_EXT_PRECACHED) && 
forced shrink of precached extents"); 
while (node != NULL) { 
if (!ext4_es_is_delayed(es)) { 
if (--nr_to_scan == 0) 
file : ./test/kernel/fs/ext4/file.c 
[ OK ] open : 4 ok... 
forms by Jakub Jelinek 
ifferent 
if (ext4_test_inode_state(inode, EXT4_STATE_DA_ALLOC_CLOSE)) { 
if we are the last writer on the inode, drop the block reservation */ 
if (is_dx(inode) && filp->private_data) 
if (pos >= i_size_read(inode)) 
if ((pos | iov_iter_alignment(from)) & blockmask) 
if (o_direct && 
if (file->f_flags & O_APPEND) 
format file, the size limit 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) { 
if ((pos > sbi->s_bitmap_maxbytes) || 
if (pos + length > sbi->s_bitmap_maxbytes) 
if (o_direct) { 
if (ext4_should_dioread_nolock(inode) && !aio_mutex && 
for initialized extents.  1) If we 
if (err == len && (map.m_flags & EXT4_MAP_MAPPED)) 
if (ret > 0) { 
if (err < 0) 
if (o_direct) 
if (aio_mutex) 
if (!mapping->a_ops->readpage) 
if (unlikely(!(sbi->s_mount_flags & EXT4_MF_MNTDIR_SAMPLED) && 
for sysadmin convenience 
if (!IS_ERR(cp)) { 
if (IS_ERR(handle)) 
if (err) { 
if we are opening the inode for 
if (filp->f_mode & FMODE_WRITE) { 
if (ret < 0) 
for a extent-based 
for block-mapped and extent-mapped file at the same 
for a file and we can directly use it to 
for SEEK_DATA/SEEK_HOLE, we would need to 
if this range contains an unwritten extent, 
if (nr_pages == 0) { 
if (lastoff == startoff || lastoff < endoff) 
if (lastoff == startoff && whence == SEEK_HOLE && 
for (i = 0; i < nr_pages; i++) { 
if (lastoff < endoff && whence == SEEK_HOLE && 
if (unlikely(page->mapping != inode->i_mapping)) { 
if (!page_has_buffers(page)) { 
if (page_has_buffers(page)) { 
if (buffer_uptodate(bh) || 
if (whence == SEEK_DATA) 
if (whence == SEEK_HOLE) 
if (found) { 
while (bh != head); 
if (nr_pages < num && whence == SEEK_HOLE) { 
while (index <= end); 
for SEEK_DATA. 
if (offset >= isize) { 
if (ret > 0 && !(map.m_flags & EXT4_MAP_UNWRITTEN)) { 
if (es.es_len != 0 && in_range(last, es.es_lblk, es.es_len)) { 
if (map.m_flags & EXT4_MAP_UNWRITTEN) { 
if (unwritten) 
while (last <= end); 
if (dataoff > isize) 
for SEEK_HOLE. 
if (offset >= isize) { 
if (ret > 0 && !(map.m_flags & EXT4_MAP_UNWRITTEN)) { 
if (es.es_len != 0 && in_range(last, es.es_lblk, es.es_len)) { 
if (map.m_flags & EXT4_MAP_UNWRITTEN) { 
if (!unwritten) { 
while (last <= end); 
if (holeoff > isize) 
for each. 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) 
ifdef CONFIG_COMPAT 
if 
file : ./test/kernel/fs/ext4/ext4_jbd2.c 
[ OK ] open : 4 ok... 
for jbd2_journal_start/end. 
if (sb->s_flags & MS_RDONLY) 
if the journal has aborted behind our 
if (journal && is_journal_aborted(journal)) { 
if (err < 0) 
if (!journal) 
if (!ext4_handle_valid(handle)) { 
if (!err) 
if (err) 
if (!ext4_handle_valid(handle)) 
if (err < 0) { 
if (err < 0) 
if (bh) 
if (!handle->h_err) 
if (is_handle_aborted(handle)) 
if (ext4_handle_valid(handle)) { 
if (err) 
if we are freeing data 
forget(const char *where, unsigned int line, handle_t *handle, 
forget(inode, is_metadata, blocknr); 
forgetting bh %p: is_metadata = %d, mode %o, " 
forget and return */ 
forget(bh); 
if we are doing full data 
if (test_opt(inode->i_sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA || 
if (bh) { 
forget"); 
if (err) 
if (err) { 
if (ext4_handle_valid(handle)) { 
if (err) 
if (ext4_handle_valid(handle)) { 
if there is a bug */ 
if (inode == NULL) { 
if (inode) 
if (inode && inode_needs_sync(inode)) { 
if (buffer_req(bh) && !buffer_uptodate(bh)) { 
if (ext4_handle_valid(handle)) { 
if (err) 
file : ./test/kernel/fs/ext4/ioctl.c 
[ OK ] open : 4 ok... 
for @len bytes. 
while (len-- > 0) { 
for the primary swap between inode1 and inode2 
fore you have to make sure, that calling this method twice 
formation from the given @inode and the inode 
if (inode->i_nlink != 1 || !S_ISREG(inode->i_mode)) 
if (!inode_owner_or_capable(inode) || !capable(CAP_SYS_ADMIN)) 
if (IS_ERR(inode_bl)) 
for all existing dio workers */ 
if (IS_ERR(handle)) { 
if (inode_bl->i_nlink == 0) { 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, 
if (err < 0) { 
if (err < 0) { 
if (!inode_owner_or_capable(inode)) 
if (get_user(flags, (int __user *) arg)) 
if (err) 
if (IS_NOQUOTA(inode)) 
ifiable only by root */ 
if ((flags ^ oldflags) & (EXT4_APPEND_FL | EXT4_IMMUTABLE_FL)) { 
if ((jflag ^ oldflags) & (EXT4_JOURNAL_DATA_FL)) { 
if ((flags ^ oldflags) & EXT4_EXTENTS_FL) 
if (flags & EXT4_EOFBLOCKS_FL) { 
if (!(oldflags & EXT4_EOFBLOCKS_FL)) { 
if (oldflags & EXT4_EOFBLOCKS_FL) 
if (IS_ERR(handle)) { 
if (IS_SYNC(inode)) 
if (err) 
for (i = 0, mask = 1; i < 32; i++, mask <<= 1) { 
if (mask & flags) 
if (err) 
if ((jflag ^ oldflags) & (EXT4_JOURNAL_DATA_FL)) 
if (err) 
if (migrate) { 
if (!inode_owner_or_capable(inode)) 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (err) 
if (get_user(generation, (int __user *) arg)) { 
if (IS_ERR(handle)) { 
if (err == 0) { 
if (err) 
if (get_user(n_blocks_count, (__u32 __user *)arg)) { 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (err) 
if (EXT4_SB(sb)->s_journal) { 
if (err == 0) 
if (!(filp->f_mode & FMODE_READ) || 
if (copy_from_user(&me, 
if (!donor.file) 
if (!(donor.file->f_mode & FMODE_WRITE)) { 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (err) 
if (copy_to_user((struct move_extent __user *)arg, 
if (err) 
if (copy_from_user(&input, (struct ext4_new_group_input __user *)arg, 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (err) 
if (EXT4_SB(sb)->s_journal) { 
if (err == 0) 
if (!err && ext4_has_group_desc_csum(sb) && 
if (!inode_owner_or_capable(inode)) 
if (err) 
fore we switch the 
if (!inode_owner_or_capable(inode)) 
if (err) 
if (!(filp->f_mode & FMODE_WRITE)) 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (copy_from_user(&n_blocks_count, (__u64 __user *)arg, 
if (err) 
if (err) 
if (EXT4_SB(sb)->s_journal) { 
if (err == 0) 
if (!err && (o_group > EXT4_SB(sb)->s_groups_count) && 
if (!capable(CAP_SYS_ADMIN)) 
if (!blk_queue_discard(q)) 
if (copy_from_user(&range, (struct fstrim_range __user *)arg, 
if (ret < 0) 
if (copy_to_user((struct fstrim_range __user *)arg, &range, 
ifdef CONFIG_COMPAT 
if (err) 
if 
file : ./test/kernel/fs/ext4/extents.c 
[ OK ] open : 4 ok... 
iffer <pierre.peiffer@bull.net> 
for more details. 
if not, write to the Free Software 
for EXT4 
if split fails \ 
ify(struct inode *inode, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (et->et_checksum != ext4_extent_block_csum(inode, eh)) 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!ext4_handle_valid(handle)) 
if (handle->h_buffer_credits > needed) 
if (err <= 0) 
if (err == 0) 
if (path->p_bh) { 
if (path->p_bh) { 
if (path) { 
if we are writing a sparse file such as 
if the latter case turns out to be 
if (ex) { 
if (block > ext_block) 
if (path[depth].p_bh) 
for a meta data block 
ifdef AGGRESSIVE_TEST 
if 
ifdef AGGRESSIVE_TEST 
if 
ifdef AGGRESSIVE_TEST 
if 
ifdef AGGRESSIVE_TEST 
if 
if (ei->i_da_metadata_calc_len && 
if ((ei->i_da_metadata_calc_len % idxs) == 0) 
if ((ei->i_da_metadata_calc_len % (idxs*idxs)) == 0) 
if ((ei->i_da_metadata_calc_len % (idxs*idxs*idxs)) == 0) { 
if (depth == ext_depth(inode)) { 
if (depth == 0) 
if (lblock > last) 
if (eh->eh_entries == 0) 
if (depth == 0) { 
while (entries) { 
for overlapping extents */ 
if ((lblock <= prev) && prev) { 
while (entries) { 
if (unlikely(eh->eh_magic != EXT4_EXT_MAGIC)) { 
if (unlikely(le16_to_cpu(eh->eh_depth) != depth)) { 
if (unlikely(eh->eh_max == 0)) { 
if (unlikely(le16_to_cpu(eh->eh_max) > max)) { 
if (unlikely(le16_to_cpu(eh->eh_entries) > le16_to_cpu(eh->eh_max))) { 
if (!ext4_valid_extent_entries(inode, eh, depth)) { 
ify checksum on non-root extent tree nodes */ 
ify(inode, eh)) { 
if (unlikely(!bh)) 
if (!bh_uptodate_or_lock(bh)) { 
if (err < 0) 
if (buffer_verified(bh) && !(flags & EXT4_EX_FORCE_CACHE)) 
if (err) 
ified(bh); 
if (!(flags & EXT4_EX_NOCACHE) && depth == 0) { 
for (i = le16_to_cpu(eh->eh_entries); i > 0; i--, ex++) { 
if (prev && (prev != lblk)) 
if (ext4_ext_is_unwritten(ex)) 
formation in the 
if (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if (path == NULL) { 
if there are no external extent blocks */ 
if (ret) 
while (i >= 0) { 
if ((i == depth) || 
if (IS_ERR(bh)) { 
ifdef EXT_DEBUG 
for (k = 0; k <= l; k++, path++) { 
if (path->p_ext) { 
if (!path) 
for inode %lu\n", inode->i_ino); 
if (depth != level) { 
while (idx <= EXT_MAX_INDEX(path[level].p_hdr)) { 
while (ex <= EXT_MAX_EXTENT(path[depth].p_hdr)) { 
if 
for (i = 0; i <= depth; i++, path++) 
for the closest index of the given block 
for %u(idx):  ", block); 
while (l <= r) { 
if (block < le32_to_cpu(m->ei_block)) 
ifdef CHECK_BINSEARCH 
for (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ix++) { 
if (block < le32_to_cpu(ix->ei_block)) 
if 
for closest extent of the given block 
if (eh->eh_entries == 0) { 
for %u:  ", block); 
while (l <= r) { 
if (block < le32_to_cpu(m->ee_block)) 
ifdef CHECK_BINSEARCH 
for (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ex++) { 
if (block < le32_to_cpu(ex->ee_block)) 
if 
if (!path) { 
if (!path) 
while (i) { 
if (IS_ERR(bh)) { 
if (unlikely(ppos > depth)) { 
if not an empty leaf */ 
if (alloc) 
fore @curp or after @curp 
if (err) 
if (unlikely(logical == le32_to_cpu(curp->p_idx->ei_block))) { 
if (unlikely(le16_to_cpu(curp->p_hdr->eh_entries) 
if (logical > le32_to_cpu(curp->p_idx->ei_block)) { 
fore */ 
if (len > 0) { 
if (unlikely(ix > EXT_MAX_INDEX(curp->p_hdr))) { 
if (unlikely(ix > EXT_LAST_INDEX(curp->p_hdr))) { 
if current leaf will be split, then we should use 
if (unlikely(path[depth].p_ext > EXT_MAX_EXTENT(path[depth].p_hdr))) { 
if (path[depth].p_ext != EXT_MAX_EXTENT(path[depth].p_hdr)) { 
if (!ablocks) 
for indexes/leaf\n", depth - at); 
if (newblock == 0) 
if (unlikely(newblock == 0)) { 
if (unlikely(!bh)) { 
if (err) 
if (unlikely(path[depth].p_hdr->eh_entries != 
if (m) { 
if (err) 
if (m) { 
if (err) 
if (err) 
if (unlikely(k < 0)) { 
if (k) 
while (k--) { 
if (unlikely(!bh)) { 
if (err) 
if (unlikely(EXT_MAX_INDEX(path[i].p_hdr) != 
if (m) { 
if (err) 
if (m) { 
if (err) 
if (err) 
if (bh) { 
if (err) { 
for (i = 0; i < depth; i++) { 
if (newblock == 0) 
if (unlikely(!bh)) 
if (err) { 
if (ext_depth(inode)) 
if (err) 
if (neh->eh_depth == 0) { 
if no free index is found, then it requests in-depth growing. 
for free index entry */ 
while (i > 0 && !EXT_HAS_FREE_INDEX(curp)) { 
for index block, 
if (EXT_HAS_FREE_INDEX(curp)) { 
if (err) 
if (IS_ERR(path)) 
if (err) 
if (IS_ERR(path)) { 
if (path[depth].p_hdr->eh_entries == path[depth].p_hdr->eh_max) { 
for *logical 
if *logical is the smallest allocated block, the function 
if (unlikely(path == NULL)) { 
if (depth == 0 && path->p_ext == NULL) 
if (*logical < le32_to_cpu(ex->ee_block)) { 
while (--depth >= 0) { 
if (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) { 
if (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) { 
for *logical 
if *logical is the largest allocated block, the function 
if (unlikely(path == NULL)) { 
if (depth == 0 && path->p_ext == NULL) 
if (*logical < le32_to_cpu(ex->ee_block)) { 
while (--depth >= 0) { 
if (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) { 
if (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) { 
if (ex != EXT_LAST_EXTENT(path[depth].p_hdr)) { 
for index to the right */ 
while (--depth >= 0) { 
if (ix != EXT_LAST_INDEX(path[depth].p_hdr)) 
while (++depth < path->p_depth) { 
if (IS_ERR(bh)) 
if (IS_ERR(bh)) 
if (bh) 
if (depth == 0 && path->p_ext == NULL) 
while (depth >= 0) { 
if (path[depth].p_ext && 
if (path[depth].p_idx != 
if (depth == 0) 
while (depth >= 0) { 
if leaf gets modified and modified extent is first in the leaf, 
if (unlikely(ex == NULL || eh == NULL)) { 
if (depth == 0) { 
if (ex != EXT_FIRST_EXTENT(eh)) { 
if border is smaller than current one 
if (err) 
if (err) 
while (k--) { 
if (path[k+1].p_idx != EXT_FIRST_INDEX(path[k+1].p_hdr)) 
if (err) 
if (err) 
if (ext4_ext_is_unwritten(ex1) != ext4_ext_is_unwritten(ex2)) 
if (le32_to_cpu(ex1->ee_block) + ext1_ee_len != 
for preallocated extents to be added 
if (ext1_ee_len + ext2_ee_len > EXT_INIT_MAX_LEN) 
if (ext4_ext_is_unwritten(ex1) && 
ifdef AGGRESSIVE_TEST 
if 
if the extents (ex and ex+1) were _not_ merged and returns 
while (ex < EXT_LAST_EXTENT(eh)) { 
if (unwritten) 
if (ex + 1 < EXT_LAST_EXTENT(eh)) { 
if (!eh->eh_entries) 
if we can collapse 
if ((path[0].p_depth != 1) || 
ify the block allocation bitmap and the block 
if (ext4_journal_extend(handle, 2)) 
if merge left else 0. 
if (ex > EXT_FIRST_EXTENT(eh)) 
if (!merge_done) 
if a portion of the "newext" extent overlaps with an 
if (!path[depth].p_ext) 
if the extent in the path 
fore the requested block(s) 
if (b2 < b1) { 
if (b2 == EXT_MAX_BLOCKS) 
for wrap through zero on extent logical start block*/ 
for overlap */ 
if (unlikely(ext4_ext_get_actual_len(newext) == 0)) { 
if (unlikely(path[depth].p_hdr == NULL)) { 
if (ex && !(gb_flags & EXT4_GET_BLOCKS_PRE_IO)) { 
if (ex < EXT_LAST_EXTENT(eh) && 
if ((ex > EXT_FIRST_EXTENT(eh)) && 
if (ext4_can_extents_be_merged(inode, ex, newext)) { 
if (err) 
if (unwritten) 
if (ext4_can_extents_be_merged(inode, newext, ex)) { 
if (err) 
if (unwritten) 
if (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) 
for us? */ 
if (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block)) 
if (next != EXT_MAX_BLOCKS) { 
if (IS_ERR(npath)) 
if (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) { 
if (gb_flags & EXT4_GET_BLOCKS_METADATA_NOFAIL) 
if (err) 
if (err) 
if (!nearex) { 
if (le32_to_cpu(newext->ee_block) 
fore: " 
fore */ 
if (len > 0) { 
if (!(gb_flags & EXT4_GET_BLOCKS_PRE_IO)) 
if (err) 
if (npath) { 
while (block < last && block != EXT_MAX_BLOCKS) { 
for this block */ 
if (path && ext_depth(inode) != depth) { 
if (IS_ERR(path)) { 
if (unlikely(path[depth].p_hdr == NULL)) { 
if (!ex) { 
if (le32_to_cpu(ex->ee_block) > block) { 
fore found extent */ 
if (block + num < end) 
if (block >= le32_to_cpu(ex->ee_block) 
if (end >= next) 
if (block >= le32_to_cpu(ex->ee_block)) { 
if (block + num < end) 
if (!exists) { 
if (ext4_ext_is_unwritten(ex)) 
if (!exists && next_del) { 
if (unlikely(es.es_len == 0)) { 
iff next == next_del == EXT_MAX_BLOCKS. 
if (next == next_del && next == EXT_MAX_BLOCKS) { 
if (unlikely(next_del != EXT_MAX_BLOCKS || 
if (exists) { 
if (err < 0) 
if (err == 1) { 
if (path) { 
if (ex == NULL) { 
if (block < le32_to_cpu(ex->ee_block)) { 
fore): %u [%u:%u]", 
if (!ext4_find_delalloc_range(inode, lblock, lblock + len - 1)) 
if (block >= le32_to_cpu(ex->ee_block) 
if (!ext4_find_delalloc_range(inode, lblock, lblock + len - 1)) 
if (unlikely(path->p_hdr->eh_entries == 0)) { 
if (err) 
if (path->p_idx != EXT_LAST_INDEX(path->p_hdr)) { 
if (err) 
while (--depth >= 0) { 
if (err) 
if (err) 
for_single_extent: 
for_single_extent(struct inode *inode, int nrblocks, 
if (path) { 
if (le16_to_cpu(path[depth].p_hdr->eh_entries) 
for leaf block credit 
if (ext4_has_inline_data(inode)) 
if (extents <= 1) 
if (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode)) 
if (ext4_should_journal_data(inode)) 
if we 
ifferent from the 
if ((*partial_cluster > 0) && 
ifdef EXTENTS_STATS 
if (ee_len < sbi->s_ext_min) 
if (ee_len > sbi->s_ext_max) 
if (ext_depth(inode) > sbi->s_depth_max) 
if 
for the situation when the cluster is still 
if (*partial_cluster < 0 && 
if we determine that the truncate operation has 
if we did not manage to free the whole 
if (unaligned && (ee_len == num) && 
if (unaligned) 
if (*partial_cluster > 0) 
if "start" and "end" appear in the same extent 
if all extents 
if (!path[depth].p_hdr) 
if (unlikely(path[depth].p_hdr == NULL)) { 
if (!ex) 
if it shares a cluster with the extent to 
if (ex != EXT_LAST_EXTENT(eh)) { 
if (current_cluster == right_cluster && 
while (ex >= EXT_FIRST_EXTENT(eh) && 
if (ext4_ext_is_unwritten(ex)) 
if (end < ex_ee_block) { 
if this extent is not cluster aligned we have 
if (EXT4_PBLK_COFF(sbi, pblk)) 
if (b != ex_ee_block + ex_ee_len - 1) { 
if (a != ex_ee_block) { 
for leaf, sb, and inode plus 2 (bmap and group 
for 
if (ex == EXT_FIRST_EXTENT(eh)) { 
if (err) 
if (err) 
if (err) 
if (num == 0) 
if all the blocks in the 
if (unwritten && num) 
if (num == 0) { 
if (*partial_cluster > 0) 
if (err) 
if (correct_index && eh->eh_entries) 
if it isn't shared with the 
if it's not shared with 
if (*partial_cluster > 0 && eh->eh_entries && 
if this leaf is free, then we should 
if (err == 0 && eh->eh_entries == 0 && path[depth].p_bh != NULL) 
if current index has to be freed (even partial) 
if (path->p_idx < EXT_FIRST_INDEX(path->p_hdr)) 
if truncate on deeper level happened, it wasn't partial, 
for truncation 
if (le16_to_cpu(path->p_hdr->eh_entries) == path->p_block) 
if (IS_ERR(handle)) 
if we are removing extents inside the extent tree. If that 
if (end < EXT_MAX_BLOCKS - 1) { 
for this block */ 
if (IS_ERR(path)) { 
if inode has no blocks at all */ 
if (!ex) { 
if the last block is inside the extent, if so split 
if (end >= ee_block && 
if (ext4_ext_is_unwritten(ex)) 
if that happens. 
if (err < 0) 
if (path) { 
while (--k > 0) 
if (path == NULL) { 
if (ext4_ext_check(inode, path[0].p_hdr, depth, 0)) { 
while (i >= 0 && err == 0) { 
if (!path[i].p_hdr) { 
if (!path[i].p_idx) { 
if (ext4_ext_more_to_rm(path + i)) { 
if (IS_ERR(bh)) { 
if we did IO above. */ 
if (WARN_ON(i + 1 > depth)) { 
if (path[i].p_hdr->eh_entries == 0 && i > 0) { 
if (partial_cluster > 0 && path->p_hdr->eh_entries == 0) { 
if (path->p_hdr->eh_entries == 0) { 
if (err == 0) { 
if (err == -EAGAIN) { 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) { 
ifdef AGGRESSIVE_TEST 
if 
if 
if 
if 
if 
if (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) 
ifdef EXTENTS_STATS 
if 
if (ee_len == 0) 
if (ret > 0) 
if the extent could be zeroout if split fails, and 
if (err) 
if (split == ee_block) { 
if (split_flag & EXT4_EXT_MARK_UNWRIT2) 
if (!(flags & EXT4_GET_BLOCKS_PRE_IO)) 
if (split_flag & EXT4_EXT_MARK_UNWRIT1) 
if (err) 
if (split_flag & EXT4_EXT_MARK_UNWRIT2) 
if (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) { 
if (split_flag & EXT4_EXT_DATA_VALID1) { 
if (err) 
if (err) 
if (err) 
if (map->m_lblk + map->m_len < ee_block + ee_len) { 
if (unwritten) 
if (split_flag & EXT4_EXT_DATA_VALID2) 
if (err) 
if (IS_ERR(path)) 
if (!ex) { 
if (map->m_lblk >= ee_block) { 
if (unwritten) { 
if (err) 
if someone tries to write 
if (eof_block < map->m_lblk + map_len) 
for workloads doing fallocate(FALLOC_FL_KEEP_SIZE) 
if the transfer 
if ((map->m_lblk == ee_block) && 
if ((!ext4_ext_is_unwritten(abut_ex)) &&		/*C1*/ 
if (err) 
ift the start of ex by 'map_len' blocks */ 
if (((map->m_lblk + map_len) == (ee_block + ee_len)) && 
if we can merge right */ 
if ((!ext4_ext_is_unwritten(abut_ex)) &&		/*C1*/ 
if (err) 
ift the start of abut_ex by 'map_len' blocks */ 
if (allocated) { 
if extent is fully inside i_size or new_size. 
if (EXT4_EXT_MAY_ZEROOUT & split_flag) 
if (max_zeroout && (ee_len <= max_zeroout)) { 
if (err) 
if (err) 
if (max_zeroout && (allocated > map->m_len)) { 
if (err) 
if (map->m_lblk - ee_block + map->m_len < max_zeroout) { 
if (map->m_lblk != ee_block) { 
if (err) 
if (allocated < 0) 
if (!err) 
if the extent tree grow after 
fore DIO submit 
if (eof_block < map->m_lblk + map->m_len) 
if extent is fully insde i_size or new_size. 
if (flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN) { 
if (flags & EXT4_GET_BLOCKS_CONVERT) { 
if (ee_block != map->m_lblk || ee_len > map->m_len) { 
if (err < 0) 
if (IS_ERR(path)) { 
if (!ex) { 
if (err) 
ified extent as dirty */ 
if (ee_block != map->m_lblk || ee_len > map->m_len) { 
if 
if (err < 0) 
if (IS_ERR(path)) { 
if (err) 
ified extent as dirty */ 
for (i = 0; i < count; i++) 
if necessary 
if (!ext4_test_inode_flag(inode, EXT4_INODE_EOFBLOCKS)) 
for this case anymore. Simply remove the flag 
if (unlikely(!eh->eh_entries)) 
if we are writing the 
if the caller to 
if (lblk + len < le32_to_cpu(last_ex->ee_block) + 
if the current extent is the last extent in the file, by 
for (i = depth-1; i >= 0; i--) 
if there is a delalloc block in the range, otherwise 0. 
if (es.es_len == 0) 
if (es.es_lblk <= lblk_start && 
if (lblk_start <= es.es_lblk && es.es_lblk <= lblk_end) 
ified by the 'map') 
for. 
fore the delayed allocation could be resolved. 
for delayed allocation. In this case, we will exclude that 
if we 
for this allocation */ 
if (c_offset) { 
if (ext4_find_delalloc_range(inode, lblk_from, lblk_to)) 
if (allocated_clusters && c_offset) { 
if (ext4_find_delalloc_range(inode, lblk_from, lblk_to)) 
if (map->m_len > EXT_UNWRITTEN_MAX_LEN) 
if (ret >= 0) { 
if (allocated > map->m_len) 
if needed. 
fore submit the IO, split the extent */ 
if (ret <= 0) 
if (io) 
if (flags & EXT4_GET_BLOCKS_CONVERT) { 
if (ret >= 0) { 
if (allocated > map->m_len) 
if (flags & EXT4_GET_BLOCKS_UNWRIT_EXT) { 
if ((flags & EXT4_GET_BLOCKS_CREATE) == 0) { 
for us.  But 
if (ret >= 0) 
if (ret <= 0) { 
if we allocated more blocks than requested 
if (allocated > map->m_len) { 
for this offset. So cancel these reservation 
if (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) { 
if (reserved_clusters) 
if ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) { 
if (err < 0) 
if (allocated > map->m_len) 
if the requested 
for is this one: 
for data in 
if ((rr_cluster_start == ex_cluster_end) || 
if (rr_cluster_start == ex_cluster_end) 
for and handle this case: 
if (map->m_lblk < ee_block) 
for the case where there is already another allocated 
if (map->m_lblk > ee_block) { 
for extents based files 
if not allocating file system block 
if create == 0 and these are pre-allocated blocks 
if plain look up failed (blocks have not been allocated) 
for inode %lu\n", 
for this block */ 
if (IS_ERR(path)) { 
ification; 
if (unlikely(path[depth].p_ext == NULL && depth != 0)) { 
if (ex) { 
if found extent covers block, simply return it */ 
if ((!ext4_ext_is_unwritten(ex)) && 
if (!ext4_ext_is_unwritten(ex)) 
if (ret < 0) 
if ((sbi->s_cluster_ratio > 1) && 
if create flag is zero 
if ((flags & EXT4_GET_BLOCKS_CREATE) == 0) { 
if ((flags & EXT4_GET_BLOCKS_NO_PUT_HOLE) == 0) 
if the extent returned 
if (cluster_offset && ex && 
if (err) 
if (err) 
if the extent after searching to the right implies a 
if ((sbi->s_cluster_ratio > 1) && ex2 && 
if request is beyond maximum number of blocks we can have in 
for an unwritten extent this limit is 
if (map->m_len > EXT_INIT_MAX_LEN && 
if (map->m_len > EXT_UNWRITTEN_MAX_LEN && 
if we can really insert (m_lblk)::(m_lblk + m_len) extent */ 
if (err) 
for the logical block number, since when we allocate a 
if (S_ISREG(inode->i_mode)) 
for non-regular files */ 
if (flags & EXT4_GET_BLOCKS_NO_NORMALIZE) 
if (!newblock) 
if (ar.len > allocated) 
if (flags & EXT4_GET_BLOCKS_UNWRIT_EXT){ 
for every IO write to an 
form conversion when IO is done. 
if (flags & EXT4_GET_BLOCKS_PRE_IO) 
if ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) 
if (!err) 
if (!err && set_unwritten) { 
if (err && free_on_err) { 
if (allocated > map->m_len) 
if (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) { 
if (map->m_flags & EXT4_MAP_FROM_CLUSTER) { 
for this range. 
if (reserved_clusters < allocated_clusters) { 
for 
for this write. 
for [0-3] and [4-7]; and not for [8-11] as 
for writing these 
for all newly allocated blocks. 
if ((flags & EXT4_GET_BLOCKS_UNWRIT_EXT) == 0) 
if (allocated > map->m_len) 
if (path) { 
if (err == -ENOMEM) { 
if (err) { 
if it can fit in one extent so 
if (len <= EXT_UNWRITTEN_MAX_LEN) 
while (ret >= 0 && ret < len) { 
if (IS_ERR(handle)) { 
if (ret <= 0) { 
if (ret2) 
if (ret == -ENOSPC && 
if (!S_ISREG(inode->i_mode)) 
force_commit to flush all data in case of data=journal. */ 
force_commit(inode->i_sb); 
if (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) { 
if (ret) 
if (start < offset || end > offset + len) 
if (max_blocks < lblk) 
if (mode & FALLOC_FL_KEEP_SIZE) 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) { 
if (!(mode & FALLOC_FL_KEEP_SIZE) && 
if (ret) 
if (partial) 
if (max_blocks > 0) { 
if (ret) 
if (ret) 
if (IS_ERR(handle)) { 
if (new_size) { 
if (new_size > EXT4_I(inode)->i_disksize) 
if the new size is the same as i_size. 
if ((offset + len) > i_size_read(inode)) 
if (file->f_flags & O_SYNC) 
for a file. This implements ext4's fallocate file 
for file systems which do not support fallocate() system call). 
if mode is not supported */ 
if (mode & FALLOC_FL_PUNCH_HOLE) 
if (ret) 
for extent-based 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) 
if (mode & FALLOC_FL_COLLAPSE_RANGE) 
if (mode & FALLOC_FL_ZERO_RANGE) 
if (mode & FALLOC_FL_KEEP_SIZE) 
if (!(mode & FALLOC_FL_KEEP_SIZE) && 
if (ret) 
if (ret) 
if (IS_ERR(handle)) 
if (new_size) { 
if (new_size > EXT4_I(inode)->i_disksize) 
if the new size is the same as i_size. 
if ((offset + len) > i_size_read(inode)) 
if (file->f_flags & O_SYNC) 
for conversion of each extent separately. 
if (handle) { 
if (IS_ERR(handle)) 
while (ret >= 0 && ret < max_blocks) { 
if (credits) { 
if (IS_ERR(handle)) { 
if (ret <= 0) 
if (credits) 
if (ret <= 0 || ret2) 
if (!credits) 
if no delayed 
if (newes->es_pblk == 0) { 
if (es.es_len == 0) 
if (es.es_lblk > newes->es_lblk) { 
if (es.es_len == 0) 
ified here */ 
if (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) { 
if (error) 
if (physical) 
if (ext4_has_inline_data(inode)) { 
if (has_inline) 
if (fieinfo->fi_flags & FIEMAP_FLAG_CACHE) { 
if (error) 
if not in extents fmt */ 
if (fiemap_check_flags(fieinfo, EXT4_FIEMAP_FLAGS)) 
if (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) { 
if (last_blk >= EXT_MAX_BLOCKS) 
formation 
for marking it dirty. 
if (!ext4_handle_valid(handle)) 
if need to extend journal credits 
for leaf, sb, and inode plus 2 (bmap and group 
if (handle->h_buffer_credits < 7) { 
if (err && err != -EAGAIN) 
ift_path_extents: 
ift 
for each extent. 
ift_path_extents(struct ext4_ext_path *path, ext4_lblk_t shift, 
while (depth >= 0) { 
if (!ex_start) 
if (!ex_last) 
if (err) 
if (ex_start == EXT_FIRST_EXTENT(path[depth].p_hdr)) 
while (ex_start <= ex_last) { 
if ((ex_start > 
if (err) 
if (--depth < 0 || !update) 
if (err) 
ift); 
if (err) 
if current index is not a starting index */ 
ift_extents: 
ifted downwards by shift blocks. 
ift_extents(struct inode *inode, handle_t *handle, 
if (IS_ERR(path)) 
if (!extent) { 
ift, if hole is at the end of file */ 
ifting extents until we make sure the hole is big 
if (IS_ERR(path)) 
if (extent) { 
if ((start == ex_start && shift > ex_start) || 
while (start < stop_block) { 
if (IS_ERR(path)) 
if (!extent) { 
if (start > current_block) { 
if (ret != 0) { 
if (ret == 1) 
ift_path_extents(path, shift, inode, 
if (ret) 
for ext4 
if (offset & (EXT4_BLOCK_SIZE(sb) - 1) || 
if (!S_ISREG(inode->i_mode)) 
if (EXT4_SB(inode->i_sb)->s_cluster_ratio > 1) 
force_commit to flush all data in case of data=journal. */ 
force_commit(inode->i_sb); 
for page size > block size. 
if (ret) 
if (offset + len >= i_size_read(inode)) { 
for extent based files */ 
for existing dio to complete */ 
if (IS_ERR(handle)) { 
if (ret) { 
if (ret) { 
ift_extents(inode, handle, punch_stop, 
if (ret) { 
if (IS_SYNC(inode)) 
file : ./test/kernel/fs/ext4/dir.c 
[ OK ] open : 4 ok... 
if the given dir-inode refers to an htree-indexed directory 
if it is a dx dir, 0 if not 
if (EXT4_HAS_COMPAT_FEATURE(inode->i_sb, 
if the directory entry is OK, and 1 if there is a problem 
if (unlikely(rlen < EXT4_DIR_REC_LEN(1))) 
if (unlikely(rlen % 4 != 0)) 
if (unlikely(rlen < EXT4_DIR_REC_LEN(de->name_len))) 
for name_len"; 
if (unlikely(le32_to_cpu(de->inode) > 
if (filp) 
if (is_dx_dir(inode)) { 
if (err != ERR_BAD_DX_DIR) { 
if (ext4_has_inline_data(inode)) { 
if (has_inline_data) 
while (ctx->pos < inode->i_size) { 
if (err > 0) { 
if (!ra_has_index(&file->f_ra, index)) 
if (!bh) { 
if (ctx->pos > inode->i_blocks << 9) 
if (!buffer_verified(bh) && 
ified(bh); 
if (file->f_version != inode->i_version) { 
for (i = 0; i < sb->s_blocksize && i < offset; ) { 
if (ext4_rec_len_from_disk(de->rec_len, 
while (ctx->pos < inode->i_size 
if (ext4_check_dir_entry(inode, file, de, bh, 
if (le32_to_cpu(de->inode)) { 
if (ctx->pos < inode->i_size) { 
ifdef CONFIG_COMPAT 
if 
for dx directories 
ified. 
if ((filp->f_mode & FMODE_32BITHASH) || 
if ((filp->f_mode & FMODE_32BITHASH) || 
if ((filp->f_mode & FMODE_32BITHASH) || 
for dx directories 
if ((filp->f_mode & FMODE_32BITHASH) || 
if (likely(dx_dir)) 
for_each_entry_safe(fname, next, root, rb_hash) 
while (fname) { 
if (!p) 
if (!new_fn) 
while (*p) { 
if ((new_fn->hash == fname->hash) && 
if (new_fn->hash < fname->hash) 
if (new_fn->hash > fname->hash) 
if (new_fn->minor_hash < fname->minor_hash) 
if (new_fn->minor_hash > fname->minor_hash) */ 
for ext4_dx_readdir.  It calls filldir 
if (!fname) { 
while (fname) { 
if (!info) { 
if (!info) 
if (ctx->pos == ext4_get_htree_eof(file)) 
if (info->last_pos != ctx->pos) { 
if (info->extra_fname) { 
if (!info->curr_node) 
while (1) { 
if we have no more entries, 
if ((!info->curr_node) || 
if (ret < 0) 
if (ret == 0) { 
if (call_filldir(file, ctx, fname)) 
if (info->curr_node) { 
if (info->next_hash == ~0) { 
if (filp->private_data) 
ifdef CONFIG_COMPAT 
if 
file : ./test/kernel/fs/ext4/indirect.c 
[ OK ] open : 4 ok... 
if the referred-to block is likely to be 
for UNIX filesystems - tree of pointers anchored in the inode, with 
ifferently, because otherwise on an 
if our filesystem had 8Kb blocks. We might use long long, but that would 
if (i_block < direct_blocks) { 
if ((i_block -= direct_blocks) < indirect_blocks) { 
if ((i_block -= indirect_blocks) < double_blocks) { 
if (((i_block -= double_blocks) >> (ptrs_bits * 2)) < ptrs) { 
if (boundary) 
if everything went OK or the pointer to the last filled triple 
for i==0 and into the bh->b_data 
for i>0 and NULL for i==0. In other words, it holds the block 
ify that chain did not change) and buffer_heads hosting these 
if (!p->key) 
while (--depth) { 
if (unlikely(!bh)) { 
if (!bh_uptodate_or_lock(bh)) { 
if (ext4_check_indirect_blockref(inode, bh)) { 
if (!p->key) 
for allocation with sufficient locality 
for block allocation. 
if there is a block to the left of our position - allocate near it. 
if pointer will live in inode - allocate in the same 
ifferent inode 
for (p = ind->p - 1; p >= start; p--) { 
if (ind->bh) 
for allocation. 
for block allocation, 
for non-extent files, we limit the block nr 
for the given branch. 
for indirect blocks 
if (k > 0) { 
if (blks < blocks_to_boundary + 1) 
while (count < blks && count <= blocks_to_boundary && 
for this transaction 
for allocation 
if we are synchronous) writes them to disk. 
formation about that chain in the branch[], in 
forget 
for the direct block allocation 
if (S_ISREG(inode->i_mode)) 
for (i = 0; i <= indirect_blks; i++) { 
if (err) { 
if (i == 0) 
if (unlikely(!bh)) { 
if (err) { 
if (i == indirect_blks) 
for (j = 0; j < len; j++) 
if (err) 
for (; i >= 0; i--) { 
forget() only freshly allocated indirect 
fore ext4_alloc_branch() was called. 
if (i > 0 && i != indirect_blks && branch[i].bh) 
forget(handle, 1, inode, branch[i].bh, 
for this transaction 
fore the splice. 
if (where->bh) { 
if (err) 
if (num == 0 && blks > 1) { 
for (i = 1; i < blks; i++) 
if (where->bh) { 
if it is being spliced 
if (err) 
for (i = 1; i <= num; i++) { 
for ext4_map_blocks(). 
fore attaching anything 
if check fails, otherwise 
force the 
if create == 0. 
if plain lookup failed. 
if allocating filesystem 
if not allocating file system 
if (depth == 0) 
if (!partial) { 
while (count < map->m_len && count <= blocks_to_boundary) { 
if (blk == first_block + count) 
if ((flags & EXT4_GET_BLOCKS_CREATE) == 0 || err == -EIO) 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
for " 
for [d,t]indirect blocks */ 
for this branch. 
while we alter the tree 
forget any buffers 
for bitmaps where the 
if (!err) 
if (err) 
if (count > blocks_to_boundary) 
while (partial > chain) { 
for ext3 (or indirect map) based files 
if the machine crashes during the write. 
if (rw == WRITE) { 
if (final_size > inode->i_size) { 
for sb + inode write */ 
if (IS_ERR(handle)) { 
if (ret) { 
if (rw == READ && ext4_should_dioread_nolock(inode)) { 
while holding extra i_dio_count ref. 
if (unlikely(ext4_test_inode_state(inode, 
if (unlikely((rw & WRITE) && ret < 0)) { 
if (end > isize) 
if (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries)) 
if (orphan) { 
for sb + inode write */ 
if (IS_ERR(handle)) { 
if (inode->i_nlink) 
if (inode->i_nlink) 
if (ret > 0) { 
if (end > inode->i_size) { 
if (ret == 0) 
for non extent file based file 
if (lblock < EXT4_NDIR_BLOCKS) 
if (ei->i_da_metadata_calc_len && 
for the purposes of truncation.  If 
if we managed to create more room.  If we can't create more 
if (!ext4_handle_valid(handle)) 
if (ext4_handle_has_enough_credits(handle, EXT4_RESERVE_TRANS_BLOCKS+1)) 
if (!ext4_journal_extend(handle, ext4_blocks_for_truncate(inode))) 
for first non-zero word 
while (p < q) 
for partial truncation. 
if some data below the new i_size is referred 
for (k = depth; k > 1 && !offsets[k-1]; k--) 
if (!partial) 
if (!partial->key && *partial->p) 
for (p = partial; (p > chain) && all_zeroes((__le32 *) p->bh->b_data, p->p); p--) 
if that rest 
if (p == chain + k - 1 && p > chain) { 
if 0 
if 
while (partial > p) { 
ification. 
if (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode)) 
if (ext4_should_journal_data(inode)) 
if (!ext4_data_block_valid(EXT4_SB(inode->i_sb), block_to_free, 
if (try_to_extend_transaction(handle, inode)) { 
if (unlikely(err)) 
if (unlikely(err)) 
for_truncate(inode)); 
if (bh) { 
if (unlikely(err)) 
for (p = first; p < last; p++) 
for this transaction 
if these 
if @first and @last point into the inode's direct 
for current block */ 
if (this_bh) {				/* For indirect block */ 
if we can't update the indirect pointers 
if (err) 
for (p = first; p < last; p++) { 
if (nr) { 
if (count == 0) { 
if (nr == block_to_free + count) { 
if (err) 
if (!err && count > 0) 
if (err < 0) 
if (this_bh) { 
if the data is corrupted and an indirect 
for this instead of OOPSing. 
if ((EXT4_JOURNAL(inode) == NULL) || bh2jh(this_bh)) 
for this transaction 
if (ext4_handle_is_aborted(handle)) 
if (depth--) { 
while (--p >= first) { 
if (!nr) 
if (!ext4_data_block_valid(EXT4_SB(inode->i_sb), 
for the next level down */ 
if (!bh) { 
if extend_transaction() 
for some reason fails to put the bitmap changes and 
if (ext4_handle_is_aborted(handle)) 
if (try_to_extend_transaction(handle, inode)) { 
for_truncate(inode)); 
if 
former) 
if (parent_bh) { 
if (!ext4_journal_get_write_access(handle, 
if (last_block != max_block) { 
if (n == 0) 
fore the truncate completes, so it is now safe to propagate 
if (last_block == max_block) { 
if last_block is 
if (n == 1) {		/* direct blocks */ 
if (nr) { 
for it here. 
while (partial > chain) { 
if (nr) { 
if (nr) { 
if (nr) { 
for (i = 0, offset = 0; i < max; i++, i_data++, offset += inc) { 
if (*i_data == 0 || (offset + inc) <= first) 
if (level > 0) { 
if (!bh) { 
if (first > offset) { 
if (ret) { 
if (level == 0 || 
for (level = 0; level < 4; level++, max *= addr_per_block) { 
if (ret) 
if (count > max - first) 
if (level == 0) { 
file : ./test/kernel/fs/ext4/inode.c 
[ OK ] open : 4 ok... 
forms by Jakub Jelinek 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE && 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE && 
ify(struct inode *inode, struct ext4_inode *raw, 
if (EXT4_SB(inode->i_sb)->s_es->s_creator_os != 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE && 
if (EXT4_SB(inode->i_sb)->s_es->s_creator_os != 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE && 
for 
if (!EXT4_I(inode)->jinode) 
if (ext4_has_inline_data(inode)) 
fore we call here everything must be consistently dirtied against 
for blocks inside i_size since 
if i_nlink is zero. 
if (inode->i_nlink) { 
for reaping the inode might still have some pages to 
if we did not discard these 
if he tries to 
fore the transaction is checkpointed. So be 
if (ext4_should_journal_data(inode) && 
if (!is_bad_inode(inode)) 
if (ext4_should_order_data(inode)) 
if (is_bad_inode(inode)) 
for_truncate(inode)+3); 
if (IS_SYNC(inode)) 
if (err) { 
if (inode->i_blocks) 
fore there may not be 
if (!ext4_handle_has_enough_credits(handle, 3)) { 
if (err > 0) 
if (err != 0) { 
if'. 
if ext4_truncate() actually created an orphan record. 
if anything has gone wrong 
if the mark_dirty 
if (ext4_mark_inode_dirty(handle, inode)) 
ifdef CONFIG_QUOTA 
if 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if (unlikely(used > ei->i_reserved_data_blocks)) { 
if (unlikely(ei->i_allocated_meta_blocks > ei->i_reserved_meta_blocks)) { 
if (ei->i_reserved_data_blocks == 0) { 
for data blocks */ 
for fallocated blocks. 
if 
if ((ei->i_reserved_data_blocks == 0) && 
if (!ext4_data_block_valid(EXT4_SB(inode->i_sb), map->m_pblk, 
ifdef ES_AGGRESSIVE_TEST 
if (!(flags & EXT4_GET_BLOCKS_NO_LOCK)) 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) { 
if (!(flags & EXT4_GET_BLOCKS_NO_LOCK)) 
if (es_map->m_lblk != map->m_lblk || 
for inode: %lu " 
if /* ES_AGGRESSIVE_TEST */ 
if the blocks are already mapped. 
if create==0 and the blocks are pre-allocated and unwritten block, 
if plain look up failed (blocks have not been allocated), in 
ifdef ES_AGGRESSIVE_TEST 
if 
if (unlikely(map->m_len > INT_MAX)) 
if (unlikely(map->m_lblk >= EXT_MAX_BLOCKS)) 
if (ext4_es_lookup_extent(inode, map->m_lblk, &es)) { 
if (ext4_es_is_written(&es) || ext4_es_is_unwritten(&es)) { 
if (retval > map->m_len) 
if (ext4_es_is_delayed(&es) || ext4_es_is_hole(&es)) { 
ifdef ES_AGGRESSIVE_TEST 
if 
if we can get the block without requesting a new 
if (!(flags & EXT4_GET_BLOCKS_NO_LOCK)) 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) { 
if (retval > 0) { 
if (unlikely(retval != map->m_len)) { 
for inode " 
if (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) && 
if (ret < 0) 
if (!(flags & EXT4_GET_BLOCKS_NO_LOCK)) 
if (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) { 
if (ret != 0) 
if ((flags & EXT4_GET_BLOCKS_CREATE) == 0) 
if the blocks have already allocated 
if blocks have been preallocated 
if (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) 
if (!(flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN)) 
if the caller is from delayed allocation writeout path 
for allocation 
if (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) 
for EXT4 here because migrate 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) { 
if (retval > 0 && map->m_flags & EXT4_MAP_NEW) { 
format changing.  Force the migrate 
for non extent files. So we can update 
if ((retval > 0) && 
if (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) 
if (retval > 0) { 
if (unlikely(retval != map->m_len)) { 
for inode " 
if ((flags & EXT4_GET_BLOCKS_PRE_IO) && 
if (ext4_es_is_written(&es)) 
if (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) && 
if (ret < 0) 
if (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) { 
if (ret != 0) 
for direct IO at once. */ 
if (ext4_has_inline_data(inode)) 
if (flags && !(flags & EXT4_GET_BLOCKS_NO_LOCK) && !handle) { 
if (map.m_len > DIO_MAX_BLOCKS) 
if (IS_ERR(handle)) { 
if (ret > 0) { 
if (io_end && io_end->flag & EXT4_IO_END_UNWRITTEN) 
if (started) 
if create is zero 
if (create && err == 0) 
if (err < 0) 
if (err <= 0) 
if (unlikely(!bh)) { 
if (map.m_flags & EXT4_MAP_NEW) { 
if (!fatal && !buffer_uptodate(bh)) { 
if (!fatal) 
if (fatal) { 
if (!bh) 
if (buffer_uptodate(bh)) 
if (buffer_uptodate(bh)) 
for (bh = head, block_start = 0; 
if (block_end <= from || block_start >= to) { 
if (!ret) 
while thus 
if another thread had a 
for the tiny quotafile 
if (!buffer_mapped(bh) || buffer_freed(bh)) 
fore releasing a page lock and thus writeback cannot 
if (dirty) 
if (!ret && dirty) 
for addition to orphan list in case 
if (ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA)) { 
if (ret < 0) 
if (ret == 1) 
if the 
fore we start 
if needed) without using GFP_NOFS. 
if (!page) 
if (IS_ERR(handle)) { 
if (page->mapping != mapping) { 
while the page was unlocked */ 
if (ext4_should_dioread_nolock(inode)) 
if (!ret && ext4_should_journal_data(inode)) { 
if (ret) { 
fore 
if (pos + len > inode->i_size && ext4_can_truncate(inode)) 
if (pos + len > inode->i_size) { 
if (inode->i_nlink) 
if (ret == -ENOSPC && 
if (!buffer_mapped(bh) || buffer_freed(bh)) 
if (ext4_test_inode_state(inode, EXT4_STATE_ORDERED_MODE)) { 
if (ret) { 
if (ext4_has_inline_data(inode)) { 
if (ret < 0) 
while still holding page lock: 
if (pos + copied > inode->i_size) { 
if (pos + copied > EXT4_I(inode)->i_disksize) { 
forces lock 
if (i_size_changed) 
if (pos + len > inode->i_size && ext4_can_truncate(inode)) 
if (!ret) 
if (pos + len > inode->i_size) { 
if (inode->i_nlink) 
if (ext4_has_inline_data(inode)) 
if (copied < len) { 
if (!partial) 
if (new_i_size > inode->i_size) 
if (new_i_size > EXT4_I(inode)->i_disksize) { 
if (!ret) 
if (pos + len > inode->i_size && ext4_can_truncate(inode)) 
if (!ret) 
if (pos + len > inode->i_size) { 
if (inode->i_nlink) 
for a single block located at lblock 
if we fail to claim space. 
ford to run out of free blocks. 
if (ext4_claim_free_clusters(sbi, md_needed, 0)) { 
for data. 
if (ret) 
if we fail to claim space. 
ford to run out of free blocks. 
if (ext4_claim_free_clusters(sbi, md_needed + 1, 0)) { 
if (!to_free) 
if (unlikely(to_free > ei->i_reserved_data_blocks)) { 
if there aren't enough reserved blocks, then the 
if (ei->i_reserved_data_blocks == 0) { 
if (next_off > stop) 
if ((offset <= curr_off) && (buffer_delay(bh))) { 
while ((bh = bh->b_this_page) != head); 
for that cluster. */ 
while (num_clusters > 0) { 
if (sbi->s_cluster_ratio == 1 || 
if (mpd->first_page >= mpd->next_page) 
if (invalidate) { 
while (index <= end) { 
if (nr_pages == 0) 
for (i = 0; i < nr_pages; i++) { 
if (page->index > end) 
if (invalidate) { 
ifdef ES_AGGRESSIVE_TEST 
if 
if (ext4_es_lookup_extent(inode, iblock, &es)) { 
if (ext4_es_is_hole(&es)) { 
if (ext4_es_is_delayed(&es) && !ext4_es_is_unwritten(&es)) { 
if (retval > map->m_len) 
if (ext4_es_is_written(&es)) 
if (ext4_es_is_unwritten(&es)) 
ifdef ES_AGGRESSIVE_TEST 
if 
if we can get the block without requesting a new 
if (ext4_has_inline_data(inode)) { 
for this page, and let 
if ((EXT4_SB(inode->i_sb)->s_cluster_ratio > 1) && 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if (retval == 0) { 
for every block we're going to write. 
if (!(map->m_flags & EXT4_MAP_FROM_CLUSTER)) { 
if (ret) { 
if (ret) { 
if (ret) { 
if (retval > 0) { 
if (unlikely(retval != map->m_len)) { 
for inode " 
if (ret != 0) 
for a single block. 
if (ret <= 0) 
if (buffer_unwritten(bh)) { 
for partial write. 
if (inline_data) { 
if (inode_bh == NULL) 
if (!page_bufs) { 
if (IS_ERR(handle)) { 
if (inline_data) { 
if (ret == 0) 
if (!ret) 
if (!ext4_has_inline_data(inode)) 
if 
ified via mmap(), no one guarantees in which 
if we do with blocksize 1K 
ify 
if we have any buffer_heads that is either delay or 
if (page->index == size >> PAGE_CACHE_SHIFT) 
if (ext4_walk_page_buffers(NULL, page_bufs, 0, len, NULL, 
for_writepage(wbc, page); 
if we came here 
if (PageChecked(page) && ext4_should_journal_data(inode)) 
if (!io_submit.io_end) { 
for_writepage(wbc, page); 
if (page->index == size >> PAGE_CACHE_SHIFT) 
for_io(page); 
if (!err) 
for writeback and we haven't started the 
if the block has been added to the extent, false if the block couldn't be 
for writeback? */ 
if (map->m_len == 0) 
if (map->m_len == 0) { 
if (map->m_len >= MAX_WRITEPAGES_EXTENT_LEN) 
if (lblk == map->m_lblk + map->m_len && 
for IO or add them to extent 
if all buffers in this page were mapped and there's no 
if the caller can continue 
if (lblk >= blocks || !mpage_add_bh_to_extent(mpd, lblk, bh)) { 
if (mpd->map.m_len) 
while (lblk++, (bh = bh->b_this_page) != head); 
if (mpd->map.m_len == 0) { 
if (err < 0) 
for IO 
form writes to unwritten extents 
for IO. 
while (start <= end) { 
if (nr_pages == 0) 
for (i = 0; i < nr_pages; i++) { 
if (page->index > end) 
if (lblk < mpd->map.m_lblk) 
if (lblk >= mpd->map.m_lblk + mpd->map.m_len) { 
for IO. 
if (err > 0) 
if (buffer_delay(bh)) { 
while (lblk++, (bh = bh->b_this_page) != head); 
if dioread_nolock 
if (err < 0) { 
if 
if the blocks 
ifferent parts of the allocation call path.  This flag exists 
if (dioread_nolock) 
if (map->m_flags & (1 << BH_Delay)) 
if (err < 0) 
if (dioread_nolock && (map->m_flags & EXT4_MAP_UNWRITTEN)) { 
if (map->m_flags & EXT4_MAP_NEW) { 
for (i = 0; i < map->m_len; i++) 
for IO 
iff there is a fatal error and there 
if it is unwritten, we may need to convert 
forward progress is 
if (err < 0) { 
if (EXT4_SB(sb)->s_mount_flags & EXT4_MF_FS_ABORTED) 
if ext4_count_free_blocks() 
if ((err == -ENOMEM) || 
for " 
if (err == -ENOSPC) 
if (err < 0) 
while (map->m_len); 
if (disksize > EXT4_I(inode)->i_disksize) { 
if (disksize > i_size) 
if (disksize > EXT4_I(inode)->i_disksize) 
if (err2) 
if (!err) 
for one writepages 
ifferent extents. 
for pages 
formed by 
if (mpd->wbc->sync_mode == WB_SYNC_ALL || mpd->wbc->tagged_writepages) 
while (index <= end) { 
if (nr_pages == 0) 
for (i = 0; i < nr_pages; i++) { 
if (page->index > end) 
if (mpd->wbc->sync_mode == WB_SYNC_NONE && left <= 0) 
if (mpd->map.m_len > 0 && mpd->next_page != page->index) 
if (!PageDirty(page) || 
if (mpd->map.m_len == 0) 
if (err <= 0) 
for special inodes like journal inode on last iput() 
if (!mapping->nrpages || !mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) 
if (ext4_should_journal_data(inode)) { 
if the filesystem is mounted 
if that ever happens, we would want 
if (unlikely(sbi->s_mount_flags & EXT4_MF_FS_ABORTED)) { 
if (ext4_should_dioread_nolock(inode)) { 
for the 1st page, so 
if (ext4_has_inline_data(inode)) { 
if (IS_ERR(handle)) { 
if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX) 
if (wbc->range_cyclic) { 
if (writeback_index) 
if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages) 
for_writeback(mapping, mpd.first_page, mpd.last_page); 
while (!done && mpd.first_page <= mpd.last_page) { 
if (!mpd.io_submit.io_end) { 
ifference when 
if (IS_ERR(handle)) { 
if (!ret) { 
if (ret == -ENOSPC && sbi->s_journal) { 
force_commit_nested(sbi->s_journal); 
if (ret) 
if (!ret && !cycled && wbc->nr_to_write > 0) { 
if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0)) 
if we are running low 
if (dirty_clusters && (free_clusters < 2 * dirty_clusters)) 
if (2 * free_clusters < 3 * dirty_clusters || 
if (ext4_nonda_switch(inode->i_sb)) { 
if (ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA)) { 
if (ret < 0) 
if (ret == 1) 
if the 
fore we start 
if needed) without using GFP_NOFS. 
if (!page) 
if there is delayed block allocation. But we still need 
if (IS_ERR(handle)) { 
if (page->mapping != mapping) { 
while the page was unlocked */ 
if (ret < 0) { 
if (pos + len > inode->i_size) 
if (ret == -ENOSPC && 
if we should update i_disksize 
for (i = 0; i < idx; i++) 
if (!buffer_mapped(bh) || (buffer_delay(bh)) || buffer_unwritten(bh)) 
if (write_mode == FALL_BACK_TO_NONDELALLOC) 
if i_size 
if (copied && new_i_size > EXT4_I(inode)->i_disksize) { 
if (new_i_size > EXT4_I(inode)->i_disksize) 
if 
if (write_mode != CONVERT_INLINE_DATA && 
if (ret2 < 0) 
if (!ret) 
if (!page_has_buffers(page)) 
for a given inode. 
if (!EXT4_I(inode)->i_reserved_data_blocks && 
for now.  The filemap_flush() will 
for users of 
for 
for_writepage() but that 
ifying them because we wouldn't actually intend to 
for the I/O to complete. 
ific piece of data. 
if we see any bmap calls here on a modified, data-journaled file, 
for an inline file via the FIBMAP ioctl 
if (ext4_has_inline_data(inode)) 
if (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY) && 
for file 
if (EXT4_JOURNAL(inode) && 
if we run lilo or swapon on a freshly made file 
if mortal users could trigger this path at 
if (err) 
if (ext4_has_inline_data(inode)) 
if (ret == -EAGAIN) 
if (ext4_has_inline_data(inode)) 
forget about the pending dirtying 
if (offset == 0 && length == PAGE_CACHE_SIZE) 
for aops... */ 
if (PageChecked(page)) 
if (journal) 
for a DIO write or buffer write. 
if not async direct IO just return */ 
for inode %lu, iocb 0x%p, offset %llu, size %zd\n", 
if the machine crashes during the write. 
for reads and writes beyond i_size. */ 
for direct IO properly wait also for extent 
if (rw == WRITE) 
if (overwrite) { 
fore DIO complete the data IO. 
for async 
if (!is_sync_kiocb(iocb)) { 
if (!io_end) { 
for DIO. Will be dropped in ext4_end_io_dio() 
for current async direct 
if (overwrite) { 
form extent 
if (io_end) { 
if (ret <= 0 && ret != -EIOCBQUEUED && iocb->private) { 
if (ret > 0 && !overwrite && ext4_test_inode_state(inode, 
for non AIO case, since the IO is already 
if (err < 0) 
if (rw == WRITE) 
if we do a ovewrite dio */ 
if (ext4_should_journal_data(inode)) 
if (ext4_has_inline_data(inode)) 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if (test_opt(inode->i_sb, DELALLOC)) 
ified range exceeds 
if (!page) 
if it does not fall between 
if (length > max || length < 0) 
if (!page_has_buffers(page)) 
while (offset >= pos) { 
if (buffer_freed(bh)) { 
if (!buffer_mapped(bh)) { 
if (!buffer_mapped(bh)) { 
if (PageUptodate(page)) 
if (!buffer_uptodate(bh)) { 
if (!buffer_uptodate(bh)) 
if (ext4_should_journal_data(inode)) { 
if (err) 
if (ext4_should_journal_data(inode)) { 
if (ext4_test_inode_state(inode, EXT4_STATE_ORDERED_MODE)) 
if the file is later grown. 
if (start == end && 
if (partial_start) { 
if (err) 
if (partial_end != sb->s_blocksize - 1) 
if (S_ISREG(inode->i_mode)) 
if (S_ISDIR(inode->i_mode)) 
if (S_ISLNK(inode->i_mode)) 
if (!S_ISREG(inode->i_mode)) 
if (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) { 
if (ret) 
if (offset >= inode->i_size) 
if (offset + length > inode->i_size) { 
if (offset & (sb->s_blocksize - 1) || 
if we do any zeroing of 
if (ret < 0) 
if (last_block_offset > first_block_offset) 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
for_truncate(inode); 
if (IS_ERR(handle)) { 
if (ret) 
if (first_block >= stop_block) 
if (ret) { 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if (IS_SYNC(inode)) 
if (last_block_offset > first_block_offset) 
if (ei->jinode || !EXT4_SB(inode->i_sb)->s_journal) 
if (!ei->jinode) { 
if (unlikely(jinode != NULL)) 
fore* the restart of 
if (!(inode->i_state & (I_NEW|I_FREEING))) 
if (!ext4_can_truncate(inode)) 
if (inode->i_size == 0 && !test_opt(inode->i_sb, NO_AUTO_DA_ALLOC)) 
if (ext4_has_inline_data(inode)) { 
if (has_inline) 
for jbd2 */ 
if (ext4_inode_attach_jinode(inode) < 0) 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
for_truncate(inode); 
if (IS_ERR(handle)) { 
if (inode->i_size & (inode->i_sb->s_blocksize - 1)) 
if this 
while each transaction commits. 
if (ext4_orphan_add(handle, inode)) 
if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) 
if (IS_SYNC(inode)) 
if this was a real unlink then we were called by 
for us. 
if (inode->i_nlink) 
if (!ext4_valid_inum(sb, inode->i_ino)) 
if (!gdp) 
if (unlikely(!bh)) 
if (!buffer_uptodate(bh)) { 
if (buffer_write_io_error(bh) && !buffer_uptodate(bh)) 
if (buffer_uptodate(bh)) { 
while we waited */ 
formation of the inode in memory and this 
if (in_mem) { 
if (unlikely(!bitmap_bh)) 
forming two reads instead 
if (!buffer_uptodate(bitmap_bh)) { 
for (i = start; i < start + inodes_per_block; i++) { 
if (ext4_test_bit(i, bitmap_bh->b_data)) 
if (i == start + inodes_per_block) { 
if (EXT4_SB(sb)->s_inode_readahead_blks) { 
if (table > b) 
if (ext4_has_group_desc_csum(sb)) 
if (end > table) 
while (b <= end) 
if (!buffer_uptodate(bh)) { 
if (flags & EXT4_SYNC_FL) 
if (flags & EXT4_APPEND_FL) 
if (flags & EXT4_IMMUTABLE_FL) 
if (flags & EXT4_NOATIME_FL) 
if (flags & EXT4_DIRSYNC_FL) 
if (vfs_fl & S_SYNC) 
if (vfs_fl & S_APPEND) 
if (vfs_fl & S_IMMUTABLE) 
if (vfs_fl & S_NOATIME) 
if (vfs_fl & S_DIRSYNC) 
while (cmpxchg(&ei->i_flags, old_fl, new_fl) != old_fl); 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (ext4_test_inode_flag(inode, EXT4_INODE_HUGE_FILE)) { 
if (*magic == cpu_to_le32(EXT4_XATTR_MAGIC)) { 
if (!inode) 
if (!(inode->i_state & I_NEW)) 
if (ret < 0) 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) { 
if (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize > 
for inode metadata */ 
if (!ext4_inode_csum_verify(inode, raw_inode, ei)) { 
if (!(test_opt(inode->i_sb, NO_UID32))) { 
if the inode was active or not. 
if (inode->i_nlink == 0) { 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_64BIT)) 
ifdef CONFIG_QUOTA 
if 
for (block = 0; block < EXT4_N_BLOCKS; block++) 
if (journal) { 
if (journal->j_running_transaction) 
if (transaction) 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) { 
if (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) { 
if (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) { 
if (ei->i_file_acl && 
if (!ext4_has_inline_data(inode)) { 
if ((S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) || 
if (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) || 
if (ret) 
if (S_ISREG(inode->i_mode)) { 
if (S_ISDIR(inode->i_mode)) { 
if (S_ISLNK(inode->i_mode)) { 
if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode) || 
if (raw_inode->i_block[0]) 
if (ino == EXT4_BOOT_LOADER_INO) { 
if (i_blocks <= ~0U) { 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_HUGE_FILE)) 
if (i_blocks <= 0xffffffffffffULL) { 
for new inodes. */ 
if (!(test_opt(inode->i_sb, NO_UID32))) { 
if (!ei->i_dtime) { 
if (ext4_inode_blocks_set(handle, raw_inode, ei)) { 
if (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) 
if (ei->i_disksize != ext4_isize(raw_inode)) { 
if (ei->i_disksize > 0x7fffffffULL) { 
if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode)) { 
if (!ext4_has_inline_data(inode)) { 
for (block = 0; block < EXT4_N_BLOCKS; block++) 
if (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) { 
if (ei->i_extra_isize) { 
if (!err) 
if (set_large_file) { 
if (err) 
for O_SYNC files. 
if told to. 
if told to. 
for us to return without doing anything, 
for WB_SYNC_ALL 
for them to not do this.  The code: 
while `stuff()' is running, 
if (WARN_ON_ONCE(current->flags & PF_MEMALLOC)) 
if (EXT4_SB(inode->i_sb)->s_journal) { 
force transaction in WB_SYNC_NONE mode. Also 
if (wbc->sync_mode != WB_SYNC_ALL || wbc->for_sync) 
force_commit(inode->i_sb); 
if (err) 
for each inode. 
if (wbc->sync_mode == WB_SYNC_ALL && !wbc->for_sync) 
if (buffer_req(iloc.bh) && !buffer_uptodate(iloc.bh)) { 
for commit to finish and try again. 
for_tail_page_commit(struct inode *inode) 
if (offset > PAGE_CACHE_SIZE - (1 << inode->i_blkbits)) 
while (1) { 
if (!page) 
if (ret != -EBUSY) 
if (journal->j_committing_transaction) 
if (commit_tid) 
ify_change. 
ify 
if we are in ordered mode 
for pages under 
if (error) 
if (is_quota_modification(inode, attr)) 
if ((ia_valid & ATTR_UID && !uid_eq(attr->ia_uid, inode->i_uid)) || 
if (IS_ERR(handle)) { 
if (error) { 
if (attr->ia_valid & ATTR_UID) 
if (attr->ia_valid & ATTR_GID) 
if (attr->ia_valid & ATTR_SIZE && attr->ia_size != inode->i_size) { 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) { 
if (attr->ia_size > sbi->s_bitmap_maxbytes) 
if (IS_I_VERSION(inode) && attr->ia_size != inode->i_size) 
if (S_ISREG(inode->i_mode) && 
if (ext4_should_order_data(inode)) { 
if (error) 
if (IS_ERR(handle)) { 
if (ext4_handle_valid(handle)) { 
if (!error) 
if (!error) 
if (error) { 
for dio in flight.  Temporarily disable 
if (orphan) { 
for_tail_page_commit(inode); 
for commit 
if attr->ia_size == 
for cases like truncation of fallocated space 
if (attr->ia_valid & ATTR_SIZE) 
if (!rc) { 
if (orphan && inode->i_nlink) 
if (!rc && (ia_valid & ATTR_MODE)) 
if (!error) 
for such files, so tools like tar, rsync, 
if (unlikely(ext4_has_inline_data(inode))) 
if the block allocation is delayed 
fore the real block 
for this file. 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) 
for index blocks, block groups bitmaps and block group 
ifferent block groups 
ifferent block groups too. If they are contiguous, with flexbg, 
for superblock, inode, quota and xattr blocks 
if (groups > ngroups) 
if (groups > EXT4_SB(inode->i_sb)->s_gdb_count) 
for super block, inode, quota and xattr blocks */ 
ification of a single pages into a single transaction, 
for data blocks for journalled mode */ 
ification. 
for data blocks are not included here, as DIO 
if (IS_I_VERSION(inode)) 
if (!err) { 
if (err) { 
if (EXT4_I(inode)->i_extra_isize >= new_extra_isize) 
if (!ext4_test_inode_state(inode, EXT4_STATE_XATTR) || 
form any I/O.  This is a very good thing, 
ifferent journal. 
if (ext4_handle_valid(handle) && 
for that inode. 
force a large enough s_min_extra_isize. 
if ((jbd2_journal_extend(handle, 
if (ret) { 
if (mnt_count != 
if (!err) 
if (IS_ERR(handle)) 
if 0 
if (handle) { 
if (!err) { 
if (!err) 
if 
forgetting to revoke the old log record 
if (!journal) 
if (is_journal_aborted(journal)) 
for delalloc blocks 
if (val && test_opt(inode->i_sb, DELALLOC)) { 
if (err < 0) 
for all existing dio workers */ 
ify 
if (val) 
if (IS_ERR(handle)) 
if (test_opt(inode->i_sb, DELALLOC) && 
while (ret == -ENOSPC && 
if (page->mapping != mapping || page_offset(page) > size) { 
if (page->index == size >> PAGE_CACHE_SHIFT) 
if we have all the buffers mapped. This avoids the need to do 
if (page_has_buffers(page)) { 
for_stable_page(page); 
if (ext4_should_dioread_nolock(inode)) 
if (IS_ERR(handle)) { 
if (!ret && ext4_should_journal_data(inode)) { 
if (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries)) 
file : ./test/kernel/fs/ext4/namei.c 
[ OK ] open : 4 ok... 
forward compatibility hooks 
while searching them. 
if (unlikely(EXT4_SB(inode->i_sb)->s_max_dir_size_kb && 
if (!bh) 
if (err) { 
ify(struct inode *inode, 
if (!bh) { 
if (is_dx(inode)) { 
if (ext4_rec_len_from_disk(dirent->rec_len, 
if (!is_dx_block && type == INDEX) { 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
ified(bh)) 
for a index block; for 
if (is_dx_block && type == INDEX) { 
ified(bh); 
if (!is_dx_block) { 
ified(bh); 
ifndef assert 
if 
if 
if it should somehow get overlaid by a 
fore, the 
ifdef PARANOID 
while (d < top && d->rec_len) 
if (d != top) 
if 
for_csum(struct inode *inode) 
ify(struct inode *inode, struct ext4_dir_entry *dirent) 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!t) { 
for_csum(inode); 
if (t->det_checksum != ext4_dirent_csum(inode, dirent, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!t) { 
for_csum(inode); 
if (le16_to_cpu(dirent->rec_len) == EXT4_BLOCK_SIZE(inode->i_sb)) 
if (le16_to_cpu(dirent->rec_len) == 12) { 
if (le16_to_cpu(dp->rec_len) != 
if (root->reserved_zero || 
if (offset) 
ify(struct inode *inode, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!c) { 
if (count_offset + (limit * sizeof(struct dx_entry)) > 
for_csum(inode); 
if (t->dt_checksum != ext4_dx_csum(inode, dirent, count_offset, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!c) { 
if (count_offset + (limit * sizeof(struct dx_entry)) > 
for_csum(inode); 
fore the end of page 
for coalesce-on-delete flags 
if (EXT4_HAS_RO_COMPAT_FEATURE(dir->i_sb, 
if (EXT4_HAS_RO_COMPAT_FEATURE(dir->i_sb, 
ifdef DX_DEBUG 
for (i = 0; i < n; i++) { 
while ((char *) de < base + size) 
if (show_names) 
while (len--) printk("%c", *name++); 
for (i = 0; i < count; i++, entries++) 
if (!(bh = ext4_bread (NULL,dir, block, 0,&err))) continue; 
if (bcount) 
if /* DX_DEBUG */ 
for a directory leaf block to search. 
for this error code, and make sure it never gets reflected 
if (IS_ERR(bh)) { 
if (root->info.hash_version != DX_HASH_TEA && 
if (hinfo->hash_version <= DX_HASH_TEA) 
if (d_name) 
if (root->info.unused_flags & 1) { 
if ((indirect = root->info.indirect_levels) > 1) { 
if (dx_get_limit(entries) != dx_root_limit(dir, 
while (1) 
if (!count || count > dx_get_limit(entries)) { 
while (p <= q) 
if (dx_get_hash(m) > hash) 
if (0) // linear search cross check 
while (n--) 
if (dx_get_hash(++at) > hash) 
if (!indirect--) return frame; 
if (IS_ERR(bh)) { 
if (dx_get_limit(entries) != dx_node_limit (dir)) { 
while (frame >= frame_in) { 
if (*err == ERR_BAD_DX_DIR) 
if (frames[0].bh == NULL) 
if (((struct dx_root *) frames[0].bh->b_data)->info.indirect_levels) 
if the search 
if the next block starts with that 
for a specific file. 
if the caller should continue to search, 
while (1) { 
if (p == frames) 
if the next page has a 
for readdir 
if (start_hash) 
if ((hash & 1) == 0) { 
while (num_frames--) { 
if (IS_ERR(bh)) 
formation from a 
if (IS_ERR(bh)) 
for (; de < top; de = ext4_next_entry(de, dir->i_sb->s_blocksize)) { 
if ((hinfo->hash < start_hash) || 
if (de->inode == 0) 
if ((err = ext4_htree_store_dirent(dir_file, 
formation from a 
if (!(ext4_test_inode_flag(dir, EXT4_INODE_INDEX))) { 
if (hinfo.hash_version <= DX_HASH_TEA) 
if (ext4_has_inline_data(dir)) { 
if (has_inline_data) { 
if (!frame) 
if (!start_hash && !start_minor_hash) { 
if ((err = ext4_htree_store_dirent(dir_file, 0, 0, de)) != 0) 
if (start_hash < 2 || (start_hash ==2 && start_minor_hash==0)) { 
if ((err = ext4_htree_store_dirent(dir_file, 2, 0, de)) != 0) 
while (1) { 
if (ret < 0) { 
if (ret < 0) { 
if:  (a) there are no more entries, or 
if ((ret == 0) || 
while ((char *) de < base + blocksize) { 
while (count > 2) { 
if (count - 9 < 2) /* 9, 10 -> 11 */ 
for (p = top, q = p - count; q >= map; p--, q--) 
while (q-- > map) { 
while(more); 
for success, 0 for failure. 
if (len != de->name_len) 
if (!de->inode) 
if not found, -1 on failure, and 1 on success 
while ((char *) de < dlimit) { 
if ((char *) de + namelen <= dlimit && 
if (ext4_check_dir_entry(dir, NULL, de, bh, bh->b_data, 
if (de_len <= 0) 
if (!is_dx(dir)) 
if (block == 0) 
if (de->inode == 0 && 
ified directory with the wanted name. It 
if you want to. 
if (namelen > EXT4_NAME_LEN) 
if (ext4_has_inline_data(dir)) { 
if (has_inline_data) { 
if ((namelen <= 2) && (name[0] == '.') && 
if (is_dx(dir)) { 
if the error was file not found, 
if (bh || (err != ERR_BAD_DX_DIR)) 
if (start >= nblocks) 
if (ra_ptr >= ra_max) { 
for (ra_max = 0; ra_max < NAMEI_RA_SIZE; ra_max++) { 
if we reach the end of the 
if (b >= nblocks || (num && block == start)) { 
if (bh) 
if ((bh = bh_use[ra_ptr++]) == NULL) 
if (!buffer_uptodate(bh)) { 
for the best */ 
if (!buffer_verified(bh) && 
ify(dir, 
ified(bh); 
if (i == 1) { 
if (i < 0) 
if (++block >= nblocks) 
while (block != start); 
while we were searching, then 
if (block < nblocks) { 
for (; ra_ptr < ra_max; ra_ptr++) 
if (!(frame = dx_probe(d_name, dir, &hinfo, frames, err))) 
if (IS_ERR(bh)) { 
if (retval == 1) { 	/* Success! */ 
if (retval == -1) { 
if we should continue to search */ 
if (retval < 0) { 
while (retval == 1); 
if (dentry->d_name.len > EXT4_NAME_LEN) 
if (bh) { 
if (!ext4_valid_inum(dir->i_sb, ino)) { 
if (unlikely(ino == dir->i_ino)) { 
if (inode == ERR_PTR(-ESTALE)) { 
if (!bh) 
if (!ext4_valid_inum(child->d_inode->i_sb, ino)) { 
while (count--) { 
while ((char*)de < base + blocksize) { 
if (de->inode && de->name_len) { 
if (de > to) 
for a new dir entry. 
if (EXT4_HAS_RO_COMPAT_FEATURE(dir->i_sb, 
if (IS_ERR(bh2)) { 
if (err) 
if (err) 
for (i = count-1; i >= 0; i--) { 
if (size + map[i].size/2 > blocksize/2) 
if (csum_size) { 
if (hinfo->hash >= hash2) 
if (err) 
if (err) 
while ((char *) de <= top) { 
if (ext4_match(namelen, name, de)) 
if ((de->inode ? rlen - nlen : rlen) >= reclen) 
if ((char *) de > top) 
if (de->inode) { 
for new directory entry.  If de is NULL, then 
if no space is available, and -EIO 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!de) { 
if (err) 
if (err) { 
for journaling */ 
ifferent from the directory change time. 
if (err) 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (retval) { 
if ((char *) de >= (((char *) root) + blocksize)) { 
for '..'"); 
for the 0th block's dirents */ 
if (IS_ERR(bh2)) { 
while ((char *)(de2 = ext4_next_entry(de, blocksize)) < top) 
if (csum_size) { 
for dx_probe */ 
if (hinfo.hash_version <= DX_HASH_TEA) 
if (!de) { 
if the block split failed, we have to properly write 
ified directory, using the same 
while you slept. 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!dentry->d_name.len) 
if (ext4_has_inline_data(dir)) { 
if (retval < 0) 
if (retval == 1) { 
if (is_dx(dir)) { 
if (!retval || (retval != ERR_BAD_DX_DIR)) 
for (block = 0; block < blocks; block++) { 
if (IS_ERR(bh)) 
if (retval != -ENOSPC) { 
if (blocks == 1 && !dx_fallback && 
if (IS_ERR(bh)) 
if (csum_size) { 
if (retval == 0) 
for success, or a negative error value 
if (!frame) 
if (IS_ERR(bh)) { 
if (err) 
if (err != -ENOSPC) 
for now just split */ 
if (dx_get_count(entries) == dx_get_limit(entries)) { 
if (levels && (dx_get_count(frames->entries) == 
if (IS_ERR(bh2)) { 
if (err) 
if (levels) { 
if (err) 
if (at - entries >= icount1) { 
if (err) 
if (err) 
if (err) { 
if (!de) 
while (i < buf_size - csum_size) { 
if (de == de_del)  { 
if (ext4_has_inline_data(dir)) { 
if (has_inline_data) 
if (EXT4_HAS_RO_COMPAT_FEATURE(dir->i_sb, 
if (unlikely(err)) 
if (err) 
if (unlikely(err)) 
if (err != -ENOENT) 
if 1) nlinks > EXT4_LINK_MAX or 2) nlinks == 2, 
if (is_dx(inode) && inode->i_nlink > 1) { 
if (inode->i_nlink >= EXT4_LINK_MAX || inode->i_nlink == 2) { 
if (!S_ISDIR(inode->i_mode) || inode->i_nlink > 2) 
if (!err) { 
for the new file, but it 
formation 
if (!IS_ERR(inode)) { 
if (!err && IS_DIRSYNC(dir)) 
if (handle) 
if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries)) 
if (!new_valid_dev(rdev)) 
if (!IS_ERR(inode)) { 
if (!err && IS_DIRSYNC(dir)) 
if (handle) 
if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries)) 
if (!IS_ERR(inode)) { 
if (err) 
if (handle) 
if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries)) 
if (!dotdot_real_len) 
if (EXT4_HAS_RO_COMPAT_FEATURE(dir->i_sb, 
if (ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA)) { 
if (err < 0 && err != -ENOSPC) 
if (!err) 
if (IS_ERR(dir_block)) 
if (err) 
if (csum_size) { 
if (err) 
ified(dir_block); 
if (EXT4_DIR_LINK_MAX(dir)) 
if (IS_ERR(inode)) 
if (err) 
if (!err) 
if (err) { 
if (err) 
if (IS_DIRSYNC(dir)) 
if (handle) 
if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries)) 
ified directory is empty (for rmdir) 
if (ext4_has_inline_data(inode)) { 
if (has_inline_data) 
if (inode->i_size < EXT4_DIR_REC_LEN(1) + EXT4_DIR_REC_LEN(2)) { 
if (IS_ERR(bh)) 
if (le32_to_cpu(de->inode) != inode->i_ino || 
while (offset < inode->i_size) { 
if (IS_ERR(bh)) 
if (ext4_check_dir_entry(inode, NULL, de, bh, 
if (le32_to_cpu(de->inode)) { 
fore the 
if (!sbi->s_journal) 
if inode already is on orphan list. This is a big speedup 
if (!list_empty(&EXT4_I(inode)->i_orphan)) 
for files with data blocks 
if (err) 
if (err) 
ification. 
if (!NEXT_ORPHAN(inode) || NEXT_ORPHAN(inode) > 
if (dirty) { 
if (!err) 
if (err) { 
if 
if (!sbi->s_journal && !(sbi->s_mount_state & EXT4_ORPHAN_FS)) 
fore taking global s_orphan_lock. */ 
if (handle) { 
fore taking global s_orphan_lock */ 
if (!handle || err) { 
if (prev == &sbi->s_orphan) { 
if (err) { 
if (err) { 
if (err) 
fore so that eventual writes go in 
if (!bh) 
if (le32_to_cpu(de->inode) != inode->i_ino) 
if (!empty_dir(inode)) 
if (IS_ERR(handle)) { 
if (IS_DIRSYNC(dir)) 
if (retval) 
if (!EXT4_DIR_LINK_EMPTY(inode)) 
if (handle) 
fore so that eventual writes go 
if (!bh) 
if (le32_to_cpu(de->inode) != inode->i_ino) 
if (IS_ERR(handle)) { 
if (IS_DIRSYNC(dir)) 
if (!inode->i_nlink) { 
if (retval) 
if (!inode->i_nlink) 
if (handle) 
if (l > dir->i_sb->s_blocksize) 
if (l > EXT4_N_BLOCKS * 4) { 
if (IS_ERR(inode)) 
if (l > EXT4_N_BLOCKS * 4) { 
if we are running out of space 
if (err) 
if (err) 
ified 
if (IS_ERR(handle)) { 
if (err) { 
format for fast symlink */ 
if (!err && IS_DIRSYNC(dir)) 
if (handle) 
if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries)) 
if (inode->i_nlink >= EXT4_LINK_MAX) 
if (IS_ERR(handle)) 
if (IS_DIRSYNC(dir)) 
if (!err) { 
for tmpfile being 
if (inode->i_nlink == 1) 
if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries)) 
if it is inlined or the 1st block 
if (!ext4_has_inline_data(inode)) { 
if (IS_ERR(bh)) { 
for "dentry" */ 
if it's a directory */ 
if (!ent->dir_bh) 
if (le32_to_cpu(ent->parent_de->inode) != ent->dir->i_ino) 
if (!ent->dir_inlined) { 
if (retval) { 
if (retval) 
if (EXT4_HAS_INCOMPAT_FEATURE(ent->dir->i_sb, 
if (!ent->inlined) { 
if (unlikely(retval)) { 
if (bh) { 
if (le32_to_cpu(ent->de->inode) != ent->inode->i_ino || 
if (retval == -ENOENT) { 
if (retval) { 
if (ent->dir_nlink_delta) { 
while new_{dentry,inode) refers to the destination dentry/inode 
fore so that eventual writes go 
if (new.inode) 
for inode number is _not_ due to possible IO errors. 
if (!old.bh || le32_to_cpu(old.de->inode) != old.inode->i_ino) 
if (new.bh) { 
if (new.inode && !test_opt(new.dir->i_sb, NO_AUTO_DA_ALLOC)) 
if (IS_ERR(handle)) 
if (IS_DIRSYNC(old.dir) || IS_DIRSYNC(new.dir)) 
if (S_ISDIR(old.inode->i_mode)) { 
if (!empty_dir(new.inode)) 
if (new.dir != old.dir && EXT4_DIR_LINK_MAX(new.dir)) 
if (retval) 
if (!new.bh) { 
if (retval) 
if (retval) 
for inodes on a 
if (new.inode) { 
if (old.dir_bh) { 
if (retval) 
if (new.inode) { 
for many-linked dirs */ 
if (new.inode) { 
if (!new.inode->i_nlink) 
if (handle) 
for inode number is _not_ due to possible IO errors. 
if (!old.bh || le32_to_cpu(old.de->inode) != old.inode->i_ino) 
if (!new.bh || le32_to_cpu(new.de->inode) != new.inode->i_ino) 
if (IS_ERR(handle)) 
if (IS_DIRSYNC(old.dir) || IS_DIRSYNC(new.dir)) 
if (S_ISDIR(old.inode->i_mode)) { 
if (retval) 
if (S_ISDIR(new.inode->i_mode)) { 
if (retval) 
ified if this is a cross directory rename. 
if (old.dir != new.dir && old.is_dir != new.is_dir) { 
if ((old.dir_nlink_delta > 0 && EXT4_DIR_LINK_MAX(old.dir)) || 
if (retval) 
if (retval) 
for inodes on a 
if (old.dir_bh) { 
if (retval) 
if (new.dir_bh) { 
if (retval) 
if (handle) 
if (flags & ~(RENAME_NOREPLACE | RENAME_EXCHANGE)) 
if (flags & RENAME_EXCHANGE) { 
file : ./test/kernel/fs/ext4/balloc.c 
[ OK ] open : 4 ok... 
for a given block number 
if (test_opt2(sb, STD_GROUP_SIZE)) 
if (offsetp) 
if (blockgrpp) 
if so 
for file system metadata; this 
if the block is in the block group.  If it 
for in the clusters used for the base metadata cluster, or 
for the allocation bitmap or inode table explicitly. 
for *very* 
if (ext4_block_in_group(sb, ext4_block_bitmap(sb, gdp), block_group)) { 
if (block_cluster < num_clusters) 
if (block_cluster == num_clusters) { 
if (ext4_block_in_group(sb, ext4_inode_bitmap(sb, gdp), block_group)) { 
if (inode_cluster < num_clusters) 
if (inode_cluster == num_clusters) { 
for (i = 0; i < sbi->s_itb_per_group; i++) { 
if ((c < num_clusters) || (c == inode_cluster) || 
if (c == num_clusters) { 
if (block_cluster != -1) 
if (inode_cluster != -1) 
if (block_group == ext4_get_groups_count(sb) - 1) { 
if (!ext4_group_desc_csum_verify(sb, block_group, gdp)) { 
for group %u", block_group); 
if (!EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
if (!EXT4_MB_GRP_IBITMAP_CORRUPT(grp)) { 
for (bit = 0; bit < bit_max; bit++) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG)) 
for block and inode bitmaps, and inode table */ 
if (!flex_bg || ext4_block_in_group(sb, tmp, block_group)) 
if (!flex_bg || ext4_block_in_group(sb, tmp, block_group)) 
for (; tmp < ext4_inode_table(sb, gdp) + 
if (!flex_bg || ext4_block_in_group(sb, tmp, block_group)) 
if the number of blocks within the group is less than 
for blocks, 1 bitmap 
if (block_group >= ngroups) { 
if (!sbi->s_group_desc[group_desc]) { 
if (bh) 
if 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG)) { 
for those groups 
ify they are set. 
if (!ext4_test_bit(EXT4_B2C(sbi, offset), bh->b_data)) 
if (!ext4_test_bit(EXT4_B2C(sbi, offset), bh->b_data)) 
if (next_zero_bit < 
for inode tables */ 
if (buffer_verified(bh)) 
if (unlikely(blk != 0)) { 
if (!EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
if (unlikely(!ext4_block_bitmap_csum_verify(sb, block_group, 
if (!EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
ified(bh); 
for a given block_group,and validate the 
if (!desc) 
if (unlikely(!bh)) { 
for block bitmap - " 
if (bitmap_uptodate(bh)) 
if (bitmap_uptodate(bh)) { 
ify; 
if (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) { 
if (buffer_uptodate(bh)) { 
if not uninit if bh is uptodate, 
ify; 
for reading 
ify: 
if (buffer_verified(bh)) 
if (!buffer_new(bh)) 
if (!desc) 
if (!buffer_uptodate(bh)) { 
if block bitmap is invalid */ 
for error just in case errors=continue. */ 
if (!bh) 
if (ext4_wait_block_bitmap(sb, block_group, bh)) { 
if filesystem has nclusters free & available for allocation. 
ift only. 
if (free_clusters - (nclusters + rsv + dirty_clusters) < 
for current 
if (free_clusters >= (rsv + nclusters + dirty_clusters)) 
if (uid_eq(sbi->s_resuid, current_fsuid()) || 
if (free_clusters >= (nclusters + dirty_clusters + 
if we can dip into reserved pool */ 
if (free_clusters >= (nclusters + dirty_clusters)) 
if (ext4_has_free_clusters(sbi, nclusters, flags)) { 
if 
for the current or committing transaction to complete, and then 
if the total number of retries exceed three times, return FALSE. 
if (!ext4_has_free_clusters(EXT4_SB(sb), 1, 0) || 
force_commit_nested(EXT4_SB(sb)->s_journal); 
if (count) 
for the allocated meta blocks.  We will never 
if (!(*errp) && 
ifdef EXT4FS_DEBUG 
for (i = 0; i < ngroups; i++) { 
if (!gdp) 
if (EXT4_SB(sb)->s_group_info) 
if (!grp || !EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
if (bitmap_bh == NULL) 
for (i = 0; i < ngroups; i++) { 
if (!gdp) 
if (EXT4_SB(sb)->s_group_info) 
if (!grp || !EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
if 
while (1) { 
if (a == b) 
if ((a % b) != 0) 
for filesystem 
if (group == 0) 
if (EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_SPARSE_SUPER2)) { 
if ((group <= 1) || !EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (!(group & 1)) 
if (test_root(group, 3) || (test_root(group, 5)) || 
if (group == first || group == first + 1 || group == last) 
if (!ext4_bg_has_super(sb, group)) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb,EXT4_FEATURE_INCOMPAT_META_BG)) 
for filesystem 
ifferent number of descriptor blocks in each group. 
if (!EXT4_HAS_INCOMPAT_FEATURE(sb,EXT4_FEATURE_INCOMPAT_META_BG) || 
for superblock and gdt backups in this group */ 
if (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_META_BG) || 
if (num) { 
for block allocation 
for a 
if (flex_size >= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) { 
for directories and special files.  Regular 
if (S_ISREG(inode->i_mode)) 
if (test_opt(inode->i_sb, DELALLOC)) 
if (bg_start + EXT4_BLOCKS_PER_GROUP(inode->i_sb) <= last_block) 
file : ./test/kernel/fs/ext4/mballoc.c 
[ OK ] open : 4 ok... 
ify 
for more details. 
if not, write to the Free Software 
ifdef CONFIG_EXT4_DEBUG 
for ext4's mballoc"); 
for metadata in few groups 
if no free space left (policy?) 
for superuser 
for better group selection 
for multiple number of blocks 
for having small file use group preallocation is to 
for this particular inode. The inode prealloc space is 
for this prealloc space 
for this prealloc space (in clusters) 
ify the values associated to inode prealloc space except 
if we 
for having a per cpu locality group is to reduce the contention 
ifferent 
formation is attached to buddy cache inode so that 
formation involve 
for bitmap and buddy information.  So for each group we 
formation regarding groups_per_page 
for count number of blocks in the buddy cache. If we were able 
fore allocating blocks via buddy cache we normalize the request 
for non-bigalloc file systems, it is 
if the request len is power of 
for contiguous block in 
ific group using bitmap for best extents. The 
for a best 
for the blocks starts with 
for allocation. ext4_mb_good_group explains how the groups are 
for the first 
for the 
ific inode and can be used for this inode only. 
fore taking some block from descriptor, one must 
ific locality group which does not translate to 
for any inode. thus 
if we follow this strict logic, then all operations above should be atomic. 
formance on high-end SMP hardware. let's try to relax it using 
if buddy is referenced, it's already initialized 
while block is used in buddy and the buddy is referenced, 
if on-disk has 
if buddy has same bit set or/and PA covers corresponded 
for PA are allocated in the buddy, buddy must be referenced 
ifferent PAs covering different blocks 
while it is no discard is possible 
fore 
ify buddy 
for given object (inode, locality group): 
for given group: 
for groupinfo data structures based on the 
if BITS_PER_LONG == 64 
if BITS_PER_LONG == 32 
if 
if (ret > max) 
if (ret > max) 
if (order > e4b->bd_blkbits + 1) { 
if (order == 0) { 
ifdef DOUBLE_CHECK 
if (unlikely(e4b->bd_info->bb_bitmap == NULL)) 
for (i = 0; i < count; i++) { 
if (unlikely(e4b->bd_info->bb_bitmap == NULL)) 
for (i = 0; i < count; i++) { 
if (memcmp(e4b->bd_info->bb_bitmap, bitmap, e4b->bd_sb->s_blocksize)) { 
for (i = 0; i < e4b->bd_sb->s_blocksize; i++) { 
if 
if (!(assert)) {						\ 
while (0) 
if (mb_check_counter++ % 100 != 0) 
while (order > 1) { 
for (i = 0; i < max; i++) { 
if (!mb_test_bit(i << 1, buddy2)) { 
if (!mb_test_bit((i << 1) + 1, buddy2)) { 
for (j = 0; j < (1 << order); j++) { 
for (i = 0; i < max; i++) { 
if (fstart == -1) { 
for (j = 0; j < e4b->bd_blkbits + 1; j++) { 
for_each(cur, &grp->bb_prealloc_list) { 
for (i = 0; i < pa->pa_len; i++) 
if 
for corresponded chunk size. 
while (len > 0) { 
if (max < min) 
if (min > 0) 
for (i = bits; i >= 0; i--) { 
for_stack 
while (i < max) { 
if (len > 1) 
if (i < max) 
if (free != grp->bb_free) { 
if (!EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
while ((buddy = mb_find_buddy(e4b, order++, &count))) { 
formation is attached the buddy cache inode 
formation involve 
for bitmap and buddy information. 
formation regarding groups_per_page which 
for this page; do not hold this lock when calling this routine! 
if (groups_per_page == 0) 
if (groups_per_page > 1) { 
if (bh == NULL) { 
for (i = 0, group = first_group; i < groups_per_page; i++, group++) { 
if (PageUptodate(page) && !EXT4_MB_GRP_NEED_INIT(grinfo)) { 
if (!(bh[i] = ext4_read_block_bitmap_nowait(sb, group))) { 
for group %u\n", group); 
for (i = 0, group = first_group; i < groups_per_page; i++, group++) { 
for (i = 0; i < blocks_per_page; i++) { 
if (group >= ngroups) 
if (!bh[group - first_group]) 
formation regarding this 
if ((first_block + i) & 1) { 
for group %u in page %lu/%x\n", 
for group %u in page %lu/%x\n", 
formation can be 
if (bh) { 
for (i = 0; i < groups_per_page; i++) 
if (bh != &bhs) 
formation in consecutive blocks. 
if (!page) 
if (blocks_per_page >= 2) { 
if (!page) 
if (e4b->bd_bitmap_page) { 
if (e4b->bd_buddy_page) { 
for this page; do not hold the BG lock when 
for_stack 
if (ret || !EXT4_MB_GRP_NEED_INIT(this_grp)) { 
if (ret) 
if (!PageUptodate(page)) { 
if (e4b.bd_buddy_page == NULL) { 
force 
if (ret) 
if (!PageUptodate(page)) { 
for this page; do not hold the BG lock when 
for_stack int 
if (unlikely(EXT4_MB_GRP_NEED_INIT(grp))) { 
if (ret) 
formation in consecutive blocks. 
if (page == NULL || !PageUptodate(page)) { 
for it to initialize. 
if (page) { 
if (!PageUptodate(page)) { 
if (ret) { 
if (page == NULL) { 
if (!PageUptodate(page)) { 
if (page == NULL || !PageUptodate(page)) { 
if (page) { 
if (!PageUptodate(page)) { 
if (ret) { 
if (page == NULL) { 
if (!PageUptodate(page)) { 
if (page) 
if (e4b->bd_bitmap_page) 
if (e4b->bd_buddy_page) 
if (e4b->bd_bitmap_page) 
if (e4b->bd_buddy_page) 
for_block(struct ext4_buddy *e4b, int block) 
while (order <= e4b->bd_blkbits + 1) { 
if (!mb_test_bit(block, bb)) { 
while (cur < len) { 
if any, -1 otherwise 
while (cur < len) { 
if (*addr != (__u32)(-1) && zero_bit == -1) 
if (!mb_test_and_clear_bit(cur, bm) && zero_bit == -1) 
while (cur < len) { 
if (mb_test_bit(*bit + side, bitmap)) { 
while (buddy) { 
form buddies on 
ift range to [0; 2], go up and do the same. 
if (first & 1) 
if (!(last & 1)) 
if (first > last) 
if (first == last || !(buddy2 = mb_find_buddy(e4b, order, &max))) { 
if the block group is corrupt. */ 
if (first < e4b->bd_info->bb_first_free) 
if (first != 0) 
if (last + 1 < EXT4_SB(sb)->s_mb_maxs[0]) 
if (unlikely(block != -1)) { 
if (!EXT4_MB_GRP_BBITMAP_CORRUPT(e4b->bd_info)) 
if (left_is_free && right_is_free) 
if (!left_is_free && !right_is_free) 
if neighbours are to be coaleasced, 
if (first & 1) { 
if (!(last & 1)) { 
if (first <= last) 
if (mb_test_bit(block, buddy)) { 
for_block(e4b, block); 
ifference from given start */ 
while (needed > ex->fe_len && 
if (block + 1 >= max) 
if (mb_test_bit(next, e4b->bd_bitmap)) 
for_block(e4b, next); 
if (e4b->bd_info->bb_first_free == start) 
if (start != 0) 
if (start + len < EXT4_SB(e4b->bd_sb)->s_mb_maxs[0]) 
if (mlen && max) 
if (!mlen && !max) 
while (len) { 
if (((start >> ord) << ord) == start && len >= (1 << ord)) { 
for history */ 
for history */ 
for this 
for subsequent stream allocation */ 
for general purposes allocation 
if (ac->ac_status == AC_STATUS_FOUND) 
for a whole year 
if (ac->ac_found > sbi->s_mb_max_to_scan && 
if (bex->fe_len < gex->fe_len) 
if ((finish_group || ac->ac_found > sbi->s_mb_min_to_scan) 
if (max >= gex->fe_len) { 
if new one is better, then it's stored 
if (unlikely(ac->ac_flags & EXT4_MB_HINT_FIRST)) { 
if (ex->fe_len == gex->fe_len) { 
if (bex->fe_len == 0) { 
if (bex->fe_len < gex->fe_len) { 
if (ex->fe_len > bex->fe_len) 
if (ex->fe_len > gex->fe_len) { 
if (ex->fe_len < bex->fe_len) 
for_stack 
if (err) 
if (max > 0) { 
for_stack 
if (!(ac->ac_flags & EXT4_MB_HINT_TRY_GOAL)) 
if (grp->bb_free == 0) 
if (err) 
if (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(e4b->bd_info))) { 
if (max >= ac->ac_g_ex.fe_len && ac->ac_g_ex.fe_len == sbi->s_stripe) { 
if (do_div(start, sbi->s_stripe) == 0) { 
if (max >= ac->ac_g_ex.fe_len) { 
if (max > 0 && (ac->ac_flags & EXT4_MB_HINT_MERGE)) { 
for_stack 
for (i = ac->ac_2order; i <= sb->s_blocksize_bits + 1; i++) { 
if (EXT4_SB(sb)->s_mb_stats) 
for_stack 
while (free && ac->ac_status == AC_STATUS_CONTINUE) { 
if (i >= EXT4_CLUSTERS_PER_GROUP(sb)) { 
if (free < ex.fe_len) { 
iffers. This mostly 
for storages like raid5 
for_stack 
while (i < EXT4_CLUSTERS_PER_GROUP(sb)) { 
if (max >= sbi->s_stripe) { 
if (free == 0) 
if (cr <= 2 && free < ac->ac_g_ex.fe_len) 
if (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(grp))) 
if the grp has never been initialized */ 
if (ret) 
if (fragments == 0) 
for data files */ 
if ((ac->ac_2order > ac->ac_sb->s_blocksize_bits+1) || 
if (grp->bb_largest_free_order < ac->ac_2order) 
if ((free / fragments) >= ac->ac_g_ex.fe_len) 
if (free >= ac->ac_g_ex.fe_len) 
for_stack int 
if (!(ext4_test_inode_flag(ac->ac_inode, EXT4_INODE_EXTENTS))) 
if (err || ac->ac_status == AC_STATUS_FOUND) 
if (unlikely(ac->ac_flags & EXT4_MB_HINT_GOAL_ONLY)) 
if the fe_len is a power of 2 
if the order of the request 
if (i >= sbi->s_mb_order2_reqs) { 
if fe_len is exactly power of 2 
if ((ac->ac_g_ex.fe_len & (~(1 << (i - 1)))) == 0) 
if stream allocation is enabled, use global goal */ 
for (; cr < 4 && ac->ac_status == AC_STATUS_CONTINUE; cr++) { 
for the right group start 
for (i = 0; i < ngroups; group++, i++) { 
ificially restricted ngroups for non-extent 
if (group >= ngroups) 
if (!ext4_mb_good_group(ac, group, cr)) 
if (err) 
if (!ext4_mb_good_group(ac, group, cr)) { 
if (cr == 0 && ac->ac_2order < sb->s_blocksize_bits+2) 
if (cr == 1 && sbi->s_stripe && 
if (ac->ac_status != AC_STATUS_CONTINUE) 
if (ac->ac_b_ex.fe_len > 0 && ac->ac_status != AC_STATUS_FOUND && 
if (ac->ac_status != AC_STATUS_FOUND) { 
if (*pos < 0 || *pos >= ext4_get_groups_count(sb)) 
if (*pos < 0 || *pos >= ext4_get_groups_count(sb)) 
if (group == 0) 
if not already loaded. */ 
if (err) { 
if (buddy_loaded) 
for (i = 0; i <= 13; i++) 
if (rc == 0) { 
ified number 
if (size <= sbi->s_group_info_size) 
if (!new_groupinfo) { 
if (sbi->s_group_info) { 
for %d meta_bg's\n",  
for the given group. */ 
if this group is the first of a reserved block. 
if (group % EXT4_DESC_PER_BLOCK(sb) == 0) { 
if (meta_group_info == NULL) { 
for a buddy group"); 
if (meta_group_info[i] == NULL) { 
if (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) { 
ifdef DOUBLE_CHECK 
if 
if (group % EXT4_DESC_PER_BLOCK(sb) == 0) { 
if (err) 
if (sbi->s_buddy_cache == NULL) { 
for the buddy cache inode number.  This inode is 
if it ever shows up during debugging. */ 
for (i = 0; i < ngroups; i++) { 
if (desc == NULL) { 
if (ext4_mb_add_groupinfo(sb, i, desc) != 0) 
while (i-- > 0) 
while (i-- > 0) 
for (i = 0; i < NR_GRPINFO_CACHES; i++) { 
if (cache_index >= NR_GRPINFO_CACHES) 
if (unlikely(cache_index < 0)) 
if (ext4_groupinfo_caches[cache_index]) { 
if (!cachep) { 
for groupinfo slab cache\n"); 
if (sbi->s_mb_offsets == NULL) { 
if (sbi->s_mb_maxs == NULL) { 
if (ret < 0) 
while (i <= sb->s_blocksize_bits + 1); 
for 4k block 
if the cluster size 
for cluster sizes up to 64k, and after 
if (sbi->s_stripe > 1) { 
if (sbi->s_locality_groups == NULL) { 
for_each_possible_cpu(i) { 
for (j = 0; j < PREALLOC_TB_SIZE; j++) 
for buddy data */ 
if (ret != 0) 
if (sbi->s_proc) 
for_each_safe(cur, tmp, &grp->bb_prealloc_list) { 
if (count) 
if (sbi->s_proc) 
if (sbi->s_group_info) { 
for (i = 0; i < ngroups; i++) { 
ifdef DOUBLE_CHECK 
if 
for (i = 0; i < num_meta_group_infos; i++) 
if (sbi->s_buddy_cache) 
if (sbi->s_mb_stats) { 
if (test_opt(sb, DISCARD)) { 
if (err && err != -EOPNOTSUPP) 
for the group so that the next 
if (!test_opt(sb, DISCARD)) 
if (!db->bb_free_root.rb_node) { 
if (ext4_pspace_cachep == NULL) 
if (ext4_ac_cachep == NULL) { 
if (ext4_free_data_cachep == NULL) { 
for completion of call_rcu()'s on ext4_pspace_cachep 
if success or error code 
for_stack int 
if (!bitmap_bh) 
if (err) 
if (!gdp) 
if (err) 
if (!ext4_data_block_valid(sbi, block, len)) { 
if (!err) 
ifdef AGGRESSIVE_CHECK 
for (i = 0; i < ac->ac_b_ex.fe_len; i++) { 
if 
if (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) { 
if (!(ac->ac_flags & EXT4_MB_DELALLOC_RESERVED)) 
if (sbi->s_log_groups_per_flex) { 
if (err) 
for locality group 
if we set the same via mount option. 
for locality group\n", 
for_stack void 
if (!(ac->ac_flags & EXT4_MB_HINT_DATA)) 
if (unlikely(ac->ac_flags & EXT4_MB_HINT_GOAL_ONLY)) 
for example) */ 
if (ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC) { 
if (size < i_size_read(ac->ac_inode)) 
if (size <= 16 * 1024) { 
if (size <= 32 * 1024) { 
if (size <= 64 * 1024) { 
if (size <= 128 * 1024) { 
if (size <= 256 * 1024) { 
if (size <= 512 * 1024) { 
if (size <= 1024 * 1024) { 
if (NRL_CHECK_SIZE(size, 4 * 1024 * 1024, max, 2 * 1024)) { 
if (NRL_CHECK_SIZE(size, 8 * 1024 * 1024, max, 4 * 1024)) { 
if (NRL_CHECK_SIZE(ac->ac_o_ex.fe_len, 
if (ar->pleft && start <= ar->lleft) { 
if (ar->pright && start + size - 1 >= ar->lright) 
for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) { 
if (pa->pa_deleted) 
if (pa->pa_deleted) { 
if (pa->pa_lstart >= end || pa_end <= start) { 
if (pa_end <= ac->ac_o_ex.fe_logical) { 
if (pa->pa_lstart > ac->ac_o_ex.fe_logical) { 
for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) { 
if (pa->pa_deleted == 0) { 
if (start + size <= ac->ac_o_ex.fe_logical && 
if (ar->pright && (ar->lright == (start + size))) { 
if (ar->pleft && (ar->lleft + 1 == start)) { 
if (sbi->s_mb_stats && ac->ac_g_ex.fe_len > 1) { 
if (ac->ac_b_ex.fe_len >= ac->ac_o_ex.fe_len) 
if (ac->ac_g_ex.fe_start == ac->ac_b_ex.fe_start && 
if (ac->ac_found > sbi->s_mb_max_to_scan) 
if (ac->ac_op == EXT4_MB_HISTORY_ALLOC) 
for this 
if (pa && pa->pa_type == MB_INODE_PA) 
if (cpa == NULL) { 
if (cur_distance <= new_distance) 
for_stack int 
if (!(ac->ac_flags & EXT4_MB_HINT_DATA)) 
for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) { 
for them */ 
if (!(ext4_test_inode_flag(ac->ac_inode, EXT4_INODE_EXTENTS)) && 
if (pa->pa_deleted == 0 && pa->pa_free) { 
if (!(ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC)) 
for some reason */ 
if (lg == NULL) 
if (order > PREALLOC_TB_SIZE - 1) 
for the prealloc space that is having 
for (i = order; i < PREALLOC_TB_SIZE; i++) { 
for_each_entry_rcu(pa, &lg->lg_prealloc_list[i], 
if (pa->pa_deleted == 0 && 
if (cpa) { 
while (n) { 
for_stack 
form of preallocation discards first load group, 
for 
for_each(cur, &grp->bb_prealloc_list) { 
if (unlikely(len == 0)) 
for group %u\n", preallocated, group); 
if this was the last reference and the space is consumed 
if (!atomic_dec_and_test(&pa->pa_count) || pa->pa_free != 0) { 
if (pa->pa_deleted == 1) { 
if (pa->pa_type == MB_GROUP_PA) 
for given inode 
for_stack int 
if (pa == NULL) 
if (ac->ac_b_ex.fe_len < ac->ac_g_ex.fe_len) { 
if (offs && offs < win) 
for history */ 
for %u\n", pa, 
for locality group inodes belongs to 
for_stack int 
if (pa == NULL) 
for history */ 
for %u\n", pa, 
if (ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC) 
for_stack int 
while (bit < end) { 
if (bit >= end) 
if (free != pa->pa_free) { 
for_stack int 
for_stack int 
for group %u\n", group); 
if (bitmap_bh == NULL) { 
for %u", group); 
if (err) { 
formation for %u", group); 
if (needed == 0) 
for_each_entry_safe(pa, tmp, 
if (atomic_read(&pa->pa_count)) { 
if (pa->pa_deleted) { 
if we still need more blocks and some PAs were used, try again */ 
if (list_empty(&list)) { 
for_each_entry_safe(pa, tmp, &list, u.pa_tmp_list) { 
if (pa->pa_type == MB_GROUP_PA) 
for given inode 
if (!S_ISREG(inode->i_mode)) { 
for inode %lu\n", inode->i_ino); 
while (!list_empty(&ei->i_prealloc_list)) { 
if (atomic_read(&pa->pa_count)) { 
while we're discarding it */ 
while discarding"); 
if (pa->pa_deleted == 0) { 
if this happens too often, we can 
force wait only in case 
for_each_entry_safe(pa, tmp, &list, u.pa_tmp_list) { 
if (err) { 
formation for %u", 
if (bitmap_bh == NULL) { 
for %u", 
ifdef CONFIG_EXT4_DEBUG 
if (!ext4_mballoc_debug || 
for (i = 0; i < ngroups; i++) { 
for_each(cur, &grp->bb_prealloc_list) { 
if (grp->bb_free == 0) 
if 
for small size file. The size of the 
if (!(ac->ac_flags & EXT4_MB_HINT_DATA)) 
if (unlikely(ac->ac_flags & EXT4_MB_HINT_GOAL_ONLY)) 
if ((size == isize) && 
if (sbi->s_mb_group_prealloc <= 0) { 
for large files */ 
if (size > sbi->s_mb_stream_request) { 
for having 
for_stack int 
if (len >= EXT4_CLUSTERS_PER_GROUP(sb)) 
if (goal < le32_to_cpu(es->s_first_data_block) || 
for_stack void 
for_each_entry_rcu(pa, &lg->lg_prealloc_list[order], 
if (atomic_read(&pa->pa_count)) { 
for block allocation. So don't 
if (pa->pa_deleted) { 
if (total_entries <= 5) { 
for this list. 
for_each_entry_safe(pa, tmp, &discard_list, u.pa_tmp_list) { 
if (ext4_mb_load_buddy(sb, group, &e4b)) { 
formation for %u", 
if (order > PREALLOC_TB_SIZE - 1) 
for_each_entry_rcu(tmp_pa, &lg->lg_prealloc_list[order], 
if (tmp_pa->pa_deleted) { 
if (!added && pa->pa_free < tmp_pa->pa_free) { 
if (!added) 
if (lg_prealloc_count > 8) { 
if (pa) { 
if (pa) { 
while adding 
if ((pa->pa_type == MB_GROUP_PA) && likely(pa->pa_free)) { 
if (ac->ac_bitmap_page) 
if (ac->ac_buddy_page) 
if (ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC) 
for (i = 0; i < ngroups && needed > 0; i++) { 
for quota file */ 
if (ext4_test_inode_state(ar->inode, EXT4_STATE_DELALLOC_RESERVED)) 
ify 
ify allocation doesn't exceed the quota limits. 
while (ar->len && 
if (!ar->len) { 
if (ar->flags & EXT4_MB_USE_ROOT_BLOCKS) { 
while (ar->len && 
if (ar->len == 0) { 
if (!ac) { 
if (*errp) { 
if (!ext4_mb_use_preallocated(ac)) { 
if (*errp) 
if (ac->ac_status == AC_STATUS_FOUND && 
if (*errp) { 
if (likely(ac->ac_status == AC_STATUS_FOUND)) { 
if (*errp == -EAGAIN) { 
if (*errp) { 
if (freed) 
if (*errp) { 
if (ac) 
if (inquota && ar->len < inquota) 
if (!ar->len) { 
if non delalloc */ 
if the physical blocks 
if ((entry1->efd_tid == entry2->efd_tid) && 
for_stack int 
if (!*n) { 
while (*n) { 
if (cluster < entry->efd_start_cluster) 
if (cluster >= (entry->efd_start_cluster + entry->efd_count)) 
if (node) { 
if (can_merge(entry, new_entry) && 
if (node) { 
if (can_merge(new_entry, entry) && 
for this transaction 
if (bh) { 
if (!(flags & EXT4_FREE_BLOCKS_VALIDATED) && 
if (flags & EXT4_FREE_BLOCKS_FORGET) { 
for (i = 0; i < count; i++) { 
if (!bh) 
if (!tbh) 
forget(handle, flags & EXT4_FREE_BLOCKS_METADATA, 
if the inode is to be written in writeback mode 
if (!ext4_should_writeback_data(inode)) 
if (overflow) { 
if (count > overflow) 
if (overflow) { 
if (count > overflow) 
if (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT( 
if we are freeing blocks across a group 
if (EXT4_C2B(sbi, bit) + count > EXT4_BLOCKS_PER_GROUP(sb)) { 
if (!bitmap_bh) { 
if (!gdp) { 
if (in_range(ext4_block_bitmap(sb, gdp), block, count) || 
if (err) 
ify some metadata.  Call the journal APIs 
if (err) 
ifdef AGGRESSIVE_CHECK 
for (i = 0; i < count_clusters; i++) 
if 
if (err) 
if ((flags & EXT4_FREE_BLOCKS_METADATA) && ext4_handle_valid(handle)) { 
if (!new_entry) { 
if (test_opt(sb, DISCARD)) { 
if (err && err != -EOPNOTSUPP) 
if (sbi->s_log_groups_per_flex) { 
if (flags & EXT4_FREE_BLOCKS_RESERVE && ei->i_reserved_data_blocks) { 
if (flags & EXT4_FREE_BLOCKS_METADATA) 
if (!(flags & EXT4_FREE_BLOCKS_NO_QUOT_UPDATE)) 
if (!(flags & EXT4_FREE_BLOCKS_NO_QUOT_UPDATE)) 
if (!err) 
if (overflow && !err) { 
if (count == 0) 
if we are freeing blocks across a group 
if (bit + count > EXT4_BLOCKS_PER_GROUP(sb)) { 
if (!bitmap_bh) { 
if (!desc) { 
if (in_range(ext4_block_bitmap(sb, desc), block, count) || 
if (err) 
ify some metadata.  Call the journal APIs 
if (err) 
for (i = 0, blocks_freed = 0; i < count; i++) { 
if (!mb_test_bit(bit + i, bitmap_bh->b_data)) { 
for block %llu", 
if (err) 
if (sbi->s_log_groups_per_flex) { 
if (!err) 
for the file system 
for the group 
while 
for file system 
for free 
for free 
if (ret) { 
formation for %u", group); 
if (EXT4_MB_GRP_WAS_TRIMMED(e4b.bd_info) && 
while (start <= max) { 
if (start > max) 
if ((next - start) >= minblocks) { 
if (ret && ret != -EOPNOTSUPP) 
if (fatal_signal_pending(current)) { 
if (need_resched()) { 
if ((e4b.bd_info->bb_free - free_count) < minblocks) 
if (!ret) { 
for filesystem 
if (minlen > EXT4_CLUSTERS_PER_GROUP(sb) || 
if (end >= max_blks) 
if (end <= first_data_blk) 
if (start < first_data_blk) 
for (group = first_group; group <= last_group; group++) { 
if the grp has never been initialized */ 
if (ret) 
for the last group, note that last_cluster is 
if (group == last_group) 
if (grp->bb_free >= minlen) { 
if (cnt < 0) { 
if (!ret) 
file : ./test/kernel/fs/ext4/block_validity.c 
[ OK ] open : 4 ok... 
if (ext4_system_zone_cachep == NULL) 
if ((entry1->start_blk + entry1->count) == entry2->start_blk) 
while (*n) { 
if (start_blk < entry->start_blk) 
if (start_blk >= (entry->start_blk + entry->count)) 
if (start_blk + count > (entry->start_blk + 
if (!new_entry) { 
if (!new_entry) 
if (node) { 
if (can_merge(entry, new_entry)) { 
if (node) { 
if (can_merge(new_entry, entry)) { 
while (node) { 
if (!test_opt(sb, BLOCK_VALIDITY)) { 
if (EXT4_SB(sb)->system_blks.rb_node) 
for (i=0; i < ngroups; i++) { 
if (ret) 
if (ret) 
if (ret) 
if (test_opt(sb, DEBUG)) 
for_each_entry_safe(entry, n, 
if the passed-in block region (start_blk, 
if ((start_blk <= le32_to_cpu(sbi->s_es->s_first_data_block)) || 
while (n) { 
if (start_blk + count - 1 < entry->start_blk) 
if (start_blk >= (entry->start_blk + entry->count)) 
while (bref < p+max) { 
if (blk && 
file : ./test/kernel/fs/ext4/xattr_trusted.c 
[ OK ] open : 4 ok... 
for trusted extended attributes. 
if (!capable(CAP_SYS_ADMIN)) 
if (list && total_len <= list_size) { 
if (strcmp(name, "") == 0) 
if (strcmp(name, "") == 0) 
file : ./test/kernel/fs/ext4/symlink.c 
[ OK ] open : 4 ok... 
file : ./test/kernel/fs/ext4/hash.c 
[ OK ] open : 4 ok... 
form(__u32 buf[4], __u32 const in[]) 
while (--n); 
while (len--) { 
if (hash & 0x80000000) 
while (len--) { 
if (hash & 0x80000000) 
if (len > num*4) 
for (i = 0; i < len; i++) { 
if ((i % 4) == 3) { 
if (--num >= 0) 
while (--num >= 0) 
if (len > num*4) 
for (i = 0; i < len; i++) { 
if ((i % 4) == 3) { 
if (--num >= 0) 
while (--num >= 0) 
ify a hash.  If the seed is all zero's, then some default seed 
ifies whether or not the seed is 
for the minor hash. 
for the hash checksum functions */ 
if the seed is all zero's */ 
for (i = 0; i < 4; i++) { 
while (len > 0) { 
form(buf, in); 
while (len > 0) { 
form(buf, in); 
if (hash == (EXT4_HTREE_EOF_32BIT << 1)) 
file : ./test/kernel/fs/ext4/bitmap.c 
[ OK ] open : 4 ok... 
ify(struct super_block *sb, ext4_group_t group, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (sbi->s_desc_size >= EXT4_BG_INODE_BITMAP_CSUM_HI_END) { 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (sbi->s_desc_size >= EXT4_BG_INODE_BITMAP_CSUM_HI_END) 
ify(struct super_block *sb, ext4_group_t group, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (sbi->s_desc_size >= EXT4_BG_BLOCK_BITMAP_CSUM_HI_END) { 
if (provided == calculated) 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (sbi->s_desc_size >= EXT4_BG_BLOCK_BITMAP_CSUM_HI_END) 
file : ./test/kernel/fs/ext4/ialloc.c 
[ OK ] open : 4 ok... 
for blocks, 1 bitmap 
for the rest of the bitmap as there are no other users. 
if (start_bit >= end_bit) 
for (i = start_bit; i < ((start_bit + 7) & ~7UL); i++) 
if (i < end_bit) 
if (!ext4_group_desc_csum_verify(sb, block_group, gdp)) { 
for group %u", block_group); 
if (!EXT4_MB_GRP_BBITMAP_CORRUPT(grp)) 
if (!EXT4_MB_GRP_IBITMAP_CORRUPT(grp)) { 
if (uptodate) { 
for a given block_group, reading 
if (!desc) 
if (unlikely(!bh)) { 
if (bitmap_uptodate(bh)) 
if (bitmap_uptodate(bh)) { 
ify; 
if (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) { 
ified(bh); 
if (buffer_uptodate(bh)) { 
if not uninit if bh is uptodate, 
ify; 
for reading 
if (!buffer_uptodate(bh)) { 
ify: 
if (!buffer_verified(bh) && 
if (!EXT4_MB_GRP_IBITMAP_CORRUPT(grp)) { 
ified(bh); 
fore_ we mark the inode not in use in the inode 
if (!sb) { 
if (atomic_read(&inode->i_count) > 1) { 
if (inode->i_nlink) { 
fore locking the superblock, 
if (ino < EXT4_FIRST_INO(sb) || ino > le32_to_cpu(es->s_inodes_count)) { 
if the inode bitmap is corrupt. */ 
if (unlikely(EXT4_MB_GRP_IBITMAP_CORRUPT(grp)) || !bitmap_bh) 
if (fatal) 
if (gdp) { 
if (fatal || !cleared) { 
if (is_directory) { 
if (sbi->s_log_groups_per_flex) { 
if (is_directory) 
if (cleared) { 
if (!fatal) 
for inode %lu", ino); 
for Orlov's allocator; returns critical information 
if (flex_size > 1) { 
if (desc) { 
for directories. 
if it doesn't satisfy these 
for a group with more 
if (flex_size > 1) { 
if (S_ISDIR(mode) && 
if (qstr) { 
for (i = 0; i < ngroups; i++) { 
if (!stats.free_inodes) 
if (stats.used_dirs >= best_ndir) 
if (stats.free_inodes < avefreei) 
if (stats.free_clusters < avefreec) 
if (ret) 
if (flex_size == 1) { 
for (i = 0; i < flex_size; i++) { 
if (desc && ext4_free_inodes_count(sb, desc)) { 
if (min_inodes < 1) 
for this parent directory 
if (EXT4_I(parent)->i_last_alloc_group != ~0) { 
if (flex_size > 1) 
for (i = 0; i < ngroups; i++) { 
if (stats.used_dirs >= max_dirs) 
if (stats.free_inodes < min_inodes) 
if (stats.free_clusters < min_clusters) 
for (i = 0; i < ngroups; i++) { 
if (desc) { 
if (grp_free && grp_free >= avefreei) { 
if (avefreei) { 
for really small 
formation in the 
for future allocations. 
if (flex_size > 1) { 
if (last > ngroups) 
for  (i = parent_group; i < last; i++) { 
if (desc && ext4_free_inodes_count(sb, desc)) { 
if (!retry && EXT4_I(parent)->i_last_alloc_group != ~0) { 
if (*group > ngroups) 
if (desc && ext4_free_inodes_count(sb, desc) && 
ifferent blockgroup from its 
ifferent 
ifferent blockgroup. 
for the hash. 
for (i = 1; i < ngroups; i <<= 1) { 
if (*group >= ngroups) 
if (desc && ext4_free_inodes_count(sb, desc) && 
if that group 
for (i = 0; i < ngroups; i++) { 
if (desc && ext4_free_inodes_count(sb, desc)) 
if an inode has recently been deleted, we want 
if (unlikely(!gdp)) 
if (unlikely(!bh) || !buffer_uptodate(bh)) 
if (buffer_dirty(bh)) 
if (dtime && (dtime < now) && (now < dtime + recentcy)) 
for allocating an inode.  If the new inode is 
if that fails, then of 
forward from the parent directory's block 
if (!dir || !dir->i_nlink) 
if (!inode) 
for quota initialization worst case in standard inode creating 
if (owner) { 
if (test_opt(sb, GRPID)) { 
if (!goal) 
if (goal && goal <= le32_to_cpu(sbi->s_es->s_inodes_count)) { 
if (S_ISDIR(mode)) 
if (ret2 == -1) 
for (i = 0; i < ngroups; i++, ino = 0) { 
if (!gdp) 
fore loading bitmap. 
if (ext4_free_inodes_count(sb, gdp) == 0) { 
if (EXT4_MB_GRP_IBITMAP_CORRUPT(grp)) { 
if (EXT4_MB_GRP_IBITMAP_CORRUPT(grp) || !inode_bitmap_bh) { 
if (ino >= EXT4_INODES_PER_GROUP(sb)) 
if (group == 0 && (ino+1) < EXT4_FIRST_INO(sb)) { 
if ((EXT4_SB(sb)->s_journal == NULL) && 
if (!handle) { 
if (IS_ERR(handle)) { 
if (err) { 
if (!ret2) 
if (ino < EXT4_INODES_PER_GROUP(sb)) 
if (++group == ngroups) 
if (err) { 
if it isn't already */ 
if (err) { 
if we still need to */ 
if (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) { 
if (err) { 
if (err) { 
if (ext4_has_group_desc_csum(sb)) { 
ify the bg desc */ 
if (gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) { 
if it is greater 
if (ino > free) 
if (S_ISDIR(mode)) { 
if (sbi->s_log_groups_per_flex) { 
if (ext4_has_group_desc_csum(sb)) { 
if (err) { 
if (S_ISDIR(mode)) 
if (sbi->s_log_groups_per_flex) { 
for stat), not the fs block size */ 
if (IS_DIRSYNC(inode)) 
if (insert_inode_locked(inode) < 0) { 
for inode metadata */ 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_INLINE_DATA)) 
if (err) 
if (err) 
if (err) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) { 
for directory, file and normal symlink*/ 
if (ext4_handle_valid(handle)) { 
if (err) { 
ify that we are loading a valid orphan from disk */ 
for us */ 
if (!bitmap_bh) { 
for orphan %lu", ino); 
if (!ext4_test_bit(bit, bitmap_bh->b_data)) 
if (IS_ERR(inode)) 
if (inode->i_nlink && !ext4_can_truncate(inode)) 
if (NEXT_ORPHAN(inode) > max_ino) 
if (inode) { 
if we got a bad deleted inode */ 
ifdef EXT4FS_DEBUG 
for (i = 0; i < ngroups; i++) { 
if (!gdp) 
if (!bitmap_bh) 
for (i = 0; i < ngroups; i++) { 
if (!gdp) 
if 
for (i = 0; i < ngroups; i++) { 
if (!gdp) 
if (sb->s_flags & MS_RDONLY) { 
if (!gdp) 
if (gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)) 
if (IS_ERR(handle)) { 
if (!(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT))) 
if ((used_blks < 0) || (used_blks > sbi->s_itb_per_group)) { 
if (ret) 
if the inode table is full. But we set the ZEROED 
if (unlikely(num == 0)) 
if (ret < 0) 
if (barrier) 
file : ./test/kernel/fs/ext4/fsync.c 
[ OK ] open : 4 ok... 
for little endian machines 
if it was freshly created) since 
for 
if (!ext4_test_inode_state(inode, EXT4_STATE_NEWENTRY)) 
while (ext4_test_inode_state(inode, EXT4_STATE_NEWENTRY)) { 
if (!dentry) 
if (!next) 
if (ret) 
if (ret) 
for ext4_sync_file(). 
if (inode->i_sb->s_flags & MS_RDONLY) { 
if (EXT4_SB(inode->i_sb)->s_mount_flags & EXT4_MF_FS_ABORTED) 
if (!journal) { 
if (!ret && !hlist_empty(&inode->i_dentry)) 
if (ret) 
for proper transaction to 
force_commit will write the file data into the journal and 
if (ext4_should_journal_data(inode)) { 
force_commit(inode->i_sb); 
if (journal->j_flags & JBD2_BARRIER && 
if (needs_barrier) { 
if (!ret) 
file : ./test/kernel/fs/ext4/inline.c 
[ OK ] open : 4 ok... 
ify it 
for more details. 
if (EXT4_I(inode)->i_inline_off) 
if (!ext4_test_inode_state(inode, EXT4_STATE_XATTR)) 
for (; !IS_LAST_ENTRY(entry); entry = EXT4_XATTR_NEXT(entry)) { 
if (offs < min_offs) 
if (EXT4_I(inode)->i_inline_off) { 
if (free > EXT4_XATTR_ROUND) 
for a xattr entry, don't use the space 
if (EXT4_I(inode)->i_extra_isize == 0) 
if (error) { 
if (!max_inline_size) 
form ext4_iget, before 
if (EXT4_I(inode)->i_extra_isize == 0) 
if (error) 
if (error) 
if (!is.s.not_found) { 
if (!len) 
if (!len) 
if (pos < EXT4_MIN_INLINE_DATA_SIZE) { 
if (!len) 
if (error) 
if (error) 
if (len > EXT4_MIN_INLINE_DATA_SIZE) { 
if (error) 
if (error) { 
if (len <= EXT4_I(inode)->i_inline_size) 
if (error) 
if (error) 
if (!value) 
if (error == -ENODATA) 
if (error) 
if (error) 
if (!ext4_test_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA)) 
if (size < len) 
if (ei->i_inline_off) 
if (!ei->i_inline_off) 
if (error) 
if (error) 
if (error) 
if (error) 
if (EXT4_HAS_INCOMPAT_FEATURE(inode->i_sb, 
if (S_ISDIR(inode->i_mode) || 
if (error == -ENODATA) 
if (!EXT4_I(inode)->i_inline_off) { 
if (ret) 
if (!ext4_has_inline_data(inode)) { 
for all the other pages, just set them uptodate. 
if (!page->index) 
if (!PageUptodate(page)) { 
if (!ext4_has_inline_data(inode)) { 
if (ret) 
if (IS_ERR(handle)) { 
if (!page) { 
for us, just exit. */ 
if (!PageUptodate(page)) { 
if (ret < 0) 
if (ret) 
if (ext4_should_dioread_nolock(inode)) 
if (!ret && ext4_should_journal_data(inode)) { 
if (ret) { 
if (inode->i_nlink) 
if (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries)) 
if (page) { 
if (sem_held) 
if (handle) 
for it. 
if (pos + len > ext4_get_max_inline_size(inode)) 
if (ret) 
if (IS_ERR(handle)) { 
if (ret && ret != -ENOSPC) 
if (ret == -ENOSPC) { 
if (!page) { 
if (!ext4_has_inline_data(inode)) { 
if (!PageUptodate(page)) { 
if (ret < 0) 
if (handle) 
if (unlikely(copied < len)) { 
if (ret) { 
for us. */ 
if (ret) { 
for the inline data case. 
if (!page) 
if (!ext4_has_inline_data(inode)) { 
if (!PageUptodate(page)) { 
if (ret < 0) 
if (ret) { 
if (page) { 
for the inline data. 
if (ret) 
if (IS_ERR(handle)) { 
if (inline_size >= pos + len) { 
if (ret && ret != -ENOSPC) 
if (ret == -ENOSPC) { 
if (ret == -ENOSPC && 
if (!page) { 
if (!ext4_has_inline_data(inode)) { 
if (!PageUptodate(page)) { 
if (ret < 0) 
while still holding page lock: 
if (pos+copied > inode->i_size) { 
forces lock 
if (i_size_changed) 
ifdef INLINE_DIR_DEBUG 
while ((void *)de < dlimit) { 
if (ext4_check_dir_entry(dir, NULL, de, bh, 
if 
if no space is available, and -EIO 
if (err) 
if (err) 
ifferent from the directory change time. 
if (old_size) { 
while (de_buf < limit); 
if (new_size - old_size <= EXT4_DIR_REC_LEN(1)) 
if (ret) 
formation 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (csum_size) { 
if (err) 
ified(dir_block); 
if (!buf) { 
if (error < 0) 
if (error) 
if (error < 0) 
if (!(map.m_flags & EXT4_MAP_MAPPED)) { 
if (!data_bh) { 
if (error) { 
if (!S_ISDIR(inode->i_mode)) { 
if (error) 
if (ret) 
if (!ext4_has_inline_data(dir)) 
if (ret != -ENOSPC) 
if (!inline_size) { 
if (ret && ret != -ENOSPC) 
if (inline_size) { 
if (ret != -ENOSPC) 
for it. 
formation from an 
if (ret) 
if (!ext4_has_inline_data(inode)) { 
if (!dir_buf) { 
if (ret < 0) 
while (pos < inline_size) { 
formation about '.' and 
ifferently. 
if (pos == 0) { 
if (pos == EXT4_INLINE_DOTDOT_OFFSET) { 
if (ext4_check_dir_entry(inode, dir_file, de, 
if ((hinfo->hash < start_hash) || 
if (de->inode == 0) 
if (err) { 
if '.' and '..' really take place. 
if (ret) 
if (!ext4_has_inline_data(inode)) { 
if (!dir_buf) { 
if (ret < 0) 
if the dir is block based while 
for them are only EXT4_INLINE_DOTDOT_SIZE. 
if (file->f_version != inode->i_version) { 
for (i = 0; i < extra_size && i < offset;) { 
if (!i) { 
if (i == dotdot_offset) { 
for other entry, the real offset in 
if (ext4_rec_len_from_disk(de->rec_len, extra_size) 
while (ctx->pos < extra_size) { 
if (!dir_emit(ctx, ".", 1, inode->i_ino, DT_DIR)) 
if (ctx->pos == dotdot_offset) { 
if (ext4_check_dir_entry(inode, file, de, iloc.bh, dir_buf, 
if (le32_to_cpu(de->inode)) { 
if (*retval) 
for the new dir. 
if (ret) 
if (ret) 
formation for the ".." 
if (ext4_get_inode_loc(dir, &iloc)) 
if (!ext4_has_inline_data(dir)) { 
if (ret == 1) 
if (ret < 0) 
if (ext4_get_inline_size(dir) == EXT4_MIN_INLINE_DATA_SIZE) 
if (ret == 1) 
if (err) 
if (!ext4_has_inline_data(dir)) { 
if ((void *)de_del - ((void *)ext4_raw_inode(&iloc)->i_block) < 
if (err) 
if (err) 
if (unlikely(err)) 
if (err != -ENOENT) 
if (offset < EXT4_MIN_INLINE_DATA_SIZE) { 
if (inline_start) 
if (err) { 
if (!ext4_has_inline_data(dir)) { 
if (!le32_to_cpu(de->inode)) { 
while (offset < dir->i_size) { 
if (ext4_check_dir_entry(dir, NULL, de, 
if (le32_to_cpu(de->inode)) { 
if (!ext4_has_inline_data(inode)) { 
if (error) 
if (physical) 
if we can sparse space 'needed', 
if (error) 
if (EXT4_XATTR_LEN(entry->e_name_len) + 
if (IS_ERR(handle)) 
if (!ext4_has_inline_data(inode)) { 
if (ext4_orphan_add(handle, inode)) 
if (ext4_get_inode_loc(inode, &is.iloc)) 
if (i_size < inline_size) { 
if (inline_size > EXT4_MIN_INLINE_DATA_SIZE) { 
if (!value) 
if (ext4_xattr_ibody_get(inode, i.name_index, i.name, 
if (ext4_xattr_ibody_inline_set(handle, inode, &i, &is)) 
if (i_size < EXT4_MIN_INLINE_DATA_SIZE) { 
if (inode->i_nlink) 
if (IS_SYNC(inode)) 
if (!ext4_has_inline_data(inode)) { 
if (error) 
if (IS_ERR(handle)) { 
if (!ext4_has_inline_data(inode)) { 
file : ./test/kernel/fs/ext4/xattr_security.c 
[ OK ] open : 4 ok... 
for storing security labels as extended attributes. 
if (list && total_len <= list_size) { 
if (strcmp(name, "") == 0) 
if (strcmp(name, "") == 0) 
for (xattr = xattr_array; xattr->name != NULL; xattr++) { 
if (err < 0) 
file : ./test/kernel/fs/ext4/acl.c 
[ OK ] open : 4 ok... 
if (!value) 
if (size < sizeof(ext4_acl_header)) 
if (((ext4_acl_header *)value)->a_version != 
if (count < 0) 
if (count == 0) 
if (!acl) 
for (n = 0; n < count; n++) { 
if ((char *)value + sizeof(ext4_acl_entry_short) > end) 
if ((char *)value > end) 
if ((char *)value > end) 
if (value != end) 
if (!ext_acl) 
for (n = 0; n < acl->a_count; n++) { 
if (retval > 0) { 
if (!value) 
if (retval > 0) 
if (retval == -ENODATA || retval == -ENOSYS) 
if (!IS_ERR(acl)) 
if (acl) { 
if (error < 0) 
if (error == 0) 
if (!S_ISDIR(inode->i_mode)) 
if (acl) { 
if (IS_ERR(value)) 
if (!error) 
if (IS_ERR(handle)) 
if (error == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries)) 
if (error) 
if (default_acl) { 
if (acl) { 
file : ./test/kernel/fs/ext4/migrate.c 
[ OK ] open : 4 ok... 
ify it 
if (lb->first_pblock == 0) 
if (IS_ERR(path)) { 
for_single_extent(inode, 
if (needed && ext4_handle_has_enough_credits(handle, 
if (retval) 
if (needed) { 
if (retval) { 
if (retval) 
if (path) { 
if we can add on to the existing range (if it exists) 
if (lb->first_pblock && 
if (!bh) 
for (i = 0; i < max_entries; i++) { 
if (retval) 
if (!bh) 
for (i = 0; i < max_entries; i++) { 
if (retval) 
if (!bh) 
for (i = 0; i < max_entries; i++) { 
if (retval) 
for_blkdel(handle_t *handle, struct inode *inode) 
if (ext4_handle_has_enough_credits(handle, EXT4_RESERVE_TRANS_BLOCKS+1)) 
if (ext4_journal_extend(handle, needed) != 0) 
if (!bh) 
for (i = 0; i < max_entries; i++) { 
for_blkdel(handle, inode); 
for_blkdel(handle, inode); 
if (!bh) 
for (i = 0; i < max_entries; i++) { 
if (retval) { 
for_blkdel(handle, inode); 
if (i_data[0]) { 
for_blkdel(handle, inode); 
if (i_data[1]) { 
if (retval) 
if (i_data[2]) { 
if (retval) 
for writing the 
if (retval) { 
if (retval) 
if EXT4_STATE_EXT_MIGRATE is cleared a block allocation 
if (!ext4_test_inode_state(inode, EXT4_STATE_EXT_MIGRATE)) { 
for extent index 
for extent blocks 
if (!bh) 
if (eh->eh_depth != 0) { 
for (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ix++) { 
if (retval) 
for_blkdel(handle, inode); 
if (eh->eh_depth == 0) 
for extent meta data 
for (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ix++) { 
if (retval) 
if (!EXT4_HAS_INCOMPAT_FEATURE(inode->i_sb, 
if (S_ISLNK(inode->i_mode) && inode->i_blocks == 0) 
ifying the quota inode. 
if (IS_ERR(handle)) { 
if (IS_ERR(tmp_inode)) { 
for 
if (IS_ERR(handle)) { 
for (i = 0; i < EXT4_NDIR_BLOCKS; i++) { 
if (retval) 
if (i_data[EXT4_IND_BLOCK]) { 
if (retval) 
if (i_data[EXT4_DIND_BLOCK]) { 
if (retval) 
if (i_data[EXT4_TIND_BLOCK]) { 
if (retval) 
if (retval) 
formation with the 
if (retval) 
if we fail to swap inode data free the extent 
if (ext4_journal_extend(handle, 1) != 0) 
if (!EXT4_HAS_INCOMPAT_FEATURE(inode->i_sb, 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (IS_ERR(handle)) 
if (ret) 
if (ext4_blocks_count(es) > EXT4_MAX_BLOCK_FILE_PHYS || 
if (eh->eh_entries == 0) 
if (len > EXT4_NDIR_BLOCKS) { 
for (i=0; i < len; i++) 
file : ./test/kernel/fs/ext4/resize.c 
[ OK ] open : 4 ok... 
for resizing an ext4 filesystem while it is mounted. 
if (!capable(CAP_SYS_RESOURCE)) 
if (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) { 
if (test_and_set_bit_lock(EXT4_RESIZING, &EXT4_SB(sb)->s_resize_flags)) 
if (ext4_bg_has_super(sb, group)) 
ify_group_input(struct super_block *sb, 
if (group != sbi->s_groups_count) { 
if (test_opt(sb, DEBUG)) 
if (offset != 0) 
if (input->reserved_blocks > input->blocks_count / 5) 
if (free_blocks_count < 0) 
if (!(bh = sb_bread(sb, end - 1))) 
if (outside(input->block_bitmap, start, end)) 
if (outside(input->inode_bitmap, start, end)) 
if (outside(input->inode_table, start, end) || 
if (input->inode_bitmap == input->block_bitmap) 
if (inside(input->block_bitmap, input->inode_table, itend)) 
if (inside(input->inode_bitmap, input->inode_table, itend)) 
if (inside(input->block_bitmap, start, metaend)) 
if (inside(input->inode_bitmap, start, metaend)) 
if (inside(input->inode_table, start, metaend) || 
for groups 
if (flex_gd == NULL) 
if (flexbg_size >= UINT_MAX / sizeof(struct ext4_new_flex_group_data)) 
if (flex_gd->groups == NULL) 
if (flex_gd->bg_flags == NULL) 
for a flex group. 
if (src_group >= group_data[0].group + flex_gd->count) 
for (; src_group <= last_group; src_group++) { 
if (overhead == 0) 
for (; bb_index < flex_gd->count; bb_index++) { 
for (; ib_index < flex_gd->count; ib_index++) { 
for (; it_index < flex_gd->count; it_index++) { 
if (start_blk + itb > last_blk) 
if (start_blk + itb > next_group_start) { 
if (test_opt(sb, DEBUG)) { 
for (i = 0; i < flex_gd->count; i++) { 
if (unlikely(!bh)) 
if ((err = ext4_journal_get_write_access(handle, bh))) { 
for the 
if (ext4_handle_has_enough_credits(handle, thresh)) 
if (err < 0) 
if (err) { 
if (err) 
for ext4_setup_new_group_blocks() which set . 
for (count2 = count; count > 0; count -= count2, block += count2) { 
if (count2 > count) 
if (flex_gd->bg_flags[group] & EXT4_BG_BLOCK_UNINIT) { 
if (err) 
if (unlikely(!bh)) 
if (err) 
if (unlikely(err)) 
for the new groups. 
if necessary. 
for blocks taken by 
for blocks taken by group tables. 
if (IS_ERR(handle)) 
for (i = 0; i < flex_gd->count; i++, group++) { 
if (meta_bg == 0 && !ext4_bg_has_super(sb, group)) 
if (meta_bg == 1) { 
if (first_group != group + 1 && 
for (j = 0; j < gdblocks; j++, block++) { 
if (err) 
if (unlikely(!gdb)) { 
if (err) { 
if (unlikely(err)) { 
if (ext4_bg_has_super(sb, group)) { 
if (err) 
if (!(bg_flags[i] & EXT4_BG_INODE_ZEROED)) 
if (err) 
if (bg_flags[i] & EXT4_BG_BLOCK_UNINIT) 
if (err) 
if (IS_ERR(bh)) { 
if (overhead != 0) { 
if (err) 
if (bg_flags[i] & EXT4_BG_INODE_UNINIT) 
if (err) 
if (IS_ERR(bh)) { 
if (err) 
for (j = 0; j < GROUP_TABLE_COUNT; j++) { 
for (i = 1; i < flex_gd->count; i++) { 
if (block == (&group_data[i].block_bitmap)[j]) { 
if (err) 
if (count) { 
if (err) 
if (err2 && !err) 
fore 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (*five < *min) { 
if (*seven < *min) { 
ify_reserved_gdb(struct super_block *sb, 
while ((grp = ext4_list_backups(sb, &three, &five, &seven)) < end) { 
if (++gdbackups > EXT4_ADDR_PER_BLOCK(sb)) 
ifying the data on disk, because JBD has no rollback. 
if (test_opt(sb, DEBUG)) 
if (EXT4_SB(sb)->s_sbh->b_blocknr != 
if (!gdb_bh) 
ify_reserved_gdb(sb, group, gdb_bh); 
if (!dind) { 
if (le32_to_cpu(data[gdb_num % EXT4_ADDR_PER_BLOCK(sb)]) != gdblock) { 
if (unlikely(err)) 
if (unlikely(err)) 
if (unlikely(err)) 
if (unlikely(err)) 
if (!n_group_desc) { 
for %lu groups", 
for use (which also "frees" the backup GDT blocks 
if (unlikely(err)) { 
if (unlikely(err)) { 
if (err) 
if (!gdb_bh) 
if (!n_group_desc) { 
for %lu groups", 
if (unlikely(err)) 
for future resizing and not allocated to files. 
ify it is pointing to the primary reserved 
if (!primary) 
if (!dind) { 
ify it holds backups */ 
for (res = 0; res < reserved_gdb; res++, blk++) { 
if (!primary[res]) { 
ify_reserved_gdb(sb, group, primary[res]); 
if (++data >= end) 
for (i = 0; i < reserved_gdb; i++) { 
if ((err = ext4_journal_get_write_access(handle, primary[i]))) 
if ((err = ext4_reserve_inode_write(handle, inode, &iloc))) 
for (i = 0; i < reserved_gdb; i++) { 
if (!err) 
while (--res >= 0) 
if there 
if possible, in case the primary gets trashed 
for some reason and we need to run e2fsck from a backup superblock.  The 
for this, because these 
if (IS_ERR(handle)) { 
if (meta_bg == 0) { 
while (group < sbi->s_groups_count) { 
if (ext4_handle_valid(handle) && 
if (meta_bg == 0) 
if (unlikely(!bh)) { 
if ((err = ext4_journal_get_write_access(handle, bh))) 
if (rest) 
if (unlikely(err)) 
if (meta_bg == 0) 
if (group == last) 
if ((err2 = ext4_journal_stop(handle)) && !err) 
if we got here we have a journal problem too, so we 
if not - we will simply wait until next fsck. 
if (err) { 
for group %u (err %d), " 
for (i = 0; i < count; i++, group++) { 
for the first group in a new group block. 
if (gdb_off) { 
if (!err && reserved_gdb && ext4_bg_num_gdb(sb, group)) 
if (meta_bg != 0) { 
if (err) 
if (unlikely(!bh)) 
if (!bh_uptodate_or_lock(bh)) { 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (!bh) 
if (!bh) 
for (i = 0; i < flex_gd->count; i++, group_data++, bg_flags++) { 
for new group */ 
if (err) { 
if (ext4_has_group_desc_csum(sb)) 
if (unlikely(err)) { 
for mb_alloc based on the new group 
if (err) 
fore 
fore the group is live won't actually let us 
for (i = 0; i < flex_gd->count; i++) { 
form a smp_wmb() after updating all 
form an smp_rmb() after reading the groups 
while freeing data, as we can only allocate from a block 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, 
formation 
if (test_opt(sb, DEBUG)) 
ifying the filesystem, because we cannot abort the 
if (err) 
ifying at least the superblock and  GDT 
ify the inode and the dindirect block.  If we 
ify each of the reserved GDT dindirect blocks. 
if (IS_ERR(handle)) { 
if (err) 
if (err) 
if (err) 
if (!err) 
if (!err) { 
for (; gdb_num <= gdb_num_end; gdb_num++) { 
if (old_gdb == gdb_bh->b_blocknr) 
if (o_blocks_count == n_blocks_count) 
if (last_group > n_group) 
for (i = 0; i < flex_gd->count; i++) { 
if (ext4_has_group_desc_csum(sb)) { 
if (!test_opt(sb, INIT_INODE_TABLE)) 
if (last_group == n_group && ext4_has_group_desc_csum(sb)) 
if ((last_group == n_group) && (last != blocks_per_group - 1)) { 
ifying 
for a sparse group. 
while we are actually adding 
ifying. 
if (gdb_off == 0 && !EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (ext4_blocks_count(es) + input->blocks_count < 
if (le32_to_cpu(es->s_inodes_count) + EXT4_INODES_PER_GROUP(sb) < 
if (reserved_gdb || gdb_off == 0) { 
if (IS_ERR(inode)) { 
ify_group_input(sb, input); 
if (err) 
if (err) 
if (IS_ERR(handle)) { 
if (err) { 
if (err) 
if (err2 && !err) 
if (!err) { 
ified.  This entry 
for emergencies (because it has no dependencies on reserved blocks). 
for arbitrary resizing, assuming enough 
if (test_opt(sb, DEBUG)) 
if (n_blocks_count == 0 || n_blocks_count == o_blocks_count) 
if (n_blocks_count > (sector_t)(~0ULL) >> (sb->s_blocksize_bits - 9)) { 
if (sizeof(sector_t) < 8) 
if (n_blocks_count < o_blocks_count) { 
if (last == 0) { 
if (o_blocks_count + add < o_blocks_count) { 
if (o_blocks_count + add > n_blocks_count) 
if (o_blocks_count + add < n_blocks_count) 
if the device is actually as big as what was requested */ 
if (!bh) { 
if there 
if (inode) { 
if (inode->i_blocks != 1 << (inode->i_blkbits - 9)) 
for (i = 0; i < EXT4_N_BLOCKS; i++) { 
if (ei->i_data[i]) 
if (ei->i_data[i]) 
if (IS_ERR(handle)) 
if (err) 
if (err) { 
if (inode) { 
if (err) 
if (!err) 
ified by @n_blocks_count 
if the device is actually as big as what was requested */ 
if (!bh) { 
if (n_blocks_count < o_blocks_count) { 
if (n_blocks_count == o_blocks_count) 
if (n_group > (0xFFFFFFFFUL / EXT4_INODES_PER_GROUP(sb))) { 
if (EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_RESIZE_INODE)) { 
if (n_desc_blocks > o_desc_blocks + 
if (!resize_inode) 
if (IS_ERR(resize_inode)) { 
if ((!resize_inode && !meta_bg) || n_blocks_count == o_blocks_count) { 
if (err) 
if (resize_inode) { 
if (n_blocks_count_retry) { 
if (n_group == o_group) 
if (add > 0) { 
if (err) 
if (ext4_blocks_count(es) == n_blocks_count) 
if (err) 
if (err) 
if (flex_gd == NULL) { 
while (ext4_setup_next_flex_gd(sb, flex_gd, n_blocks_count, 
if (jiffies - last_update_time > HZ * 10) { 
iffies; 
if (ext4_alloc_group_tables(sb, flex_gd, flexbg_size) != 0) 
if (unlikely(err)) 
if (!err && n_blocks_count_retry) { 
if (flex_gd) 
if (resize_inode != NULL) 
file : ./test/kernel/fs/ext4/super.c 
[ OK ] open : 4 ok... 
for trace points definition */ 
if !defined(CONFIG_EXT2_FS) && !defined(CONFIG_EXT2_FS_MODULE) && defined(CONFIG_EXT4_USE_FOR_EXT23) 
if 
if 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
ify(struct super_block *sb, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (!ret) 
if (!ret) 
if (is_vmalloc_addr(ptr)) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT) 
if (!es->s_first_error_time) { 
if it hasn't been 
if (!es->s_error_count) 
ific data 
for example, by ext4_commit_super), will cause a kernel OOPS. 
form the VFS and file system layers. 
while (!list_empty(&txn->t_private_list)) { 
if (sb->s_flags & MS_RDONLY) 
if (!test_opt(sb, ERRORS_CONT)) { 
if (journal) 
if (test_opt(sb, ERRORS_RO)) { 
fore ->s_flags update 
if (test_opt(sb, ERRORS_PANIC)) 
forced after error\n", 
format vaf; 
if (ext4_error_ratelimit(sb)) { 
format vaf; 
if (ext4_error_ratelimit(inode->i_sb)) { 
if (block) 
format vaf; 
if (ext4_error_ratelimit(inode->i_sb)) { 
if (IS_ERR(path)) 
if (block) 
if (!sb || (EXT4_SB(sb)->s_journal && 
for unknown 
if (nbuf) { 
for truncated error codes... */ 
if the error is EROFS, and we're not already 
if (errno == -EROFS && journal_current_handle() == NULL && 
if (ext4_error_ratelimit(sb)) { 
force the filesystem into an ABORT|READONLY state, 
if ((sb->s_flags & MS_RDONLY) == 0) { 
fore ->s_flags update 
if (EXT4_SB(sb)->s_journal) 
if (test_opt(sb, ERRORS_PANIC)) 
format vaf; 
if (!___ratelimit(&(EXT4_SB(sb)->s_msg_ratelimit_state), "EXT4-fs")) 
format vaf; 
if (!___ratelimit(&(EXT4_SB(sb)->s_warning_ratelimit_state), 
format vaf; 
if (ext4_error_ratelimit(sb)) { 
if (ino) 
if (block) 
if (test_opt(sb, ERRORS_CONT)) { 
if (le32_to_cpu(es->s_rev_level) > EXT4_GOOD_OLD_REV) 
if empty */ 
if not it 
if (IS_ERR(bdev)) 
if (bdev) { 
for_each(l, &sbi->s_orphan) { 
if (sbi->s_journal) { 
if (err < 0) 
if (!(sb->s_flags & MS_RDONLY)) { 
if (!(sb->s_flags & MS_RDONLY)) 
if (sbi->s_proc) { 
for (i = 0; i < sbi->s_gdb_count; i++) 
ifdef CONFIG_QUOTA 
for (i = 0; i < MAXQUOTAS; i++) 
if 
if we've 
if (!list_empty(&sbi->s_orphan)) 
if (sbi->journal_bdev && sbi->journal_bdev != sb->s_bdev) { 
if (sbi->s_mb_cache) { 
if (sbi->s_mmp_tsk) 
for_completion(&sbi->s_kobj_unregister); 
if (!ei) 
ifdef CONFIG_QUOTA 
if 
if (!list_empty(&(EXT4_I(inode)->i_orphan))) { 
if (ext4_inode_cachep == NULL) 
fore we 
if (EXT4_I(inode)->jinode) { 
if (ino < EXT4_FIRST_INO(sb) && ino != EXT4_ROOT_INO) 
if (ino > le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count)) 
if the inode is currently unallocated!! 
if the inode had been 
for parent directory, so 
if (IS_ERR(inode)) 
if (generation && inode->i_generation != generation) { 
if (!page_has_buffers(page)) 
if (journal) 
ifdef CONFIG_QUOTA 
format_id, 
format_id); 
format_id, 
if 
ifdef CONFIG_QUOTA 
if 
ifdef CONFIG_QUOTA 
if 
if (!options || strncmp(options, "sb=", 3) != 0) 
if (*options && *options != ',') { 
if (*options == ',') 
if you think we should keep it.\n"; 
if (sb_any_quota_loaded(sb) && 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_QUOTA)) { 
if (!qname) { 
for storing quotafile name"); 
if (sbi->s_qf_names[qtype]) { 
ified", 
if (strchr(qname, '/')) { 
if (sb_any_quota_loaded(sb) && 
if 
ifdef CONFIG_QUOTA 
if 
ifdef CONFIG_EXT4_FS_POSIX_ACL 
if 
ifdef CONFIG_QUOTA 
if (token == Opt_grpjquota) 
if (token == Opt_offusrjquota) 
if (token == Opt_offgrpjquota) 
if 
for (m = ext4_mount_opts; m->token != Opt_err; m++) 
if (m->token == Opt_err) { 
if ((m->flags & MOPT_NO_EXT2) && IS_EXT2_SB(sb)) { 
if ((m->flags & MOPT_NO_EXT3) && IS_EXT3_SB(sb)) { 
if (args->from && !(m->flags & MOPT_STRING) && match_int(args, &arg)) 
if (args->from && (m->flags & MOPT_GTE0) && (arg < 0)) 
if (m->flags & MOPT_EXPLICIT) 
if (m->flags & MOPT_CLEAR_ERR) 
if (token == Opt_noquota && sb_any_quota_loaded(sb)) { 
if (m->flags & MOPT_NOSUPPORT) { 
if (token == Opt_commit) { 
if (token == Opt_max_batch_time) { 
if (token == Opt_min_batch_time) { 
if (token == Opt_inode_readahead_blks) { 
if (token == Opt_init_itable) { 
if (!args->from) 
if (token == Opt_max_dir_size_kb) { 
if (token == Opt_stripe) { 
if (token == Opt_resuid) { 
if (!uid_valid(uid)) { 
if (token == Opt_resgid) { 
if (!gid_valid(gid)) { 
if (token == Opt_journal_dev) { 
ify journal on remount"); 
if (token == Opt_journal_path) { 
if (is_remount) { 
ify journal on remount"); 
if (!journal_path) { 
if (error) { 
if (!S_ISBLK(journal_inode->i_mode)) { 
if (token == Opt_journal_ioprio) { 
if (m->flags & MOPT_DATAJ) { 
if (!sbi->s_journal) 
if (test_opt(sb, DATA_FLAGS) != m->mount_opt) { 
ifdef CONFIG_QUOTA 
if (sb_any_quota_loaded(sb) && 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if 
if (!args->from) 
if (m->flags & MOPT_CLEAR) 
if (unlikely(!(m->flags & MOPT_SET))) { 
if (arg != 0) 
if (!options) 
while ((p = strsep(&options, ",")) != NULL) { 
if (handle_mount_opt(sb, p, token, args, journal_devnum, 
ifdef CONFIG_QUOTA 
if (sbi->s_qf_names[USRQUOTA] || sbi->s_qf_names[GRPQUOTA]) { 
if (test_opt(sb, GRPQUOTA) && sbi->s_qf_names[GRPQUOTA]) 
if (test_opt(sb, GRPQUOTA) || test_opt(sb, USRQUOTA)) { 
format mixing"); 
if (!sbi->s_jquota_fmt) { 
format " 
if (sbi->s_jquota_fmt) { 
format " 
if 
if (blocksize < PAGE_CACHE_SIZE) { 
if block size != PAGE_SIZE"); 
if defined(CONFIG_QUOTA) 
if (sbi->s_jquota_fmt) { 
if (sbi->s_qf_names[USRQUOTA]) 
if (sbi->s_qf_names[GRPQUOTA]) 
if 
for (t = tokens; t->token != Opt_err; t++) 
if 
if the per-sb default is different from the global default 
if (sbi->s_sb_block != 1) 
for (m = ext4_mount_opts; m->token != Opt_err; m++) { 
if (((m->flags & (MOPT_SET|MOPT_CLEAR)) == 0) || 
if (!(m->mount_opt & (sbi->s_mount_opt ^ def_mount_opt))) 
if ((want_set && 
if (nodefs || !uid_eq(sbi->s_resuid, make_kuid(&init_user_ns, EXT4_DEF_RESUID)) || 
if (nodefs || !gid_eq(sbi->s_resgid, make_kgid(&init_user_ns, EXT4_DEF_RESGID)) || 
if (test_opt(sb, ERRORS_RO) && def_errors != EXT4_ERRORS_RO) 
if (test_opt(sb, ERRORS_CONT) && def_errors != EXT4_ERRORS_CONTINUE) 
if (test_opt(sb, ERRORS_PANIC) && def_errors != EXT4_ERRORS_PANIC) 
if (nodefs || sbi->s_commit_interval != JBD2_DEFAULT_MAX_COMMIT_AGE*HZ) 
if (nodefs || sbi->s_min_batch_time != EXT4_DEF_MIN_BATCH_TIME) 
if (nodefs || sbi->s_max_batch_time != EXT4_DEF_MAX_BATCH_TIME) 
if (sb->s_flags & MS_I_VERSION) 
if (nodefs || sbi->s_stripe) 
if (EXT4_MOUNT_DATA_FLAGS & (sbi->s_mount_opt ^ def_mount_opt)) { 
if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA) 
if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_WRITEBACK_DATA) 
if (nodefs || 
if (nodefs || (test_opt(sb, INIT_INODE_TABLE) && 
if (nodefs || sbi->s_max_dir_size_kb) 
if (le32_to_cpu(es->s_rev_level) > EXT4_MAX_SUPP_REV) { 
forcing read-only mode"); 
if (read_only) 
if (!(sbi->s_mount_state & EXT4_VALID_FS)) 
if (sbi->s_mount_state & EXT4_ERROR_FS) 
if ((__s16) le16_to_cpu(es->s_max_mnt_count) > 0 && 
if (le32_to_cpu(es->s_checkinterval) && 
if (!sbi->s_journal) 
if (!(__s16) le16_to_cpu(es->s_max_mnt_count)) 
if (sbi->s_journal) 
if (test_opt(sb, DEBUG)) 
if (!sbi->s_log_groups_per_flex) 
if (size <= sbi->s_flex_groups_allocated) 
if (!new_groups) { 
for %d flex groups", 
if (sbi->s_flex_groups) { 
if (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) { 
if (err) 
for (i = 0; i < sbi->s_groups_count; i++) { 
if ((sbi->s_es->s_feature_ro_compat & 
for checksum of struct ext4_group_desc do the rest...*/ 
ify(struct super_block *sb, __u32 block_group, 
if (ext4_has_group_desc_csum(sb) && 
if (!ext4_has_group_desc_csum(sb)) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG)) 
for (i = 0; i < sbi->s_groups_count; i++) { 
if (i == sbi->s_groups_count - 1 || flexbg_flag) 
if ((grp == sbi->s_groups_count) && 
if (block_bitmap < first_block || block_bitmap > last_block) { 
for group %u not in group " 
if (inode_bitmap < first_block || inode_bitmap > last_block) { 
for group %u not in group " 
if (inode_table < first_block || 
for group %u not in group " 
if (!ext4_group_desc_csum_verify(sb, i, gdp)) { 
for group %u failed (%u!=%u)", 
if (!(sb->s_flags & MS_RDONLY)) { 
if (!flexbg_flag) 
if (NULL != first_not_zeroed) 
for us). 
if 
for us, so we can safely abort without any further action. 
ifdef CONFIG_QUOTA 
if 
if (bdev_read_only(sb->s_bdev)) { 
if feature set would not allow a r/w mount */ 
if (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) { 
if (es->s_last_orphan && !(s_flags & MS_RDONLY)) { 
if (s_flags & MS_RDONLY) { 
ifdef CONFIG_QUOTA 
for iput() to work correctly and not trash data */ 
for (i = 0; i < MAXQUOTAS; i++) { 
if (ret < 0) 
if 
while (es->s_last_orphan) { 
if (IS_ERR(inode)) { 
if (inode->i_nlink) { 
if (test_opt(sb, DEBUG)) 
if (nr_orphans) 
if (nr_truncates) 
ifdef CONFIG_QUOTA 
for (i = 0; i < MAXQUOTAS; i++) { 
if 
format file size. 
format containers, within a sector_t, and within i_blocks 
form 
format containers as 
for vfs i_blocks. 
if (!has_huge_files || sizeof(blkcnt_t) < sizeof(u64)) { 
if (res > upper_limit) 
for a dense, block 
if (!has_huge_files || sizeof(blkcnt_t) < sizeof(u64)) { 
if (res > upper_limit) 
if (res > MAX_LFS_FILESIZE) 
if (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_META_BG) || 
if (ext4_bg_has_super(sb, bg)) 
if (sb->s_blocksize == 1024 && nr == 0 && 
ified it via mount option, then 
if (sbi->s_stripe && sbi->s_stripe <= sbi->s_blocks_per_group) 
if (stripe_width <= sbi->s_blocks_per_group) 
if (stride <= sbi->s_blocks_per_group) 
if (ret <= 1) 
if (!ret && *value > max) 
if (!sb->s_bdev->bd_part) 
ifetime_write_kbytes_show(struct ext4_attr *a, 
if (!sb->s_bdev->bd_part) 
if (ret) 
if (t && (!is_power_of_2(t) || t > 0x40000000)) 
if (ret) 
if (parse_strtoull(buf, -1ULL, &val)) 
if (!capable(CAP_SYS_ADMIN)) 
if (len && buf[len-1] == '\n') 
if (len) 
ify(_name), .mode = _mode },	\ 
ify(_name), .mode = 0444 },	\ 
ifetime_write_kbytes); 
ifetime_write_kbytes), 
if this filesystem can be mounted as requested, 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, ~EXT4_FEATURE_INCOMPAT_SUPP)) { 
if (readonly) 
for a read-write mount */ 
if kernel is built with CONFIG_LBDAF 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_HUGE_FILE)) { 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_BIGALLOC) && 
ifndef CONFIG_QUOTA 
if  /* CONFIG_QUOTA */ 
if we have errors logged 
if (es->s_error_count) 
if (es->s_first_error_time) { 
if (es->s_first_error_ino) 
if (es->s_first_error_block) 
if (es->s_last_error_time) { 
if (es->s_last_error_ino) 
if (es->s_last_error_block) 
iffies + 24*60*60*HZ);  /* Once a day */ 
for (group = elr->lr_next_group; group < ngroups; group++) { 
if (!gdp) { 
if (!(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED))) 
if (group >= ngroups) 
if (!ret) { 
if (elr->lr_timeout == 0) { 
iffies + elr->lr_timeout; 
if (!elr) 
if (!ext4_li_info) { 
for next scheduled filesystem. 
while (true) { 
if (list_empty(&eli->li_request_list)) { 
for_each_safe(pos, n, &eli->li_request_list) { 
if (time_after_eq(jiffies, elr->lr_next_sched)) { 
if (time_before(elr->lr_next_sched, next_wakeup)) 
iffies; 
if (kthread_should_stop()) { 
if (!list_empty(&eli->li_request_list)) { 
for_each_safe(pos, n, &ext4_li_info->li_request_list) { 
if (IS_ERR(ext4_lazyinit_task)) { 
for (group = 0; group < ngroups; group++) { 
if (!gdp) 
if (!(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED))) 
if (!eli) 
if (!elr) 
iffies + (prandom_u32() % 
if (sbi->s_li_request != NULL) { 
if (first_not_zeroed == ngroups || 
if (!elr) { 
if (NULL == ext4_li_info) { 
if (ret) 
if (!(ext4_li_info->li_state & EXT4_LAZYINIT_RUNNING)) { 
if (ret) 
if (ret) 
if (!ext4_li_info || !ext4_lazyinit_task) 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (test_opt(sb, JOURNAL_ASYNC_COMMIT)) { 
if (test_opt(sb, JOURNAL_CHECKSUM)) { 
ifficult in the face of 
ifferent block group can end up in the same allocation cluster. 
for 
for very large cluster sizes --- but for newer 
for non-bigalloc file systems), we will use it. 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_BIGALLOC)) 
for (i = 0; i < ngroups; i++) { 
if (b >= first_block && b <= last_block) { 
if (b >= first_block && b <= last_block) { 
if (b >= first_block && b + sbi->s_itb_per_group <= last_block) 
for (j = 0; j < sbi->s_itb_per_group; j++, b++) { 
if (i != grp) 
if (ext4_bg_has_super(sb, grp)) { 
for (j = ext4_bg_num_gdb(sb, grp); j > 0; j--) { 
if (!count) 
if (!buf) 
for a given filesystem unless the number of block groups 
fore first_data_block are overhead 
for (i = 0; i < ngroups; i++) { 
if (blks) 
if (sbi->s_journal) 
if (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) 
ford to run 
if (count >= clusters) 
if (!sbi) 
if (!sbi->s_blockgroup_lock) { 
if (sb->s_bdev->bd_part) 
for (cp = sb->s_id; (cp = strchr(cp, '/'));) 
if (!blocksize) { 
for other than 1kB 
if (blocksize != EXT4_MIN_BLOCK_SIZE) { 
if (!(bh = sb_bread(sb, logical_sb_block))) { 
if (sb->s_magic != EXT4_SUPER_MAGIC) 
if metadata_csum and gdt_csum are both set. */ 
for a known checksum algorithm */ 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (IS_ERR(sbi->s_chksum_driver)) { 
if (!ext4_superblock_csum_verify(sb, es)) { 
for all metadata */ 
fore we parse the mount options */ 
if (def_mount_opts & EXT4_DEFM_DEBUG) 
if (def_mount_opts & EXT4_DEFM_BSDGROUPS) 
if (def_mount_opts & EXT4_DEFM_UID16) 
ifdef CONFIG_EXT4_FS_POSIX_ACL 
if 
if ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_ORDERED) 
if ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_WBACK) 
if (le16_to_cpu(sbi->s_es->s_errors) == EXT4_ERRORS_PANIC) 
if (le16_to_cpu(sbi->s_es->s_errors) == EXT4_ERRORS_CONTINUE) 
if (def_mount_opts & EXT4_DEFM_BLOCK_VALIDITY) 
if (def_mount_opts & EXT4_DEFM_DISCARD) 
if ((def_mount_opts & EXT4_DEFM_NOBARRIER) == 0) 
if (!IS_EXT3_SB(sb) && !IS_EXT2_SB(sb) && 
for lazyinit, for the case there is 
if (!parse_options((char *) sbi->s_es->s_mount_opts, sb, 
if (!parse_options((char *) data, sb, &journal_devnum, 
if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA) { 
if (test_opt2(sb, EXPLICIT_DELALLOC)) { 
if (test_opt(sb, DIOREAD_NOLOCK)) { 
if (test_opt(sb, DELALLOC)) 
if (le32_to_cpu(es->s_rev_level) == EXT4_GOOD_OLD_REV && 
if (es->s_creator_os == cpu_to_le32(EXT4_OS_HURD)) { 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, 
if (IS_EXT2_SB(sb)) { 
if (IS_EXT3_SB(sb)) { 
if (!ext4_feature_set_ok(sb, (sb->s_flags & MS_RDONLY))) 
if (blocksize < EXT4_MIN_BLOCK_SIZE || 
if (sb->s_blocksize != blocksize) { 
if (!sb_set_blocksize(sb, blocksize)) { 
if (!bh) { 
if (es->s_magic != cpu_to_le16(EXT4_SUPER_MAGIC)) { 
if (le32_to_cpu(es->s_rev_level) == EXT4_GOOD_OLD_REV) { 
if ((sbi->s_inode_size < EXT4_GOOD_OLD_INODE_SIZE) || 
if (sbi->s_inode_size > EXT4_GOOD_OLD_INODE_SIZE) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_64BIT)) { 
if (EXT4_INODE_SIZE(sb) == 0 || EXT4_INODES_PER_GROUP(sb) == 0) 
if (sbi->s_inodes_per_block == 0) 
for (i = 0; i < 4; i++) 
if (EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_DIR_INDEX)) { 
if (i & EXT2_FLAGS_UNSIGNED_HASH) 
if ((i & EXT2_FLAGS_SIGNED_HASH) == 0) { 
if (!(sb->s_flags & MS_RDONLY)) 
if (!(sb->s_flags & MS_RDONLY)) 
if 
if (has_bigalloc) { 
if (sbi->s_clusters_per_group > blocksize * 8) { 
if (sbi->s_blocks_per_group != 
if (clustersize != blocksize) { 
if (sbi->s_blocks_per_group > blocksize * 8) { 
if (sbi->s_inodes_per_group > blocksize * 8) { 
if (sbi->s_blocks_per_group == clustersize << 3) 
if (err) { 
if (sizeof(sector_t) < 8) 
if (EXT4_BLOCKS_PER_GROUP(sb) == 0) 
if (blocks_count && ext4_blocks_count(es) > blocks_count) { 
for the first data block to be beyond the end 
if (le32_to_cpu(es->s_first_data_block) >= ext4_blocks_count(es)) { 
if (blocks_count > ((uint64_t)1<<32) - EXT4_DESC_PER_BLOCK(sb)) { 
if (sbi->s_group_desc == NULL) { 
if (ext4_proc_root) 
if (sbi->s_proc) 
for (i = 0; i < db_count; i++) { 
if (!sbi->s_group_desc[i]) { 
for buddy allocator 
if (!test_opt(sb, NOLOAD) && 
if (err) { 
if (!ext4_check_descriptors(sb, &first_not_zeroed)) { 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG)) 
if (!err) { 
if (!err) { 
if (!err) { 
if (!err) { 
if (err) { 
ifdef CONFIG_QUOTA 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_QUOTA)) 
if 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_MMP) && 
if (ext4_multi_mount_protect(sb, le64_to_cpu(es->s_mmp_block))) 
ified in the journal! 
if (!test_opt(sb, NOLOAD) && 
if (ext4_load_journal(sb, es, journal_devnum)) 
if (test_opt(sb, NOLOAD) && !(sb->s_flags & MS_RDONLY) && 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_64BIT) && 
if (!set_journal_csum_feature_set(sb)) { 
if required, so we can 
if the journal can 
if (jbd2_journal_check_available_features 
if (!jbd2_journal_check_available_features 
if (ext4_mballoc_ready) { 
if (!sbi->s_mb_cache) { 
if present. 
if (es->s_overhead_clusters) 
if (err) 
if (!EXT4_SB(sb)->rsv_conversion_wq) { 
if (IS_ERR(root)) { 
if (!S_ISDIR(root->i_mode) || !root->i_blocks || !root->i_size) { 
if (!sb->s_root) { 
if (ext4_setup_super(sb, es, sb->s_flags & MS_RDONLY)) 
if present */ 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (sbi->s_want_extra_isize < 
if (sbi->s_want_extra_isize < 
if enough inode space is available */ 
if (err) { 
for " 
if (err) { 
if (err) 
if (err) 
ifdef CONFIG_QUOTA 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_QUOTA) && 
if (err) 
if  /* CONFIG_QUOTA */ 
if (needs_recovery) { 
if (EXT4_SB(sb)->s_journal) { 
if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA) 
if (test_opt(sb, DISCARD)) { 
if (!blk_queue_discard(q)) 
if (es->s_error_count) 
if (!silent) 
ifdef CONFIG_QUOTA 
if 
if (EXT4_SB(sb)->rsv_conversion_wq) 
if (sbi->s_journal) { 
if (sbi->s_flex_groups) 
if (sbi->s_mmp_tsk) 
for (i = 0; i < db_count; i++) 
if (sbi->s_chksum_driver) 
if (sbi->s_proc) { 
ifdef CONFIG_QUOTA 
for (i = 0; i < MAXQUOTAS; i++) 
if 
fore we've 
if (test_opt(sb, BARRIER)) 
if (test_opt(sb, DATA_ERR_ABORT)) 
for the existence of a valid inode on disk.  Bad 
if (IS_ERR(journal_inode)) { 
if (!journal_inode->i_nlink) { 
if (!S_ISREG(journal_inode->i_mode)) { 
if (!journal) { 
if (bdev == NULL) 
if (blocksize < hblock) { 
for journal device"); 
if (!(bh = __bread(bdev, sb_block, blocksize))) { 
if ((le16_to_cpu(es->s_magic) != EXT4_SUPER_MAGIC) || 
if (memcmp(EXT4_SB(sb)->s_es->s_journal_uuid, es->s_uuid, 16)) { 
if (!journal) { 
if (!buffer_uptodate(journal->j_sb_buffer)) { 
if (be32_to_cpu(journal->j_superblock->s_nr_users) != 1) { 
if (journal_devnum && 
forming recovery after a 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER)) { 
if (really_read_only) { 
if (journal_inum && journal_dev) { 
if (journal_inum) { 
if (!(journal = ext4_get_dev_journal(sb, journal_dev))) 
if (!(journal->j_flags & JBD2_BARRIER)) 
if (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER)) 
if (!err) { 
if (save) 
if (save) 
if (err) { 
if (!really_read_only && journal_devnum && 
if (!sbh || block_device_ejected(sb)) 
if (buffer_write_io_error(sbh)) { 
for the best. 
for people who are east of GMT and who make their clock 
force a full file system check. 
if (!(sb->s_flags & MS_RDONLY)) 
if (sb->s_bdev->bd_part) 
if (sync) { 
if (error) 
if (error) { 
while writing " 
if we are mounting (or 
if (!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL)) { 
if (jbd2_journal_flush(journal) < 0) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER) && 
ifetime, move that error to the 
for any error status which may have been recorded in the 
if (j_errno) { 
force_commit(struct super_block *sb) 
if (sb->s_flags & MS_RDONLY) 
force_commit(journal); 
if 
for us. 
if (wait && sbi->s_journal->j_flags & JBD2_BARRIER && 
if (jbd2_journal_start_commit(sbi->s_journal, &target)) { 
if (needs_barrier) { 
if (!ret) 
if (wait && test_opt(sb, BARRIER)) 
fore a (read-only) snapshot is created.  This 
ifications. 
if (sb->s_flags & MS_RDONLY) 
if we failed to flush 
if (error < 0) 
if (sb->s_flags & MS_RDONLY) 
fore the fs is unlocked. */ 
for ext4_remount's benefit 
ifdef CONFIG_QUOTA 
if 
ifdef CONFIG_QUOTA 
if 
ifdef CONFIG_QUOTA 
for (i = 0; i < MAXQUOTAS; i++) 
if (!old_opts.s_qf_names[i]) { 
for (j = 0; j < i; j++) 
if 
if (!parse_options(data, sb, NULL, &journal_ioprio, 1)) { 
if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA) { 
if (test_opt(sb, DIOREAD_NOLOCK)) { 
if (sbi->s_mount_flags & EXT4_MF_FS_ABORTED) 
forced by user"); 
if (sbi->s_journal) { 
if ((*flags & MS_RDONLY) != (sb->s_flags & MS_RDONLY)) { 
if (*flags & MS_RDONLY) { 
if (err < 0) 
if (err < 0) 
if we are remounting a valid rw partition 
if (!(es->s_state & cpu_to_le16(EXT4_VALID_FS)) && 
if (sbi->s_journal) 
if (!ext4_feature_set_ok(sb, 0)) { 
for (g = 0; g < sbi->s_groups_count; g++) { 
if (!ext4_group_desc_csum_verify(sb, g, gdp)) { 
for group %u failed (%u!=%u)", 
for now. 
if (es->s_last_orphan) { 
if (sbi->s_journal) 
if (!ext4_setup_super(sb, es, 0)) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, 
if (ext4_multi_mount_protect(sb, 
if ((sb->s_flags & MS_RDONLY) || !test_opt(sb, INIT_INODE_TABLE)) 
if (sbi->s_journal == NULL && !(old_sb_flags & MS_RDONLY)) 
ifdef CONFIG_QUOTA 
for (i = 0; i < MAXQUOTAS; i++) 
if (enable_quota) { 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (err) 
if 
ifdef CONFIG_QUOTA 
for (i = 0; i < MAXQUOTAS; i++) { 
if 
if (!test_opt(sb, MINIX_DF)) 
if (buf->f_bfree < (ext4_r_blocks_count(es) + resv_blocks)) 
for writing quotas on sync - we need to start transaction 
ifdef CONFIG_QUOTA 
if (IS_ERR(handle)) 
if (!ret) 
if (IS_ERR(handle)) 
if (!ret) 
if (IS_ERR(handle)) { 
if (!ret) 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_QUOTA) || 
if (IS_ERR(handle)) 
if (!ret) 
format_id, 
if (!test_opt(sb, QUOTA)) 
if (path->dentry->d_sb != sb) 
if (EXT4_SB(sb)->s_qf_names[type]) { 
if (path->dentry->d_parent != sb->s_root) 
if (EXT4_SB(sb)->s_journal && 
if (err) 
format_id, path); 
if (!qf_inums[type]) 
if (IS_ERR(qf_inode)) { 
for quota files to avoid recursion */ 
format_id, flags); 
for all quota types. */ 
for (type = 0; type < MAXQUOTAS; type++) { 
if (err) { 
format_id) 
format_id, DQUOT_LIMITS_ENABLED); 
if (test_opt(sb, DELALLOC)) 
if (!inode) 
ification times of quota files when userspace can 
if (IS_ERR(handle)) 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_QUOTA)) 
ford 
if (off > i_size) 
if (off+len > i_size) 
while (toread > 0) { 
if (err) 
if (!bh)	/* A hole? */ 
if (EXT4_SB(sb)->s_journal && !handle) { 
if (sb->s_blocksize - offset < len) { 
if (!bh) 
if (err) { 
if (err) 
if (inode->i_size < off + len) { 
if 
if !defined(CONFIG_EXT2_FS) && !defined(CONFIG_EXT2_FS_MODULE) && defined(CONFIG_EXT4_USE_FOR_EXT23) 
if (err) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, ~EXT2_FEATURE_INCOMPAT_SUPP)) 
if (sb->s_flags & MS_RDONLY) 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, ~EXT2_FEATURE_RO_COMPAT_SUPP)) 
if 
if (err) 
if (EXT4_HAS_INCOMPAT_FEATURE(sb, ~EXT3_FEATURE_INCOMPAT_SUPP)) 
if (!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL)) 
if (sb->s_flags & MS_RDONLY) 
if (EXT4_HAS_RO_COMPAT_FEATURE(sb, ~EXT3_FEATURE_RO_COMPAT_SUPP)) 
if 
if (!ef) 
if (ret) { 
for_completion(&ext4_feat->f_kobj_unregister); 
for flags consistency */ 
for (i = 0; i < EXT4_WQ_HASH_SZ; i++) { 
if (err) 
if (err) 
if (err) 
if (!ext4_kset) { 
if (err) 
if (err) 
if (err) 
if (err) 
if (ext4_proc_root) 
file : ./test/kernel/fs/ext4/mmp.c 
[ OK ] open : 4 ok... 
ify(struct super_block *sb, struct mmp_struct *mmp) 
if (!EXT4_HAS_RO_COMPAT_FEATURE(sb, 
if (unlikely(!buffer_uptodate(bh))) 
if (*bh) 
if (!*bh) 
if (!*bh) 
if (*bh) { 
if (!buffer_uptodate(*bh)) { 
if (unlikely(!*bh)) { 
while reading MMP block %llu", 
if (le32_to_cpu(mmp->mmp_magic) != EXT4_MMP_MAGIC || 
formation as possible to help the admin. 
iff; 
if 
while (!kthread_should_stop()) { 
iffies; 
if (retval) { 
if (!(le32_to_cpu(es->s_feature_incompat) & 
if (sb->s_flags & MS_RDONLY) { 
iff = jiffies - last_update_time; 
iff); 
if the MMP block is as we left it. 
iff = jiffies - last_update_time; 
if (retval) { 
if (mmp->mmp_seq != mmp_check->mmp_seq || 
while updating MMP info. " 
for the MMP block to be written. 
iff / HZ, 
while (new_seq > EXT4_MMP_SEQ_MAX); 
if (mmp_block < le32_to_cpu(es->s_first_data_block) || 
if (retval) 
if (mmp_check_interval < EXT4_MMP_MIN_CHECK_INTERVAL) 
if (le16_to_cpu(mmp->mmp_check_interval) > mmp_check_interval) 
if (seq == EXT4_MMP_SEQ_CLEAN) 
if (seq == EXT4_MMP_SEQ_FSCK) { 
if more than 20 secs. */ 
if (schedule_timeout_interruptible(HZ * wait_time) != 0) { 
if (retval) 
if (seq != le32_to_cpu(mmp->mmp_seq)) { 
if (retval) 
for MMP interval and check mmp_seq. 
if (schedule_timeout_interruptible(HZ * wait_time) != 0) { 
if (retval) 
if (seq != le32_to_cpu(mmp->mmp_seq)) { 
if (!mmpd_data) { 
for mmpd_data"); 
if (IS_ERR(EXT4_SB(sb)->s_mmp_tsk)) { 
for %s.", 
file : ./test/kernel/fs/ext4/move_extent.c 
[ OK ] open : 4 ok... 
ify it 
for more details. 
for designated logical block number. 
for output) 
if (IS_ERR(path)) 
if (path[ext_depth(inode)].p_ext == NULL) 
for getting initialize status 
if (ext4_ext_is_unwritten(src)) 
for the next extent and set it to "extent" 
for the next extent 
if 
if (EXT_LAST_EXTENT(path[ppos].p_hdr) > path[ppos].p_ext) { 
while (--ppos >= 0) { 
if (path[ppos+1].p_bh) 
if (!path[ppos+1].p_bh) 
while (++cur_ppos < leaf_ppos) { 
if (path[cur_ppos+1].p_bh) 
if (!path[cur_ppos+1].p_bh) 
if (le16_to_cpu(eh->eh_entries) == 0) 
if (first < second) { 
if (start_ext->ee_len && new_ext->ee_len && end_ext->ee_len) { 
if (start_ext->ee_len && new_ext->ee_len && 
if (!start_ext->ee_len && new_ext->ee_len && 
if new_ext was 
if (new_ext->ee_block) 
if (new_flag) { 
if (err) 
if (ext4_ext_insert_extent(handle, orig_inode, 
if (end_flag) { 
if (err) 
if (ext4_ext_insert_extent(handle, orig_inode, 
if (orig_path) { 
if (range_to_move && o_end < EXT_LAST_EXTENT(eh)) { 
if (start_ext->ee_len) 
if (new_ext->ee_len) { 
if (end_ext->ee_len) 
if (depth) { 
if (ret) 
if (range_to_move > 0 && 
if (ret < 0) 
fore, this function creates structures to save extents of the leaf 
if (le32_to_cpu(oext->ee_block) < le32_to_cpu(new_ext.ee_block) && 
if (oext > EXT_FIRST_EXTENT(orig_path[depth].p_hdr)) { 
if these are contiguous and same extent type. 
if (ext4_can_extents_be_merged(orig_inode, prev_ext, 
if (le32_to_cpu(oext->ee_block) + oext_alen - 1 < new_ext_end) { 
if (le32_to_cpu(oext->ee_block) <= new_ext_end && 
for extent swapping. 
iff, orig_diff; 
if (orig_off < le32_to_cpu(tmp_oext->ee_block) || 
if (orig_off < le32_to_cpu(tmp_dext->ee_block) || 
iff = donor_off - le32_to_cpu(tmp_dext->ee_block); 
iff); 
if (max_count < ext4_ext_get_actual_len(tmp_dext)) 
iff = orig_off - le32_to_cpu(tmp_oext->ee_block); 
if donor extent is larger than orig */ 
iff) 
iff); 
if all extents in range has expected type, and zero otherwise. 
while (from < last) { 
if (*err) 
if (unwritten != ext4_ext_is_unwritten(ext)) 
if (path) { 
formation of original and donor inodes into 
formation of original inode to point at the 
formation of donor inode to point at the saved 
if (*err) 
if (*err) 
for the block "orig_off" */ 
if (*err) 
for the head */ 
if (*err) 
if (unlikely(!dext)) 
if (*err) 
for the donor extents */ 
while (1) { 
if (unlikely(!dext)) { 
for donor must be found"); 
if (donor_off != le32_to_cpu(tmp_dext.ee_block)) { 
if (*err) 
if (*err) 
if (replaced_count >= count) 
if (orig_path) 
if (*err) 
if (donor_path) 
if (*err) 
if (*err) 
if (orig_path) { 
if (donor_path) { 
for inode's by inode order 
if (inode1 < inode2) { 
if (!page[0]) 
if (!page[1]) { 
if 
if (inode1 > inode2) { 
if (PageUptodate(page)) 
if (!page_has_buffers(page)) 
for (bh = head, block_start = 0; bh != head || !block_start; 
if (block_end <= from || block_start >= to) { 
if (buffer_uptodate(bh)) 
if (!buffer_mapped(bh)) { 
if (err) { 
if (!buffer_mapped(bh)) { 
if (!nr) 
for (i = 0; i < nr; i++) { 
if (!bh_uptodate_or_lock(bh)) { 
if (err) 
if (!partial) 
ifferent metadata blocks. 
if (IS_ERR(handle)) { 
if (segment_eq(get_fs(), KERNEL_DS)) 
if ((orig_blk_offset + block_len_in_page - 1) == 
if (tmp_data_size == 0) 
if (unlikely(*err < 0)) 
while we 
if (unwritten) { 
if (*err) 
if (*err) 
if (!unwritten) { 
if ((page_has_private(pagep[0]) && 
if (*err) 
if ((page_has_private(pagep[0]) && !try_to_release_page(pagep[0], 0)) || 
if (*err) { 
form all necessary steps similar write_begin()/write_end() 
if (!*err) 
if (unlikely(*err < 0)) 
force transaction commit may help to free it. */ 
if (replaced_count != block_len_in_page) { 
for orig 
if (donor_inode->i_mode & (S_ISUID|S_ISGID)) { 
if (IS_IMMUTABLE(donor_inode) || IS_APPEND(donor_inode)) 
if (IS_SWAPFILE(orig_inode) || IS_SWAPFILE(donor_inode)) { 
if (!(ext4_test_inode_flag(orig_inode, EXT4_INODE_EXTENTS))) { 
if (!(ext4_test_inode_flag(donor_inode, EXT4_INODE_EXTENTS))) { 
if ((!orig_inode->i_size) || (!donor_inode->i_size)) { 
if (orig_start != donor_start) { 
if ((orig_start >= EXT_MAX_BLOCKS) || 
if (orig_inode->i_size > donor_inode->i_size) { 
ificial restriction */ 
ificial restriction */ 
if (orig_start >= orig_blocks) { 
if (orig_start + *len > orig_blocks) { 
if (!*len) { 
ified range of a file 
for orig 
if succeed, otherwise returns error value. 
ified as arguments. 
ified with the ext_cur (initial value is holecheck_path) re-cursive, 
ified as arguments. 
for the 
for the command to calculate the file offset 
if (orig_inode->i_sb != donor_inode->i_sb) { 
ifferent inodes */ 
if (!S_ISREG(orig_inode->i_mode) || !S_ISREG(donor_inode->i_mode)) { 
for inodes with full 
if (ext4_should_journal_data(orig_inode) || 
for all existing dio workers */ 
if (ret) 
if (file_end < block_end) 
if (ret) 
if (ret) 
if block_start was 
if (le32_to_cpu(ext_cur->ee_block) + 
if (last_extent < 0) { 
if (last_extent < 0) { 
if (le32_to_cpu(ext_cur->ee_block) > block_start) 
ified range. */ 
ified range of file " 
while (!last_extent && le32_to_cpu(ext_cur->ee_block) <= block_end) { 
if (seq_start + seq_blocks - 1 > block_end) 
if (last_extent < 0) { 
if extents are contiguous. 
if (ext4_can_extents_be_merged(orig_inode, 
if (data_offset_in_page + seq_blocks > blocks_per_page) { 
while (orig_page_offset <= seq_end_page) { 
if (ret < 0) 
if (*moved_len > len) { 
if (rest_blocks > blocks_per_page) 
if (ret < 0) 
if (holecheck_path) 
if (ret) 
if (orig_path) 
if (ret) 
if (*moved_len) { 
if (orig_path) { 
if (holecheck_path) { 
file : ./test/kernel/fs/ext4/page-io.c 
[ OK ] open : 4 ok... 
for ext4 
if (io_end_cachep == NULL) 
ific 
ifetime, due to LKML politics... 
for_each_segment_all(bvec, bio, i) { 
if (!page) 
if (error) { 
if (bh_offset(bh) < bio_start || 
if (buffer_async_write(bh)) 
if (error) 
while ((bh = bh->b_this_page) != head); 
if (!under_io) 
if (atomic_dec_and_test(&EXT4_I(io_end->inode)->i_ioend_count)) 
for (bio = io_end->bio; bio; bio = next_bio) { 
if (atomic_dec_and_test(&EXT4_I(inode)->i_unwritten)) 
for all DIO to finish (thus exclusion from 
fore all IOs overlapping that range are 
if (ret < 0) { 
ifdef	EXT4FS_DEBUG 
fore, *after; 
if (list_empty(head)) 
for_each_entry(io, head, list) { 
fore = cur->prev; 
if 
if (list_empty(&ei->i_rsv_conversion_list)) 
while (!list_empty(&unwritten)) { 
if (unlikely(!ret && err)) 
if (io) { 
if (atomic_dec_and_test(&io_end->count)) { 
if (atomic_dec_and_test(&io_end->count)) { 
for page writeback */ 
if (test_bit(BIO_UPTODATE, &bio->bi_flags)) 
if (error) { 
if (io_end->flag & EXT4_IO_END_UNWRITTEN) { 
if (bio) { 
if (!bio) 
if (io->io_bio && bh->b_blocknr != io->io_next_block) { 
if (io->io_bio == NULL) { 
if (ret) 
if (ret != bh->b_size) 
if (keep_towrite) 
if (len < PAGE_CACHE_SIZE) 
fore submitting so that 
if (block_start >= len) { 
if (!buffer_dirty(bh) || buffer_delay(bh) || 
if (!buffer_mapped(bh)) 
if (io->io_bio) 
if (buffer_new(bh)) { 
while ((bh = bh->b_this_page) != head); 
if (!buffer_async_write(bh)) 
if (ret) { 
for_writepage(wbc, page); 
while ((bh = bh->b_this_page) != head); 
if (ret) { 
while (bh != head); 
if (!nr_submitted) 
file : ./test/kernel/fs/ext4/xattr.c 
[ OK ] open : 4 ok... 
for symlinks and special files added per 
if an inode uses an additional block. All 
ifferent header; the entries 
format: 
ific order. 
if they are exclusive to an inode, so 
ifdef EXT4_XATTR_DEBUG 
while (0) 
while (0) 
if 
ifdef CONFIG_EXT4_FS_POSIX_ACL 
if 
ifdef CONFIG_EXT4_FS_SECURITY 
if 
ifdef CONFIG_EXT4_FS_POSIX_ACL 
if 
if 
ify(struct inode *inode, 
if (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb, 
if (name_index > 0 && name_index < ARRAY_SIZE(ext4_xattr_handler_map)) 
while (!IS_LAST_ENTRY(entry)) { 
if ((void *)next >= end) 
if (buffer_verified(bh)) 
if (BHDR(bh)->h_magic != cpu_to_le32(EXT4_XATTR_MAGIC) || 
if (!ext4_xattr_block_csum_verify(inode, bh->b_blocknr, BHDR(bh))) 
if (!error) 
if (entry->e_value_block != 0 || value_size > size || 
if (name == NULL) 
for (; !IS_LAST_ENTRY(entry); entry = EXT4_XATTR_NEXT(entry)) { 
if (!cmp) 
if (!cmp) 
if (cmp <= 0 && (sorted || cmp == 0)) 
if (!cmp && ext4_xattr_check_entry(entry, size)) 
if (!EXT4_I(inode)->i_file_acl) 
if (!bh) 
if (ext4_xattr_check_block(inode, bh)) { 
if (error == -EIO) 
if (error) 
if (buffer) { 
if (size > buffer_size) 
if (!ext4_test_inode_state(inode, EXT4_STATE_XATTR)) 
if (error) 
if (error) 
if (error) 
if (buffer) { 
if (size > buffer_size) 
if (strlen(name) > 255) 
if (error == -ENODATA) 
for (; !IS_LAST_ENTRY(entry); entry = EXT4_XATTR_NEXT(entry)) { 
if (handler) { 
if (buffer) { 
if (!EXT4_I(inode)->i_file_acl) 
if (!bh) 
if (ext4_xattr_check_block(inode, bh)) { 
if (!ext4_test_inode_state(inode, EXT4_STATE_XATTR)) 
if (error) 
if (error) 
if (ret < 0) 
if (buffer) { 
if (ret < 0) 
if (EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_EXT_ATTR)) 
if (ext4_journal_get_write_access(handle, EXT4_SB(sb)->s_sbh) == 0) { 
if (error) 
if (BHDR(bh)->h_refcount == cpu_to_le32(1)) { 
if (ce) 
if (ce) 
ifferent inodes can race and so we have to protect 
fore we are done dirtying the buffer. In 
for that case 
if (ext4_handle_valid(handle)) 
if (!ext4_handle_valid(handle)) 
if (IS_SYNC(inode)) 
for EAs. This also returns the total number of 
for (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) { 
if (offs < *min_offs) 
if (total) 
for (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) { 
if (offs < min_offs) 
if (!s->not_found) { 
if (i->value) { 
if (i->value && s->not_found) { 
if (!s->here->e_value_block && s->here->e_value_size) { 
if (i->value && size == EXT4_XATTR_SIZE(i->value_len)) { 
if (i->value == EXT4_ZERO_XATTR_VALUE) { 
while (!IS_LAST_ENTRY(last)) { 
if (!last->e_value_block && 
if (!i->value) { 
if (i->value) { 
if (i->value_len) { 
if (i->value == EXT4_ZERO_XATTR_VALUE) { 
if (EXT4_I(inode)->i_file_acl) { 
if (!bs->bh) 
if (ext4_xattr_check_block(inode, bs->bh)) { 
if (error && error != -ENODATA) 
if (i->value && i->value_len > sb->s_blocksize) 
if (s->base) { 
if (error) 
if (header(s->base)->h_refcount == cpu_to_le32(1)) { 
ifying in-place"); 
if (!error) { 
if (error == -EIO) 
if (!error) 
if (error) 
if (ce) { 
if (s->base == NULL) 
if (s->base == NULL) 
if (error == -EIO) 
if (error) 
if (!IS_LAST_ENTRY(s->first)) 
if (!IS_LAST_ENTRY(s->first)) { 
if (new_bh) { 
if (new_bh == bs->bh) 
if (error) 
if (error) 
if (error) 
if (bs->bh && s->base == bs->bh->b_data) { 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) 
if (error) 
if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) 
if (unlikely(!new_bh)) { 
if (error) { 
if (error) 
if (bs->bh && bs->bh != new_bh) 
if (ce) 
if (!(bs->bh && s->base == bs->bh->b_data)) 
if (EXT4_I(inode)->i_extra_isize == 0) 
if (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) { 
if (error) 
if (error && error != -ENODATA) 
if (EXT4_I(inode)->i_extra_isize == 0) 
if (error) { 
if (error) 
if (error) 
if (error) 
if (!IS_LAST_ENTRY(s->first)) { 
if (EXT4_I(inode)->i_extra_isize == 0) 
if (error) 
if (!IS_LAST_ENTRY(s->first)) { 
for this inode.  Value 
ify that an extended attribute must exist and must not exist 
if (!name) 
if (strlen(name) > 255) 
if (error) 
if (ext4_test_inode_state(inode, EXT4_STATE_NEW)) { 
if (error) 
if (is.s.not_found) 
if (error) 
if (is.s.not_found && bs.s.not_found) { 
if (flags & XATTR_REPLACE) 
if (!value) 
if (flags & XATTR_CREATE) 
if (!value) { 
if (!bs.s.not_found) 
if (!error && !bs.s.not_found) { 
if (error == -ENOSPC) { 
if (error) 
if (error) 
if (!is.s.not_found) { 
if (!error) { 
if (!value) 
if (IS_SYNC(inode)) 
if (no_expand == 0) 
ification is a filesystem transaction by itself. 
if (IS_ERR(handle)) { 
if (error == -ENOSPC && 
if (error == 0) 
ift the EA entries in the inode to create space for the increased 
ift_entries(struct ext4_xattr_entry *entry, 
for (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) { 
ift; 
ift the entries by n bytes */ 
if (EXT4_I(inode)->i_extra_isize >= new_extra_isize) { 
if enough free space is available in the inode to shift the 
if (free >= new_extra_isize) { 
ift_entries(entry,	EXT4_I(inode)->i_extra_isize 
if 
if (EXT4_I(inode)->i_file_acl) { 
if (!bh) 
if (ext4_xattr_check_block(inode, bh)) { 
if (free < new_extra_isize) { 
while (new_extra_isize > 0) { 
ift_bytes; /* No. of bytes to shift EAs by? */ 
if (!is || !bs) { 
for (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) { 
if (total_size <= free && total_size < min_total_size) { 
if (entry == NULL) { 
if (!tried_min_extra_isize && 
if (!buffer || !b_entry_name) { 
if (error) 
if (error) 
if (error) 
if (entry_size + EXT4_XATTR_SIZE(size) >= new_extra_isize) 
ift_bytes = entry_size + size; 
ift_entries(entry, EXT4_I(inode)->i_extra_isize - 
ift_bytes, 
ift_bytes; 
if (error) 
if (error) 
if (is) 
fore an inode is freed. We have exclusive 
if (!EXT4_I(inode)->i_file_acl) 
if (!bh) { 
if (BHDR(bh)->h_magic != cpu_to_le32(EXT4_XATTR_MAGIC) || 
if (!ce) { 
if (error) { 
if (error == -EBUSY) { 
for equality. 
while (!IS_LAST_ENTRY(entry1)) { 
if (entry1->e_hash != entry2->e_hash || 
if (entry1->e_value_block != 0 || entry2->e_value_block != 0) 
if (memcmp((char *)header1 + le16_to_cpu(entry1->e_value_offs), 
if (!IS_LAST_ENTRY(entry2)) 
if such a block was 
if (!header->h_hash) 
for cached blocks [%x]", (int)hash); 
while (ce) { 
if (IS_ERR(ce)) { 
if (!bh) { 
if (le32_to_cpu(BHDR(bh)->h_refcount) >= 
if (ext4_xattr_cmp(header, BHDR(bh)) == 0) { 
for (n = 0; n < entry->e_name_len; n++) { 
if (entry->e_value_block == 0 && entry->e_value_size != 0) { 
for (n = (le32_to_cpu(entry->e_value_size) + 
while (!IS_LAST_ENTRY(here)) { 
if an entry's hash value == 0 */ 
if (cache) 
file : ./test/kernel/fs/afs/cell.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (namelen > AFS_MAXCELLNAME) { 
if (!cell) { 
if the ip address is invalid, try dns query */ 
if (ret < 0) { 
for user-space reply */ 
if (next) 
if (sscanf(_vllist, "%u.%u.%u.%u", &a, &b, &c, &d) != 4) 
if (a > 255 || b > 255 || c > 255 || d > 255) 
while (cell->vl_naddrs < AFS_CELL_MAX_ADDRS && (_vllist = next)); 
while (*cp++); 
if (IS_ERR(key)) { 
format. 
for_each_entry(cell, &afs_cells, link) { 
if (IS_ERR(cell)) { 
for this cell */ 
if (ret < 0) 
ifdef CONFIG_AFS_FSCACHE 
for caching (this never returns an error) */ 
if 
if (retref && !IS_ERR(cell)) 
if (retref) { 
formation 
if (!rootcell) { 
if (!cp) 
for the root cell */ 
if (IS_ERR(new_root)) { 
if (name) { 
for it in the cell record list */ 
if (strncmp(cell->name, name, namesz) == 0) { 
if (dns_cell) 
if (!cell) { 
for other reasons. 
if 0 
if (cell && !list_empty(&cell->link)) 
if  /*  0  */ 
if (!cell) 
if (likely(!atomic_dec_and_test(&cell->usage))) { 
for everyone to stop using the cell */ 
for cell %s", cell->name); 
while (atomic_read(&cell->usage) > 0) { 
ifdef CONFIG_AFS_FSCACHE 
if 
fore calling this 
while (!list_empty(&afs_cells)) { 
if (!list_empty(&afs_cells)) { 
if (cell) { 
file : ./test/kernel/fs/afs/vlocation.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
for (count = cell->vl_naddrs; count > 0; count--) { 
if (ret == -ENOMEM || ret == -ENONET) 
for (count = cell->vl_naddrs; count > 0; count--) { 
if (ret == -ENOMEM || ret == -ENONET) 
if (vl->upd_busy_cnt <= 3) { 
if (ret < 0 && vl->upd_rej_cnt > 0) { 
if (vl) { 
if we found it in the cache 
if (vl->vldb.vidmask & AFS_VOL_VTM_RW) { 
if (vl->vldb.vidmask & AFS_VOL_VTM_RO) { 
if (vl->vldb.vidmask & AFS_VOL_VTM_BAK) { 
if (strcmp(vldb->name, vl->vldb.name) != 0) 
ifdef CONFIG_AFS_FSCACHE 
if 
if we have an in-cache copy (will set vl->valid if there is) */ 
if 
if (ret < 0) { 
for updates 
for_updates(struct afs_vlocation *vl) 
fore updating... */ 
if (!list_empty(&afs_vlocation_updates)) { 
if (vl->update_at <= xvl->update_at) 
if not able to find on the VL server 
if (namesz >= sizeof(vl->vldb.name)) { 
if we have an in-memory copy first */ 
for_each_entry(vl, &cell->vl_list, link) { 
if (memcmp(vl->vldb.name, name, namesz) == 0) 
if (!vl) { 
if (ret < 0) 
ifdef CONFIG_AFS_FSCACHE 
if 
for regular updates */ 
if (!list_empty(&vl->grave)) { 
if it was an abandoned record that we might try filling in */ 
while (vl->state != AFS_VL_VALID) { 
if (state == AFS_VL_NEW || state == AFS_VL_NO_VOLUME) { 
for creation or update by someone else to 
if (ret < 0) 
if (!vl) 
if (likely(!atomic_dec_and_test(&vl->usage))) { 
if (atomic_read(&vl->usage) == 0) { 
if (!list_empty(&vl->update)) { 
ifdef CONFIG_AFS_FSCACHE 
if 
while (!list_empty(&afs_vlocation_graveyard)) { 
if (expiry > now) { 
if (atomic_read(&vl->usage) > 0) { 
while (!list_empty(&corpses)) { 
for rmmod 
for (;;) { 
if (atomic_read(&vl->usage) > 0) 
if (timeout > 0) { 
form the update */ 
if (!list_empty(&afs_vlocation_updates)) { 
if (vl->update_at <= xvl->update_at) 
if (timeout < 0) 
file : ./test/kernel/fs/afs/file.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (IS_ERR(key)) { 
if (ret < 0) { 
ifdef CONFIG_AFS_FSCACHE 
if the read completes with an error, we just unlock the page and let 
if (!error) 
if 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) 
ifdef CONFIG_AFS_FSCACHE 
if 
if (ret < 0) { 
ifdef CONFIG_AFS_FSCACHE 
if 
ifdef CONFIG_AFS_FSCACHE 
if 
if (file) { 
if (IS_ERR(key)) { 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) { 
ifdef CONFIG_AFS_FSCACHE 
if 
if offset is 0 (indicating 
if the entire page is being invalidated */ 
ifdef CONFIG_AFS_FSCACHE 
if 
if (wb && !PageWriteback(page)) { 
if (!page_private(page)) 
if it's not busy 
if page is being written to the cache and the caller hasn't 
ifdef CONFIG_AFS_FSCACHE 
if 
if (wb) { 
file : ./test/kernel/fs/afs/dir.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if any char of the name (inc 
if 0 
if (qty == 0) 
if (page->index == 0 && qty != ntohs(dbuf->blocks[0].pagehdr.npages)) { 
if 
if (latter >= PAGE_SIZE) 
for (tmp = 0; tmp < qty; tmp++) { 
if (!IS_ERR(page)) { 
if (!PageChecked(page)) 
if (PageError(page)) 
if (test_bit(AFS_VNODE_DELETED, &AFS_FS_I(inode)->flags)) 
for (offset = AFS_DIRENT_PER_BLOCK - block->pagehdr.nentries; 
if (!(block->pagehdr.bitmap[offset / 8] & 
if (offset >= curr) 
for (tmp = nlen; tmp > 15; tmp -= sizeof(union afs_dirent)) { 
if (!(block->pagehdr.bitmap[next / 8] & 
if starts before the current position */ 
if (!dir_emit(ctx, dire->u.name, nlen, 
if (test_bit(AFS_VNODE_DELETED, &AFS_FS_I(dir)->flags)) { 
while (ctx->pos < dir->i_size) { 
if (IS_ERR(page)) { 
if (ret != 1) { 
while (ctx->pos < dir->i_size && blkoff < limit); 
for a name 
ifier through dtype 
if (cookie->name.len != nlen || 
if found 
if (ret < 0) { 
if (!cookie.found) { 
if the autocell 
if (ret != -ENOENT || 
if (IS_ERR(inode)) { 
if (dentry->d_name.len >= AFSNAMEMAX) { 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) { 
if (IS_ERR(key)) { 
if (ret < 0) { 
if (ret < 0) { 
if (!IS_ERR(inode)) { 
if (ret == -ENOENT) { 
if (IS_ERR(inode)) { 
if (flags & LOOKUP_RCU) 
if (dentry->d_inode) 
if (IS_ERR(key)) 
if (test_bit(AFS_VNODE_MODIFIED, &dir->flags)) 
if (test_bit(AFS_VNODE_DELETED, &dir->flags)) { 
if (dentry->d_fsdata == dir_version) 
ified"); 
for this vnode */ 
if (!dentry->d_inode) 
if (is_bad_inode(dentry->d_inode)) { 
if the vnode ID has changed, then the dirent points to a 
if (fid.vnode != vnode->fid.vnode) { 
if the vnode ID uniqifier has changed, then the file has 
if (fid.unique != vnode->fid.unique) { 
if (dentry->d_inode) 
if it exists, now points to a different vnode */ 
if we have submounts */ 
if (dentry->d_flags & DCACHE_NFSFS_RENAMED) 
if (dentry->d_inode && 
if (IS_ERR(key)) { 
if (ret < 0) 
if (IS_ERR(inode)) { 
for the new vnode */ 
if (d_unhashed(dentry)) { 
if (IS_ERR(key)) { 
if (ret < 0) 
if (dentry->d_inode) { 
if (dentry->d_name.len >= AFSNAMEMAX) 
if (IS_ERR(key)) { 
if (dentry->d_inode) { 
if (ret < 0) 
if (ret < 0) 
if (dentry->d_inode) { 
if 
fore it returns to us, and if it was deleted, 
if we didn't have a callback promise outstanding, 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) 
if (test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) 
if (IS_ERR(key)) { 
if (ret < 0) 
if (IS_ERR(inode)) { 
for the new vnode */ 
if (d_unhashed(dentry)) { 
if (IS_ERR(key)) { 
if (ret < 0) 
if (strlen(content) >= AFSPATHMAX) 
if (IS_ERR(key)) { 
if (ret < 0) 
if (IS_ERR(inode)) { 
for the new vnode */ 
if (d_unhashed(dentry)) { 
if (IS_ERR(key)) { 
if (ret < 0) 
file : ./test/kernel/fs/afs/server.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
while (*pp) { 
if (server->addr.s_addr < xserver->addr.s_addr) 
if (server->addr.s_addr > xserver->addr.s_addr) 
if (server) { 
for a cell 
if we already have the server */ 
for_each_entry(server, &cell->servers, link) { 
if (!candidate) { 
for_each_entry(server, &cell->servers, link) { 
if (afs_install_server(server) < 0) 
if (!list_empty(&server->grave)) { 
while (p) { 
if (addr.s_addr < server->addr.s_addr) { 
if (addr.s_addr > server->addr.s_addr) { 
if (!server) 
if (likely(!atomic_dec_and_test(&server->usage))) { 
if (atomic_read(&server->usage) == 0) { 
while (!list_empty(&afs_server_graveyard)) { 
if (expiry > now) { 
if (atomic_read(&server->usage) > 0) { 
while (!list_empty(&corpses)) { 
for rmmod 
file : ./test/kernel/fs/afs/inode.c 
[ OK ] open : 4 ok... 
if not, write to the Free Software 
ifdef CONFIG_AFS_FSCACHE 
if 
if (vnode->status.type == AFS_FTYPE_SYMLINK) { 
if (test_bit(AFS_VNODE_MOUNTPOINT, &vnode->flags)) { 
for inode created by autocell operations 
for autocell 
if (!inode) { 
if (!inode) { 
if (!(inode->i_state & I_NEW)) { 
if (!status) { 
if (ret < 0) 
if (!cb) { 
fore mapping the status, as map-status reads the 
ifdef CONFIG_AFS_FSCACHE 
if 
if (ret < 0) 
ifdef CONFIG_AFS_FSCACHE 
if 
if (S_ISREG(vnode->vfs_inode.i_mode)) 
if (vnode->cb_promised && 
if (vnode->cb_expires < get_seconds() + 10) { 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) 
if the promise has expired, we need to check the server again to get 
ifferent and we may no longer have 
if (!vnode->cb_promised || 
if (ret < 0) 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) { 
if the vnode's data version number changed then its contents are 
if (test_and_clear_bit(AFS_VNODE_ZAP_DATA, &vnode->flags)) 
if (test_bit(AFS_VNODE_PSEUDODIR, &AFS_FS_I(inode)->flags)) 
if (vnode->server) { 
ifdef CONFIG_AFS_FSCACHE 
if 
if (permits) 
if (!(attr->ia_valid & (ATTR_SIZE | ATTR_MODE | ATTR_UID | ATTR_GID | 
if (S_ISREG(vnode->vfs_inode.i_mode)) { 
if (attr->ia_valid & ATTR_FILE) { 
if (IS_ERR(key)) { 
if (!(attr->ia_valid & ATTR_FILE)) 
file : ./test/kernel/fs/afs/netdevices.c 
[ OK ] open : 4 ok... 
if_arp.h> 
if (dev) { 
for_each_netdev(&init_net, dev) { 
if (!idev) 
ifa(idev) { 
ifa->ifa_mask; 
if (n >= maxbufs) 
ifa(idev); 
file : ./test/kernel/fs/afs/cache.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
for the index entry 
if (klen > bufmax) 
for the index entry 
if (klen > bufmax) 
if (dlen > bufmax) 
if (dlen != buflen) 
if what's on disk is more valid than what's in memory, then use the 
if (!vlocation->valid || vlocation->vldb.rtime == cvldb->rtime) { 
if the cached info differs */ 
if the volume IDs for this name differ */ 
for the volume index entry 
if (klen > bufmax) 
for the index entry 
if (klen > bufmax) 
if (dlen > bufmax) 
if (dlen != buflen) { 
if (memcmp(buffer, 
if (memcmp(buffer + sizeof(vnode->fid.unique), 
for any object that may have data 
for (;;) { 
if (!nr_pages) 
for (loop = 0; loop < nr_pages; loop++) 
file : ./test/kernel/fs/afs/security.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (IS_ERR(key)) { 
for (loop = permits->count - 1; loop >= 0; loop--) 
ified inode itself if it's a 
if (S_ISDIR(vnode->vfs_inode.i_mode)) { 
if (IS_ERR(auth_inode)) 
if (permits) 
for a vnode to its or its parent directory's cache 
if (IS_ERR(auth_vnode)) { 
for the 
if (memcmp(&auth_vnode->fid, &vnode->status.parent, 
for the status */ 
if (key == vnode->volume->cell->anonymous_key) 
if (xpermits) { 
if it is then we just amend the list 
for (loop = count; loop > 0; loop--) { 
if (!permits) 
if (xpermits) 
if (xpermits) 
if the directory or parent directory is 
if (IS_ERR(auth_vnode)) { 
if we've got one yet */ 
if (permits) { 
for (loop = permits->count; loop > 0; loop--) { 
if (!valid) { 
if (ret < 0) { 
if (mask & MAY_NOT_BLOCK) 
if (IS_ERR(key)) { 
if the promise has expired, we need to check the server again */ 
if (ret < 0) 
if we've got one yet */ 
if (ret < 0) 
if (S_ISDIR(inode->i_mode)) { 
if (!(access & AFS_ACE_LOOKUP)) 
if (mask & MAY_READ) { 
if (mask & MAY_WRITE) { 
if (!(access & AFS_ACE_LOOKUP)) 
if (mask & (MAY_EXEC | MAY_READ)) { 
if (mask & MAY_WRITE) { 
file : ./test/kernel/fs/afs/rxrpc.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
for_call_to_complete(struct afs_call *); 
for_call_to_complete(struct afs_call *); 
for_call_to_complete, 
for_call_to_complete, 
ifications 
if (!afs_async_calls) { 
if (ret < 0) { 
if (ret < 0) { 
if (!skb) { 
if (atomic_dec_return(&afs_outstanding_skbs) == -1) 
if (!skb) { 
if (atomic_dec_return(&afs_outstanding_skbs) == -1) 
if (atomic_dec_return(&afs_outstanding_calls) == -1) 
if (call->rxcall) { 
if (call->type->destructor) 
if (!call) 
if (request_size) { 
if (!call->request) 
if (reply_size) { 
if (!call->buffer) 
if (count > ARRAY_SIZE(pages)) 
if (first + loop >= last) 
fore* sending the last 
if (first + loop >= last) 
if (ret < 0) 
while (++loop < count); 
for (loop = 0; loop < count; loop++) 
if (ret < 0) 
while (first <= last); 
if (IS_ERR(rxcall)) { 
fore* sending the last packet as RxRPC 
if (!call->send_pages) 
if (ret < 0) 
if (call->send_pages) { 
if (ret < 0) 
while ((skb = skb_dequeue(&call->rx_queue))) 
if (!call) { 
for our callback service */ 
while ((call->state == AFS_CALL_AWAIT_REPLY || 
if (last && 
if (call->state != AFS_CALL_AWAIT_REPLY) 
if the call is done with (we might have 
if (call->state >= AFS_CALL_COMPLETE) { 
while ((skb = skb_dequeue(&call->rx_queue))) 
if (call->incoming) 
for a call to complete 
for_call_to_complete(struct afs_call *call) 
for (;;) { 
if (!skb_queue_empty(&call->rx_queue)) { 
if (call->state >= AFS_CALL_COMPLETE) 
if (signal_pending(current)) 
if (call->state < AFS_CALL_COMPLETE) { 
while ((skb = skb_dequeue(&call->rx_queue))) 
for_call_to_complete(struct afs_call *call) 
form processing on an asynchronous call 
if (!skb_queue_empty(&call->rx_queue)) 
if (call->state >= AFS_CALL_COMPLETE && call->wait_mode) { 
if (skb_copy_bits(skb, 0, call->buffer + call->reply_size, len) < 0) 
while ((skb = skb_dequeue(&afs_incoming_calls))) { 
ification */ 
if (!call) { 
if (!call) { 
if (!IS_ERR(rxcall)) { 
if (call) 
forms the first four bytes of the request data */ 
if (skb_copy_bits(skb, 0, oibuf + call->offset, len) < 0) 
if (!pskb_pull(skb, len)) 
if (call->offset < 4) { 
if successful) */ 
for the remainer of this message off to the 
if (n >= 0) { 
if (n == -ENOMEM) { 
if (skb_copy_bits(skb, 0, buf + call->offset, len) < 0 || 
if (call->offset < count) { 
file : ./test/kernel/fs/afs/volume.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
forces access of that type or nothing 
if not available 
if (IS_ERR(vlocation)) { 
if (params->force && !(vlocation->vldb.vidmask & (1 << params->type))) 
for (loop = 0; loop < vlocation->vldb.nservers; loop++) 
if (params->force) { 
if (srvtmask & AFS_VOL_VTM_RO) { 
if (srvtmask & AFS_VOL_VTM_RW) { 
if (vlocation->vols[params->type]) { 
if (!volume) 
force	= params->force; 
if (ret) 
for (loop = 0; loop < 8; loop++) { 
if (IS_ERR(server)) { 
ifdef CONFIG_AFS_FSCACHE 
if 
for (loop = volume->nservers - 1; loop >= 0; loop--) 
if (!volume) 
if (likely(!atomic_dec_and_test(&volume->usage))) { 
ifdef CONFIG_AFS_FSCACHE 
if 
for (loop = volume->nservers - 1; loop >= 0; loop--) 
if we can */ 
if (volume->nservers == 0) { 
for the first live server and use 
for (loop = 0; loop < volume->nservers; loop++) { 
if (ret == 0) 
if (ret == 0 || 
if (ret == 0 || 
if (ret == 0 || 
if okay or to issue error 
if = jiffies; 
if = jiffies; 
if it 
for (loop = 0; loop < volume->nservers; loop++) 
if (volume->nservers > 0) 
for volume information 
if (!server->fs_state) { 
if = jiffies; 
file : ./test/kernel/fs/afs/cmservice.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if 0 
if  /*  0  */ 
if supported, F if not 
if (call->unmarshall == 6) { 
if the cache manager is still alive 
fore* attempting to spam the AFS server 
if (call->count > AFSCBMAX) 
if (!call->buffer) 
if (!call->request) 
for (loop = call->count; loop > 0; loop--, cb++) { 
if (tmp != call->count && tmp != 0) 
if (tmp == 0) 
for (loop = call->count; loop > 0; loop--, cb++) { 
if (skb->len != 0) 
if the final ACK isn't received. 
if (!last) 
if (!server) 
if (skb->len > 0) 
if (!last) 
if (!server) 
if (!last) 
if (!server) 
if the cache manager is still alive 
if (skb->len > 0) 
if (!last) 
if the fileserver has been rebooted 
if (memcmp(r, &afs_uuid, sizeof(afs_uuid)) == 0) 
if (skb->len > 0) 
if (!last) 
if (!call->buffer) 
if (!call->request) 
for (loop = 0; loop < 6; loop++) 
if (skb->len != 0) 
if (!last) 
ifs; 
ifs; 
ifs; 
ifaddr[32]; 
ifs = 0; 
if (ifs) { 
if (nifs < 0) { 
ifs = NULL; 
ifs = htonl(nifs); 
for (loop = 0; loop < 6; loop++) 
if (ifs) { 
for (loop = 0; loop < nifs; loop++) { 
ifs[loop].netmask.s_addr; 
ifs); 
if (skb->len > 0) 
if (!last) 
file : ./test/kernel/fs/afs/main.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (ret < 0) 
if (ret < 0) 
if (!afs_wq) 
if (ret < 0) 
ifdef CONFIG_AFS_FSCACHE 
if (ret < 0) 
if 
if (ret < 0) 
if (ret < 0) 
if (ret < 0) 
if (ret < 0) 
if (ret < 0) 
ifdef CONFIG_AFS_FSCACHE 
if 
ifdef CONFIG_AFS_FSCACHE 
if 
file : ./test/kernel/fs/afs/proc.c 
[ OK ] open : 4 ok... 
for AFS 
ify it under the terms of the GNU General Public License 
if (!proc_afs) 
if (!proc_create("cells", 0644, proc_afs, &afs_proc_cells_fops) || 
if (ret < 0) 
ification */ 
if (v == &afs_proc_cells) { 
if (size <= 1 || size >= PAGE_SIZE) 
if (!kbuf) 
if (copy_from_user(kbuf, buf, size) != 0) 
if (name) 
if (!name) 
while(*name == ' '); 
if (!args) 
while(*args == ' '); 
form */ 
if (strcmp(kbuf, "add") == 0) { 
if (IS_ERR(cell)) { 
if (size <= 1 || size >= PAGE_SIZE) 
if (!kbuf) 
if (copy_from_user(kbuf, buf, size) != 0) 
if (s) 
form */ 
if (ret >= 0) 
if (!dir) 
if (!proc_create_data("servers", 0, dir, 
if (!cell) 
if (ret < 0) 
ification */ 
if (v == &cell->vl_list) { 
if (!cell) 
if (ret<0) 
ification */ 
for the header line */ 
if (pos >= cell->vl_naddrs) 
if (pos >= cell->vl_naddrs) 
if (v == (struct in_addr *) 1) { 
if (!cell) 
if (ret < 0) 
ification */ 
if (v == &cell->servers) { 
file : ./test/kernel/fs/afs/callback.c 
[ OK ] open : 4 ok... 
if not, write to the Free Software 
if 0 
if  /*  0  */ 
while (!RB_EMPTY_ROOT(&server->cb_promises)) { 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) 
if no-one else has dealt with it yet */ 
if (test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) { 
if (afs_vnode_fetch_status(vnode, NULL, NULL) < 0) 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) 
if the vnode's data version number changed then its contents 
if (test_and_clear_bit(AFS_VNODE_ZAP_DATA, &vnode->flags)) 
if (test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) { 
if (vnode->cb_promised) { 
if (vnode->cb_promised) { 
if (list_empty(&vnode->granted_locks) && 
while (p) { 
if (fid->vid < vnode->fid.vid) 
if (fid->vid > vnode->fid.vid) 
if (fid->vnode < vnode->fid.vnode) 
if (fid->vnode > vnode->fid.vnode) 
if (fid->unique < vnode->fid.unique) 
if (fid->unique > vnode->fid.unique) 
if (!igrab(AFS_VNODE_TO_I(vnode))) 
for (; count > 0; callbacks++, count--) { 
for breaking 
if (!vnode->cb_promised) { 
if (vnode->cb_promised) { 
for a vnode on the file server when the 
if (!vnode->cb_promised) { 
if (vnode->cb_promised && afs_breakring_space(server) == 0) { 
for (;;) { 
if (!vnode->cb_promised || 
for the server to break this vnode's 
if (vnode->cb_promised) 
forget that we 
if 0 
for (;;) { 
if (atomic_read(&vnode->usage) > 0) 
if (timeout > 0) { 
form the update */ 
if (!list_empty(&server->cb_promises)) { 
if (vnode->update_at <= xvnode->update_at) 
if (timeout < 0) 
if 
file : ./test/kernel/fs/afs/vnode.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if 0 
if (!node) 
if (node->rb_left) 
if (rb_parent(node) != parent) { 
if (node->rb_right) 
if (dump_tree_aux(server->cb_promises.rb_node, NULL, 0, '-')) 
if 
if (old_server) { 
while (*p) { 
if (vnode->fid.vid < xvnode->fid.vid) 
if (vnode->fid.vid > xvnode->fid.vid) 
if (vnode->fid.vnode < xvnode->fid.vnode) 
if (vnode->fid.vnode > xvnode->fid.vnode) 
if (vnode->fid.unique < xvnode->fid.unique) 
if (vnode->fid.unique > xvnode->fid.unique) 
if (vnode->cb_promised) { 
if (vnode->cb_promised) { 
if (vnode->server != server) 
while (*p) { 
if (vnode->cb_expires_at < xvnode->cb_expires_at) 
if (server) { 
if (vnode->cb_promised) { 
if (ret == -ENOENT) { 
if: 
if (!test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags) && 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) { 
if (auth_vnode) 
if (!test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags) && 
if (vnode->update_cnt > 0) { 
for the status to be updated */ 
if (!test_bit(AFS_VNODE_CB_BROKEN, &vnode->flags)) 
if (test_bit(AFS_VNODE_DELETED, &vnode->flags)) 
if it got updated and invalidated all 
fore we saw it */ 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (auth_vnode) 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(dvnode, server, ret)); 
if (ret == 0) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (new_dvnode != orig_dvnode) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(orig_dvnode, server, ret)); 
if (ret == 0) { 
if (new_dvnode != orig_dvnode) 
if (new_dvnode != orig_dvnode) 
if (new_dvnode != orig_dvnode) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) { 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) 
if (IS_ERR(server)) 
while (!afs_volume_release_fileserver(vnode, server, ret)); 
if (ret == 0) 
file : ./test/kernel/fs/afs/super.c 
[ OK ] open : 4 ok... 
if not, write to the Free Software 
if (!afs_inode_cachep) { 
if (ret < 0) { 
if (atomic_read(&afs_count_active_inodes) != 0) { 
fore we 
while ((p = strsep(&options, ","))) { 
if (IS_ERR(cell)) 
if (!name) { 
if ((name[0] != '%' && name[0] != '#') || !name[1]) { 
for */ 
force = false; 
force = true; 
if there is one */ 
if (params->volname) { 
if (suffix) { 
force = true; 
force = true; 
if (cellname || !params->cell) { 
if (IS_ERR(cell)) { 
force ? " FORCE" : ""); 
if it's the one we're looking for 
if (IS_ERR(inode)) 
if (params->autocell) 
if (!sb->s_root) 
if (current->nsproxy->net_ns != &init_net) 
if (options) { 
if (ret < 0) 
if (ret < 0) 
if (IS_ERR(key)) { 
if (IS_ERR(vol)) { 
if (!as) { 
if (IS_ERR(sb)) { 
if (!sb->s_root) { 
if (ret < 0) { 
if (!vnode) 
formation about an AFS volume 
if (IS_ERR(key)) 
if (ret < 0) { 
if (vs.max_quota == 0) 
file : ./test/kernel/fs/afs/misc.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
file : ./test/kernel/fs/afs/vlclient.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (!last) 
if (call->reply_size != call->reply_max) 
for (loop = 0; loop < 64; loop++) 
for (loop = 0; loop < 8; loop++) 
for (loop = 0; loop < 8; loop++) { 
if (tmp & AFS_VLSF_RWVOL) 
if (tmp & AFS_VLSF_ROVOL) 
if (tmp & AFS_VLSF_BACKVOL) 
if (tmp & AFS_VLF_RWEXISTS) 
if (tmp & AFS_VLF_ROEXISTS) 
if (tmp & AFS_VLF_BACKEXISTS) 
if (!entry->vidmask) 
if (!call) 
if (padsz > 0) 
if (!call) 
file : ./test/kernel/fs/afs/flock.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if it isn't already running 
if (!afs_lock_manager) { 
if (!afs_lock_manager) { 
if (!afs_lock_manager) 
if it's running 
if (afs_lock_manager) 
if the callback is broken on this vnode, then the lock may now be available 
if the 
if (fl->fl_type == F_RDLCK) { 
for_each_entry_safe(p, _p, &vnode->pending_locks, 
if (p->fl_type == F_RDLCK) { 
for a lock, including: 
if (test_bit(AFS_VNODE_UNLOCKING, &vnode->flags)) { 
if it fails, we just 
if (ret < 0) 
if we've got a lock, then it must be time to extend that lock as AFS 
if (!list_empty(&vnode->granted_locks)) { 
if (test_and_set_bit(AFS_VNODE_LOCKING, &vnode->flags)) 
if we don't have a granted lock, then we must've been called back by 
for */ 
if (test_and_set_bit(AFS_VNODE_LOCKING, &vnode->flags)) 
if (type == AFS_LOCK_READ) 
if (list_entry(vnode->pending_locks.next, 
if (ret == AFS_LOCK_GRANTED) 
if (!list_empty(&vnode->pending_locks)) 
for the unlocking of a vnode on the server to the 
if (!test_and_clear_bit(AFS_VNODE_READLOCKED, &vnode->flags) && 
if (test_and_set_bit(AFS_VNODE_UNLOCKING, &vnode->flags)) 
if (fl->fl_start != 0 || fl->fl_end != OFFSET_MAX) 
if (ret < 0) 
if (ret < 0) 
if (vnode->status.lock_count != 0 && !(fl->fl_flags & FL_SLEEP)) { 
if we've already got a readlock on the server then we can instantly 
if (type == AFS_LOCK_READ && 
if there's no-one else with a lock on this vnode, then we need to 
for a lock */ 
for a local lock to become available */ 
if (!(fl->fl_flags & FL_SLEEP)) { 
for the lock manager thread to get the 
if (fl->fl_u.afs.state <= AFS_LOCK_GRANTED) { 
if (ret < 0) 
if (fl->fl_u.afs.state <= AFS_LOCK_GRANTED) { 
if (ret < 0) { 
if (list_empty(&vnode->granted_locks) && 
if (vnode->pending_locks.prev != &fl->fl_u.afs.link) { 
if (type == AFS_LOCK_READ) 
if (ret < 0) 
if (list_empty(&vnode->granted_locks)) 
if (fl->fl_start != 0 || fl->fl_end != OFFSET_MAX) 
if (ret < 0) { 
if all granted locks are gone */ 
if indeed we hold one 
if (fl->fl_type == F_UNLCK) { 
if (ret < 0) 
if (lock_count) { 
if (__mandatory_lock(&vnode->vfs_inode) && fl->fl_type != F_UNLCK) 
if (IS_GETLK(cmd)) 
if (fl->fl_type == F_UNLCK) 
if (!(fl->fl_flags & FL_FLOCK)) 
if (fl->fl_type == F_UNLCK) 
file : ./test/kernel/fs/afs/write.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (!list_empty(&vnode->writebacks)) { 
if (front->state == AFS_WBACK_SYNCING) { 
if (--wb->usage == 0) 
if (wb) 
for writing 
if (pos + PAGE_CACHE_SIZE > i_size) 
if (ret < 0) { 
form part of a write to a page 
if (!candidate) 
if (!page) { 
if (!PageUptodate(page) && len != PAGE_CACHE_SIZE) { 
if (ret < 0) { 
if this page is already pending a writeback under a suitable key 
if (wb) { 
if (index > 0) { 
for_each_entry(wb, &vnode->writebacks, link) { 
if (index == wb->first && from < wb->offset_first) 
if (index == wb->last && to > wb->to_last) 
if it's dirty we 
fore we can use the new context */ 
if (wb->state == AFS_WBACK_PENDING) 
if (PageDirty(page)) { 
if (ret < 0) { 
if (maybe_i_size > i_size) { 
if (maybe_i_size > i_size) 
if (PageDirty(page)) 
if (count > PAGEVEC_SIZE) 
for (loop = 0; loop < count; loop++) { 
if (error) 
while (first < last); 
if (!clear_page_dirty_for_io(primary_page)) 
if (test_set_page_writeback(primary_page)) 
if (start >= wb->last) 
if (n > ARRAY_SIZE(pages)) 
if (n == 0) 
if (pages[0]->index != start) { 
while (n > 0); 
for (loop = 0; loop < n; loop++) { 
if (page->index > wb->last) 
if (!trylock_page(page)) 
if (!PageDirty(page) || 
if (!clear_page_dirty_for_io(page)) 
if (test_set_page_writeback(page)) 
if (loop < n) { 
for (; loop < n; loop++) 
while (start <= wb->last && count < 65536); 
if (ret < 0) { 
for us 
if (ret < 0) { 
if (!n) 
if (page->index > end) { 
if (page->mapping != mapping) { 
if (wbc->sync_mode != WB_SYNC_NONE) 
if (PageWriteback(page) || !PageDirty(page)) { 
if (ret < 0) { 
while (index < end && wbc->nr_to_write > 0); 
if (wbc->range_cyclic) { 
if (start > 0 && wbc->nr_to_write > 0 && ret == 0) 
if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX) { 
if (wbc->nr_to_write > 0) 
if (count > PAGEVEC_SIZE) 
for (loop = 0; loop < count; loop++) { 
if (page_private(page) == (unsigned long) wb) { 
if (wb->usage == 0) { 
if (free_wb) { 
while (first <= last); 
if (IS_SWAPFILE(&vnode->vfs_inode)) { 
if (!count) 
if (IS_ERR_VALUE(result)) { 
for this process, and check for write errors. 
for this process. 
if (ret) 
if (!wb) { 
for_each_entry(xwb, &vnode->writebacks, link) { 
if (ret < 0) { 
for the preceding writes to actually complete */ 
ification that a previously read-only page is about to become writable 
for the page to be written to the cache before we allow it to 
ifdef CONFIG_AFS_FSCACHE 
if 
file : ./test/kernel/fs/afs/fsclient.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if ctime-type changes seen */ 
while (0) 
if (size != status->size) { 
if (vnode) { 
if (changed && !test_bit(AFS_VNODE_UNSET, &vnode->flags)) { 
if (store_version) 
if (expected_version != data_version) { 
if (vnode && !test_bit(AFS_VNODE_UNSET, &vnode->flags)) { 
if (store_version) { 
if (attr->ia_valid & ATTR_MTIME) { 
if (attr->ia_valid & ATTR_UID) { 
if (attr->ia_valid & ATTR_GID) { 
if (attr->ia_valid & ATTR_MODE) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (call->reply2) 
formation for a file 
if (!call) 
if (call->operation_ID != FSFETCHDATA64) { 
if (call->count > 0) 
if (call->count > PAGE_SIZE) 
if (call->count > 0) { 
if (call->reply2) 
if (skb->len != 0) 
if (!last) 
if (call->count < PAGE_SIZE) { 
if (!call) 
if (upper_32_bits(offset) || upper_32_bits(offset + length)) 
if (!call) 
if (skb->len > 0) 
if (ncallbacks == 0) 
if (ncallbacks > AFSCBMAX) 
if (!call) 
for (loop = ncallbacks; loop > 0; loop--) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (!call) 
if (padsz > 0) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (!call) 
if (padsz > 0) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (!call) 
if (padsz > 0) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (!call) 
if (padsz > 0) { 
if (c_padsz > 0) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (new_dvnode != orig_dvnode) 
if (!call) 
if (o_padsz > 0) { 
if (n_padsz > 0) { 
if (!last) { 
if (call->reply_size != call->reply_max) { 
if (!call) 
if (first != last) 
if (pos + size > i_size) 
if (pos >> 32 || i_size >> 32 || size >> 32 || (pos + size) >> 32) 
if (!call) 
if (!last) { 
if (call->reply_size != call->reply_max) { 
if (call->operation_ID == FSSTOREDATA) 
if (!call) 
if (attr->ia_size >> 32) 
if (!call) 
if there's a change in file 
if (attr->ia_valid & ATTR_SIZE) 
if (!call) 
if (call->count >= AFSNAMEMAX) 
if (call->count > 0) { 
if ((call->count & 3) == 0) { 
if (call->count >= AFSNAMEMAX) 
if (call->count > 0) { 
if ((call->count & 3) == 0) { 
if (call->count >= AFSNAMEMAX) 
if (call->count > 0) { 
if ((call->count & 3) == 0) { 
if (skb->len != 0) 
if (!last) 
if (!tmpbuf) 
if (!call) { 
if (!last) 
if (call->reply_size != call->reply_max) 
if (!call) 
if (!call) 
if (!call) 
file : ./test/kernel/fs/afs/mntpt.c 
[ OK ] open : 4 ok... 
ify it under the terms of the GNU General Public License 
if (IS_ERR(page)) { 
if (PageError(page)) 
if (size > 2 && 
if (!devname) 
if (!options) 
if (test_bit(AFS_VNODE_PSEUDODIR, &vnode->flags)) { 
if (size < 2 || size > AFS_MAXCELLNAME) 
if (mntpt->d_name.name[0] == '.') { 
if (size > PAGE_SIZE - 1) 
if (IS_ERR(page)) { 
if (PageError(page)) 
if (super->volume->type == AFSVL_RWVOL || rwpath) 
if (IS_ERR(newmnt)) 
if (!list_empty(&afs_vfsmounts)) { 
for_expiry(&afs_vfsmounts); 
if it's still running 
file : ./test/kernel/fs/posix_acl.c 
[ OK ] open : 4 ok... 
for manipulating 
if (acl) { 
if (acl != ACL_NOT_CACHED) 
if (old != ACL_NOT_CACHED) 
forget_cached_acl(struct inode *inode, int type) 
if (old != ACL_NOT_CACHED) 
forget_cached_acl); 
if (old_access != ACL_NOT_CACHED) 
if (old_default != ACL_NOT_CACHED) 
forget_all_cached_acls); 
if (acl != ACL_NOT_CACHED) 
if (!IS_POSIXACL(inode)) 
force a ACL callback by just never filling the 
if (!inode->i_op->get_acl) { 
ified number of entries. 
if (acl) 
if (acl) { 
if (clone) 
if an acl is valid. Returns 0 if it is, or -E... otherwise. 
if (pa->e_perm & ~(ACL_READ|ACL_WRITE|ACL_EXECUTE)) 
if (state == ACL_USER_OBJ) { 
if (state != ACL_USER) 
if (!uid_valid(pa->e_uid)) 
if (state == ACL_USER) { 
if (state != ACL_GROUP) 
if (!gid_valid(pa->e_gid)) 
if (state != ACL_GROUP) 
if (state == ACL_OTHER || 
if (state == 0) 
if the acl can be exactly represented in the traditional 
if (!acl) 
if (mode_p) 
if (!acl) 
if current is granted want access to the inode 
if (uid_eq(inode->i_uid, current_fsuid())) 
if (uid_eq(pa->e_uid, current_fsuid())) 
if (in_group_p(inode->i_gid)) { 
if ((pa->e_perm & want) == want) 
if (in_group_p(pa->e_gid)) { 
if ((pa->e_perm & want) == want) 
if (found) 
for (mask_obj = pa+1; mask_obj != pe; mask_obj++) { 
if ((pa->e_perm & mask_obj->e_perm & want) == want) 
if ((pa->e_perm & want) == want) 
ify acl when creating a new inode. The caller must ensure the acl is 
if (mask_obj) { 
if (!group_obj) 
ify the ACL for the chmod syscall. 
if (mask_obj) { 
if (!group_obj) 
if (clone) { 
if (err < 0) { 
if (clone) { 
if (err) { 
if (!IS_POSIXACL(inode)) 
if (!inode->i_op->set_acl) 
if (IS_ERR_OR_NULL(acl)) { 
if (ret) 
if (S_ISLNK(*mode) || !IS_POSIXACL(dir)) 
if (IS_ERR(p)) { 
if (!p) 
if (!*acl) 
if (ret < 0) { 
if (ret == 0) { 
if (!S_ISDIR(*mode)) { 
if (!value) 
if (size < sizeof(posix_acl_xattr_header)) 
if (header->a_version != cpu_to_le32(POSIX_ACL_XATTR_VERSION)) 
if (count < 0) 
if (count == 0) 
for (end = entry + count; entry != end; entry++) { 
if (user_ns == &init_user_ns) 
if (user_ns == &init_user_ns) 
if (!value) 
if (size < sizeof(posix_acl_xattr_header)) 
if (header->a_version != cpu_to_le32(POSIX_ACL_XATTR_VERSION)) 
if (count < 0) 
if (count == 0) 
if (!acl) 
for (end = entry + count; entry != end; acl_e++, entry++) { 
if (!uid_valid(acl_e->e_uid)) 
if (!gid_valid(acl_e->e_gid)) 
if (!buffer) 
if (real_size > size) 
for (n=0; n < acl->a_count; n++, ext_entry++) { 
if (!IS_POSIXACL(dentry->d_inode)) 
if (S_ISLNK(dentry->d_inode->i_mode)) 
if (IS_ERR(acl)) 
if (acl == NULL) 
if (!IS_POSIXACL(inode)) 
if (!inode->i_op->set_acl) 
if (type == ACL_TYPE_DEFAULT && !S_ISDIR(inode->i_mode)) 
if (!inode_owner_or_capable(inode)) 
if (value) { 
if (IS_ERR(acl)) 
if (acl) { 
if (ret) 
if (!IS_POSIXACL(dentry->d_inode)) 
if (S_ISLNK(dentry->d_inode->i_mode)) 
if (type == ACL_TYPE_ACCESS) 
if (list && size <= list_size) 
if (type == ACL_TYPE_ACCESS) { 
if (error < 0) 
if (error == 0) 
if (error) 
if (default_acl) 
if (acl) 
file : ./test/kernel/fs/logfs/file.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
if (!page) 
if ((len == PAGE_CACHE_SIZE) || PageUptodate(page)) 
if ((pos & PAGE_CACHE_MASK) >= i_size_read(inode)) { 
if (copied < len) { 
if (!PageUptodate(page)) { 
if (copied == 0) 
if (i_size_read(inode) < (index << PAGE_CACHE_SHIFT) + end) { 
if (!PageDirty(page)) { 
for filesystems that don't have to wait 
if (err) 
if (level != 0) 
if (bix < end_index) 
if (bix > end_index || offset == 0) { 
if (block->reserved_bytes) { 
if (IS_RDONLY(inode)) 
if (!inode_owner_or_capable(inode)) 
if (err) 
if (ret) 
if (err) 
if (attr->ia_valid & ATTR_SIZE) { 
if (err) 
file : ./test/kernel/fs/logfs/compr.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
if (err != Z_OK) 
if (err != Z_STREAM_END) 
if (err != Z_OK) 
if (stream.total_out >= stream.total_in) 
if (err != Z_OK) 
if (err != Z_STREAM_END) 
if (err != Z_OK) 
if (!stream.workspace) 
file : ./test/kernel/fs/logfs/dir.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
fore we need to do 
if we were 
if we were interrupted, we delete the inode. 
if we were 
for the estimated amount of information 
for (i = 0; i < len; i++) 
for a given hash 
for 2x and 3x indirect 
for a reasonable amount of hash 
fore overflowing.  Oh and currently we don't overflow but return 
force collisions for a couple of days showed that on average the 
if all entries are carefully chosen. 
if (name->len > LOGFS_MAX_NAMELEN) 
for (round = 0; round < 20; round++) { 
if (beyond_eof(dir, index)) 
if (!logfs_exist_block(dir, index)) 
if (IS_ERR(page)) 
if (name->len != be16_to_cpu(dd->namelen) || 
if (logfs_inode(inode)->li_block) 
if (!ta) 
if (!page) { 
if (IS_ERR(page)) { 
if (!ret) 
if (ret) { 
if (!logfs_empty_dir(inode)) 
if (ctx->pos < 0) 
if (!dir_emit_dots(file, ctx)) 
for (;; pos++, ctx->pos++) { 
if (beyond_eof(dir, pos)) 
if (!logfs_exist_block(dir, pos)) { 
if (IS_ERR(page)) 
if (full) 
if (IS_ERR(page)) 
if (!page) { 
if (IS_ERR(inode)) 
for dentry (%lx, %lx)n", 
if (i_size_read(dir) < index) 
for (round = 0; round < 20; round++) { 
if (logfs_exist_block(dir, index)) 
if (!page) 
if (!err) 
for this particular hash and no fallback. 
if (!ta) { 
if (dest) { 
if (!ret) 
if (ret) { 
if (!ret) 
if (ret) { 
while the mode is 
for us but for some reason fails to do so. 
if (IS_ERR(inode)) 
if (IS_ERR(inode)) 
if (dentry->d_name.len > LOGFS_MAX_NAMELEN) 
if (IS_ERR(inode)) 
if (destlen > dir->i_sb->s_blocksize) 
if (IS_ERR(inode)) 
if (IS_ERR(page)) 
while taking care to remember our operation in the journal. 
if (err) 
if (!ta) 
if (!err) 
if (err) { 
if (!err) 
if (err) 
if (err) 
if (isdir) { 
if (err) 
if (!ta) 
if (err) { 
if (!err) 
if (new_dentry->d_inode) 
fore .get_sb() returns. */ 
if (super->s_victim_ino) { 
if (IS_ERR(inode)) 
if (err) { 
if (super->s_rename_dir) { 
if (IS_ERR(inode)) 
if (err) { 
file : ./test/kernel/fs/logfs/inode.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
fore also lacks a method to store the previous 
for new inodes.  Being just a 32bit counter, 
fore we have to increment 
for now... 
ife a bit 
for GC to make progress, naturally. 
fore we 
ific reference counting on top of what the vfs 
if GC accessed the inode, its 
if (!inode) 
if (!(inode->i_state & I_NEW)) 
if (err || inode->i_nlink == 0) { 
if (!err) 
if we hand out a cached inode, 0 otherwise. 
if (ino == LOGFS_INO_MASTER) 
if (ino == LOGFS_INO_SEGFILE) 
for_each_entry(li, &super->s_freeing_list, li_freeing_list) 
if (inode->i_ino < LOGFS_RESERVED_INOS) { 
if (li->li_refcount == 0) 
if (inode->i_ino == LOGFS_INO_MASTER) 
if (inode->i_ino == LOGFS_INO_SEGFILE) 
if (is_cached) { 
for (i = 0; i < LOGFS_EMBEDDED_FIELDS; i++) 
if (!li) 
fore all other 
while other inodes are still in use and dirty.  Not 
if such 
for 
for... 
if (!inode) 
if (IS_ERR(inode)) 
if (err) { 
if creat() failed.  Safe to skip. */ 
if (super->s_inos_till_wrap < 0) { 
if (!inode) 
for (i = 0; i < LOGFS_EMBEDDED_FIELDS; i++) 
if (!logfs_inode_cache) 
fore we 
file : ./test/kernel/fs/logfs/dev_mtd.c 
[ OK ] open : 4 ok... 
for MTD 
if (ret) 
if we should loop instead. */ 
if (super->s_flags & LOGFS_SB_FLAG_RO) 
ift) << super->s_writeshift); 
if (ret || (retlen != len)) 
for completion before returning 
for (index = ofs >> PAGE_SHIFT; index < (ofs + len) >> PAGE_SHIFT; index++) { 
if (!page) 
if (logfs_super(sb)->s_flags & LOGFS_SB_FLAG_RO) 
if (ret) 
for_completion(&complete); 
if (err == -EUCLEAN || err == -EBADMSG) { 
force GC this segment */ 
while (mtd_block_isbad(mtd, *ofs)) { 
if (*ofs >= mtd->size) 
while (mtd_block_isbad(mtd, *ofs)) { 
if (*ofs <= 0) 
for (i = 0; i < nr_pages; i++) { 
if (err) 
if (super->s_flags & LOGFS_SB_FLAG_RO) 
if (len == 0) { 
if (head) { 
if (!buf) 
if (err) 
if (memchr_inv(buf, 0xff, super->s_writesize)) 
if (IS_ERR(mtd)) 
file : ./test/kernel/fs/logfs/journal.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
for_each(i) 
for GC */ 
for with speed reserve - the filesystem 
if (free < 0) 
for_each(i) { 
for (i = 0; i < LOGFS_EMBEDDED_FIELDS; i++) 
for_each(i) 
if (a->gc_level >= LOGFS_NO_AREAS) 
if (a->vim != VIM_DEFAULT) 
if (area->a_segno) 
if (super->s_writesize > 1) 
if (jh->h_compr == COMPR_NONE) 
if (err) 
if (len > sb->s_blocksize) 
if ((type < JE_FIRST) || (type > JE_LAST)) 
if (datalen > bufsize) 
if (err) 
if (jh->h_crc != logfs_crc32(jh, len + sizeof(*jh), 4)) { 
forgot about the header length 
if (jh->h_crc == logfs_crc32(jh, len, 4)) 
if (err) 
if (err) 
for most recent commit */ 
if (err) 
if (jh->h_type != cpu_to_be16(JE_COMMIT)) 
if (err) 
if ((datalen > sizeof(super->s_je_array)) || 
if (last_ofs == 0) 
if (err) 
for (i = 0; i < super->s_no_je; i++) { 
if (err) 
if (!segno) 
if (err) 
if (crc != sh.crc) { 
for_each(i) { 
if (gec[i] > max) { 
if (max_i == -1) 
for_each(i) { 
if (i == LOGFS_JOURNAL_SEGS) 
while (!super->s_journal_seg[i]); 
if (err) 
for_each(i) 
if (li->li_block) { 
for (i = 0; i < LOGFS_EMBEDDED_FIELDS; i++) 
if (super->s_writesize > 1) 
if (len == 0) 
if (compr_len < 0 || type == JE_ANCHOR) { 
if (ret) 
if (must_pad) { 
if (jh->h_type == cpu_to_be16(JE_COMMIT)) 
if (ofs < 0) 
force u8)level; 
if (fill >= sb->s_blocksize / sizeof(*oa)) { 
if (err) 
if (super->s_je_fill) 
if (!(super->s_flags & LOGFS_SB_FLAG_DIRTY)) 
for_each_area(i) { 
if (err) 
if (err) 
if (err) 
if (err) 
if (err) 
fore 
if (err) 
for_each(i) 
for (i = 0; i < super->s_no_journal_segs; i++) { 
if (!super->s_je) 
if (!super->s_compressed_je) 
if (IS_ERR(super->s_master_inode)) 
if (ret) 
file : ./test/kernel/fs/logfs/gc.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
ifference between low erase 
for "too big" 
ific meaning? :) 
for wear leveling at the 
ifile layer distance 0 */ 
force u8)__gc_level; 
ifile_levels + super->s_iblock_levels - gc_level; 
ifile_levels - (gc_level - 6); 
ifile_levels + super->s_iblock_levels; 
if (reserved) 
for_each_area(i) { 
if (area->a_is_open && area->a_segno == segno) 
if (se.ec_level == cpu_to_be32(BADSEG) || 
if (sh.crc != logfs_crc32(&sh, sizeof(sh), 4)) { 
for (seg_ofs = LOGFS_SEGMENT_HEADERSIZE; 
if (!memchr_inv(&oh, 0xff, sizeof(oh))) 
if (oh.crc != logfs_crc32(&oh, sizeof(oh) - 4, 4)) { 
if (valid == 1) { 
if (valid == 2) { 
while (*p) { 
if (list->sort_by_ec) 
if (comp) 
if (list->count <= list->maxcount) { 
if (ec) 
for normal usage.  It usually gets the 
for wear 
forget 
while.  We have better candidates for each purpose. 
if (cand->valid == 0) { 
if (cand) { 
for Garbage Collection */ 
for wear leveling, 
if (cand) 
if (cand) 
if (!cand) 
if (cand) { 
if (segment_is_reserved(sb, segno)) 
if (valid == RESERVED) 
if (list->count == 0) 
for garbage collection.  Main criterion is 
fort segment on the lowest level first, 
fort.  Hence the LOGFS_MAX_OBJECTSIZE in the comparison. 
for (i = max_dist; i >= 0; i--) { 
if (!this) 
if (!cand) 
if (this->valid + LOGFS_MAX_OBJECTSIZE <= cand->valid) 
if (!cand) { 
ift, 
if (cand) 
if a wrap occurs, 0 otherwise */ 
for (i = SCAN_RATIO; i > 0; i--) { 
if (segno >= super->s_no_segs) { 
if 
forever, looking for GC candidates 
fore 
if (super->s_shadow_tree.no_shadowed_segments >= MAX_OBJ_ALIASES) 
if (no_free_segments(sb) >= target && 
for (round = 0; round < SCAN_ROUNDS; ) { 
if (no_free_segments(sb) >= target) 
if (progress) 
if (round - last_progress > 2) 
if necessary.  However, after 
if (super->s_no_object_aliases < MAX_OBJ_ALIASES) 
if (list_empty(&super->s_object_alias)) { 
for GC not making any progress and limited 
if (*next_event < super->s_gec) { 
if (wl_ratelimit(sb, &super->s_wl_gec_ostore)) 
if (!wl_cand) 
if (!free_cand) 
if (wl_cand->erase_count < free_cand->erase_count + WL_DELTA) { 
if we 
ificant improvement.  That means that a) the current journal segments 
if it is aging 
ifference, compared to ostore wear 
if (wl_ratelimit(sb, &super->s_wl_gec_journal)) 
if (super->s_reserve_list.count < super->s_no_journal_segs) { 
for_each(i) 
for (i = 0; i < 2; i++) { 
if (min_journal_ec > max_reserve_ec + 2 * WL_DELTA) { 
fore free space is getting saturated with dirty 
if (super->s_dirty_used_bytes + super->s_dirty_free_bytes 
if (!area->a_is_open) 
if (super->s_devops->can_write_buf(sb, ofs) == 0) 
fore the journal commit happened.  In that case we wouldn't have 
if (cleaned != valid) 
for_each_area(i) { 
if (err) 
for_each_area(i) 
while (list->count) { 
if (!super->s_free_list.count) 
for us, really. 
for_each_area(i) 
file : ./test/kernel/fs/logfs/segment.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
if (err) 
ift, 
if (use_filler) 
if (page) 
if (IS_ERR(page)) 
if (!PagePrivate(page)) { 
while (len); 
if (len % PAGE_SIZE) { 
if (!PagePrivate(page)) { 
while (no_indizes) { 
if (!PagePrivate(page)) { 
for final writeout. 
if (final) 
for_each_entry(item, &block->item_list, list) { 
if (err) 
for (i = 0; i < count; i++) { 
if (!item) 
if (!block) { 
if (test_and_set_bit(item->child_no, block->alias_map)) { 
while (!list_empty(&block->item_list)) { 
if (level == 0) { 
if (inode->i_ino == LOGFS_INO_MASTER) 
if (shadow->gc_level == 0) 
ifying the 
ifications _before_ 
if (compr_len >= 0) { 
if (shadow->gc_level != 0) { 
for indirect blocks */ 
if (do_compress) 
while (len) { 
if (IS_ERR(page)) 
fore comparing. 
if 0 
if (err) 
if (crc != sh->crc) { 
if 
if (err) 
if (crc != oh->crc) { 
if (!(super->s_flags & LOGFS_SB_FLAG_OBJ_ALIAS)) 
if (!block) 
for_each_entry_safe(item, next, &block->item_list, list) { 
if (!PagePrivate(page)) { 
if (super->s_flags & LOGFS_SB_FLAG_SHUTDOWN) { 
for (pos = 0; ; pos++) { 
if (pos >= LOGFS_BLOCK_FACTOR) 
if (PagePrivate(page)) { 
if (err) 
if (be64_to_cpu(oh.ino) != inode->i_ino 
if (err) 
if (crc != oh.data_crc) { 
if (err) { 
if (crc != oh.data_crc) { 
if (err) { 
if (PageUptodate(page)) 
if (!err) { 
if (!shadow->old_ofs) 
if (shadow->gc_level == 0) 
for (ofs = start; ofs < end; ofs += PAGE_SIZE) { 
if (!page) 
if (PagePrivate(page)) { 
if (area->a_is_open && area->a_used_bytes + bytes <= super->s_segsize) 
if (area->a_is_open) { 
if (err) { 
if (super->s_writesize) 
if (len == 0) 
for_each_area(i) 
for this area.  Effectively takes a 
if (super->s_free_list.count == 0) { 
if (err) 
force u8)area->a_level; 
if (area) 
for_each_area(i) 
if (!area) 
if (IS_ERR(inode)) 
if (!super->s_alias_pool) 
if (!super->s_journal_area) 
for_each_area(i) { 
if (!super->s_area[i]) 
for (i--; i >= 0; i--) 
file : ./test/kernel/fs/logfs/super.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
for 
if (page) 
if (err) { 
if (page == emergency_page) 
for (segno = 0; segno < super->s_no_segs; segno++) { 
if (++segno < super->s_no_segs) { 
if (++segno < super->s_no_segs) { 
if (++segno < super->s_no_segs) { 
formation to device 
formation as it can gather into the spare space. 
for root, similar to ext2. 
ifdef CONFIG_BLOCK 
if 
if (sb->s_mtd) 
if 
if (mtd && sb->s_mtd == mtd) 
if (super->s_bdev && sb->s_bdev == super->s_bdev) 
ifile_levels	= super->s_ifile_levels; 
ift	= super->s_segshift; 
ift	= super->s_writeshift; 
for_each(i) 
if (!page) 
if (err) 
if (err) 
iffer, so ignore them */ 
if (err) 
if (err) 
if (!valid0 && valid1) { 
if (valid0 && !valid1) { 
if (valid0 && valid1 && ds_cmp(ds0, ds1)) { 
fore?!? */ 
if (err) 
if (err) 
for trailing unaccounted data */ 
if (err) 
fore any data gets dirtied */ 
if necessary */ 
if (err) 
if (IS_ERR(rootdir)) 
if (!sb->s_root) 
if (!super->s_erase_page) 
for read-only mounts */ 
if (err) { 
if (ds->ds_magic != cpu_to_be64(LOGFS_MAGIC)) 
if (sh->crc != logfs_crc32(sh, LOGFS_SEGMENT_HEADERSIZE, 4)) 
if (ds->ds_crc != logfs_crc32(ds, sizeof(*ds), 
if (!first || IS_ERR(first)) 
if (!last || IS_ERR(last)) { 
if (!logfs_check_ds(page_address(first))) { 
if (!logfs_check_ds(page_address(last))) { 
if (!page) 
ift; 
ift = ds->ds_segment_shift; 
ift; 
ift = ds->ds_write_shift; 
for_each(i) 
ifile_levels = ds->ds_ifile_levels; 
ifile_levels + super->s_iblock_levels 
if (!super->s_btree_pool) 
if (ret) 
if (ret) 
if (super->s_feature_incompat & ~LOGFS_FEATURES_INCOMPAT) 
if ((super->s_feature_ro_compat & ~LOGFS_FEATURES_RO_COMPAT) && 
if (ret) 
if (ret) 
if (ret) 
if (ret) 
if (super->s_erase_page) 
if (IS_ERR(sb)) { 
if (sb->s_root) { 
for indirect blocks. 
if (err) 
if (err) { 
if (!super) 
if (!devname) 
if (strncmp(devname, "mtd", 3)) 
if (*garbage) 
if (err) { 
if (!emergency_page) 
if (ret) 
if (ret) 
if (!ret) 
file : ./test/kernel/fs/logfs/dev_bdev.c 
[ OK ] open : 4 ok... 
for block devices 
if (err) { 
for_each_segment_all(bvec, bio, i) { 
if (atomic_dec_and_test(&super->s_pending_writes)) 
for (i = 0; i < nr_pages; i++) { 
if (len == 0) { 
if (head) { 
if (atomic_dec_and_test(&super->s_pending_writes)) 
for (i = 0; i < nr_pages; i++) { 
if (super->s_flags & LOGFS_SB_FLAG_RO) 
if (ensure_write) { 
for the journal they are required.  Otherwise a scan 
for block devices. */ 
if (IS_ERR(bdev)) 
if (MAJOR(bdev->bd_dev) == MTD_BLOCK_MAJOR) { 
file : ./test/kernel/fs/logfs/readwrite.c 
[ OK ] open : 4 ok... 
for Linux kernel code, license is GPLv2 
ifile) 
for 4KiB blocksize) are ignored 
if (level == 0) 
force long)level << LEVEL_SHIFT; 
if (!(index & INDIRECT_BIT)) { 
for (i = 0; i < LOGFS_EMBEDDED_FIELDS; i++) 
for (i = 0; i < LOGFS_EMBEDDED_FIELDS; i++) 
if (li->li_used_bytes >> sb->s_blocksize_bits < ULONG_MAX) 
if (lock) { 
if (PagePreLocked(page)) 
if (lock) 
if (super->s_lock_count) 
while holding 
iff the page in question 
for s_write_mutex.  We annotate this fact by setting PG_pre_locked 
if (page) 
if (lock) { 
for shadowed space 
if (page) 
fore releasing 
if (lock) 
while (unlikely(!trylock_page(page))) { 
if (PagePreLocked(page)) { 
for us, it 
for it to finish. 
if (!page) { 
if (!page) 
if (unlikely(err)) { 
if (err == -EEXIST) 
if (!PagePreLocked(page)) 
if (rw == READ) 
if (rw == READ) 
force int)skip, LOGFS_BLOCK_BITS); 
fore the actual work has been 
for now the line below isn't 
if (inode->i_ino == LOGFS_INO_MASTER) 
ifting generates good code, but must match the format 
for (pos = 0; ; pos++) { 
if (pos >= LOGFS_EMBEDDED_FIELDS + INODE_POINTER_OFS) 
if (err) 
for (pos = 0; ; pos++) { 
if (pos >= LOGFS_BLOCK_FACTOR) 
if (err) 
for_each_entry(block, &super->s_object_alias, alias_list) { 
if (err) 
if (PagePrivate(page)) { 
if (li->li_block) 
if (page->index < first_indirect_block()) { 
if (page->index == first_indirect_block()) { 
if (!page_is_empty) { 
for (i = start; i < LOGFS_BLOCK_FACTOR; i++) { 
if (ptr) 
if (ptr & LOGFS_FULLY_POPULATED) 
if (PagePrivate(page)) 
if (PagePrivate(page)) 
if (!block) 
if (!bofs) 
if (bix >= maxbix(li->li_height)) 
for (level = LEVEL(li->li_height); 
if (!ipage) 
if (ret) { 
if (!bofs) 
if (index < I0_BLOCKS) 
if (!bofs) 
if (bix >= maxbix(li->li_height)) 
for (level = LEVEL(li->li_height); level != 0; level = SUBLEVEL(level)) { 
if (!ipage) 
if (ret) { 
if (!bofs) 
if (bix < I0_BLOCKS) 
for (; bix < I0_BLOCKS; bix++) 
for (level = LEVEL(li->li_height); level != 0; level = SUBLEVEL(level)) { 
if (!page) 
if (ret) { 
while (slot < LOGFS_BLOCK_FACTOR) { 
if (!data && !(be64_to_cpu(rblock[slot]) & LOGFS_FULLY_POPULATED)) 
if (slot >= LOGFS_BLOCK_FACTOR) { 
if (!bofs) { 
if (bix < I0_BLOCKS) { 
if (bix < I0_BLOCKS) 
if (!li->li_data[INDIRECT_INDEX]) 
if (li->li_data[INDIRECT_INDEX] & LOGFS_FULLY_POPULATED) 
if (bix >= maxbix(li->li_height)) 
if (bix < maxbix(li->li_height)) 
if some port writes semi- 
if (bix < I0_BLOCKS) { 
if (bix < I0_BLOCKS) 
if (bix < maxbix(li->li_height)) { 
if (ret >= end) 
for (level = LEVEL(li->li_height); level != 0; level = SUBLEVEL(level)){ 
if (ret) { 
if (!bofs) 
if (pure_ofs(bofs) == ofs) 
if (!bofs) 
if (bix >= maxbix(li->li_height)) 
if (pure_ofs(bofs) == ofs) 
if ((inode->i_nlink == 0) && atomic_read(&inode->i_count) == 1) 
if (bix < I0_BLOCKS) 
if the block is invalid, 1 if it is valid and 2 if it will 
if (ino == -1) 
if (IS_ERR(inode)) 
if (ret) 
for a journal commit. 
if (btree_lookup64(&super->s_shadow_tree.old, ofs)) 
if (ret) { 
if (!bytes) 
if (available < bytes) 
if (available < bytes + super->s_root_reserve && 
if (block && block->reserved_bytes) 
while ((ret = logfs_reserve_bytes(inode, 6 * LOGFS_MAX_OBJECTSIZE)) && 
if (!ret) { 
if (!ta) 
if (inode->i_ino != LOGFS_INO_MASTER) { 
if (!btree_lookup32(&tree->segment_map, segno)) { 
for the current write 
for the current write to the tree, along with any shadows in 
if an inode is written, 
if (PagePrivate(page)) { 
if (shadow) { 
ift); 
if (block->inode && block->inode->i_ino == LOGFS_INO_MASTER) { 
if (!test_bit(child_no, block->alias_map)) { 
for the inode itself. 
if (shadow->new_len == shadow->old_len) 
if (wc->ofs == 0) 
if (wc->flags & WF_WRITE) 
if (wc->flags & WF_DELETE) 
if (err) { 
if (level != 0) { 
if (wc->ofs && full) 
if (err) 
if (empty0 != empty1) 
if (full0 != full1) 
if (!ipage) 
if (this_wc->ofs) { 
if (ret) 
if (!PageUptodate(ipage)) { 
if ((__force u8)level-1 > (__force u8)target_level) 
if (ret) 
if (child_wc.ofs || logfs_block(ipage)->partial) 
for indirect blocks in the future, which we cannot reserve */ 
if (li->li_height > (__force u8)target_level) 
if (ret) 
if (li->li_data[INDIRECT_INDEX] != wc.ofs) { 
if (block && block->ta) 
force u8)level; 
while (height > li->li_height || bix >= maxbix(li->li_height)) { 
if (!page) 
if (err) 
if (logfs_block(page) && logfs_block(page)->reserved_bytes) 
if (index < I0_BLOCKS) 
if (err) 
if (page->index < I0_BLOCKS) 
if (err) 
if (!page) 
if (!page) 
if (!err) { 
if (!err && shrink_level(gc_level) == 0) { 
for the inode 
if (inode->i_ino == LOGFS_INO_MASTER) 
if (size <= pageofs || size - pageofs >= PAGE_SIZE) 
if (err) 
if (err) { 
for (e = I0_BLOCKS - 1; e >= 0; e--) { 
if (!wc.ofs) 
if (!page) 
if (err) { 
if (err) 
ifferent blocksizes */ 
force u8)level]; 
force u8)level]; 
if (*bix <= logfs_start_index(SUBLEVEL(*level))) 
if (err) 
for (e = LOGFS_BLOCK_FACTOR - 1; e >= 0; e--) { 
if (size > next_bix * LOGFS_BLOCKSIZE) 
if (!child_wc.ofs) 
if (!page) 
if ((__force u8)level > 1) 
if (err) 
if (!truncate_happened) { 
if (logfs_block(ipage)->partial) 
if (!wc.ofs) 
if (!page) 
if (err) 
if (li->li_data[INDIRECT_INDEX] != wc.ofs) 
if (size >= logfs_factor(logfs_inode(inode)->li_height)) 
if (ret) 
if 
while (size > target) { 
if (size < target) 
if (!err) 
if (!err) { 
if (err) 
if (!block) 
if (PagePrivate(page)) { 
if (!block) 
if (!PagePrivate(